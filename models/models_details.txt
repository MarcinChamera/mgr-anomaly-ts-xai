- dt_maxdepth2_model - decision tree model, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, parameter max_depth=2, no hypertuning, input not scaled
- dt_maxdepth_unlim_model - decision tree model, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, default parameters (max_depth=None), no hypertuning, input scaled
- lr_model - logistic regression model, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, default parameters, no hypertuning, input scaled
- rf_model - random forest model, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, njobs=-1, no hypertuning, input scaled
- xgb_model - xgboost model, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, n_jobs=-1, no hypertuning, input scaled
- simple_mlp_model - mlp model with 1 hidden layer, train/test split, no validation, training period from 2018-07-25 to 2018-07-31, 1000 cells in the hidden layer, optimizer sgd learning_rate=0.07, epochs=25, batch_size=64, loss bce, no hypertuning, input not scaled
- dt_maxdepth5_model - decision tree model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=5 from hp tuning, hp tuning using gridsearchcv, input scaled
- lr_C1_model - logistic regression model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, C=1 from hp tuning, hp tuning using gridsearchcv, input scaled
- rf_maxdepth20_nestimators100 - random forest model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=20 from hp tuning, n_estimators=100 from hp tuning, hp tuning using gridsearchcv, input scaled
- xgb_maxdepth3_nestimators50_lr0dot3_model - xgboost model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=3 from hp tuning, n_estimators=50 from hp tuning, learning_rate=0.3 from hp tuning, hp tuning using gridsearchcv, input scaled
- dt_maxdepth5_classweight0dot01_model - decision tree model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=5, class_weight={0:0.01} from hp tuning, hp tuning using gridsearchcv, input scaled
- dt_maxdepth5_smote1_model - decision tree model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=5, smote sampling_strategy=1 from hp tuning, hp tuning using gridsearchcv, input scaled
- dt_maxdepth5_smote0dot1_randundersampl1_model - decision tree model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=5, smote sampling_strategy=0.1, random undersampling sampling_strategy=1 from hp tuning, hp tuning using gridsearchcv, input scaled
- bagging_dt_maxdepth20_nestimators100_model - bagging with decision trees model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=20, n_estimators=100, input scaled
- bagging_dt_maxdepth20_nestimators100_randundersampl0dot1_model - bagging with decision trees model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=20, n_estimators=100, random undersampling sampling_strategy=0.1 from hp tuning, hp tuning using gridsearchcv, input scaled
- rf_maxdepth20_nestimators100_randundersampl0dot05 - random forest model, prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, max_depth=20, n_estimators=100, random undersampling sampling_strategy=0.05 from hp tuning, hp tuning using gridsearchcv, input scaled
- simple_mlp_earlystop2_model - mlp model with 1 hidden layer, hold-out validation, training period from 2018-07-11 to 2018-07-17, 1000 cells in the hidden layer, optimizer sgd learning_rate=0.0005, epochs=500, batch_size=64, loss bce, early stopping patience=2, no hypertuning, input scaled
- simple_mlp_model_adam - mlp model with 1 hidden layer, hold-out validation, training period from 2018-07-11 to 2018-07-17, 1000 cells in the hidden layer, optimizer adam learning_rate=0.0005, epochs=100, batch_size=64, loss bce, early stopping patience=2, no hypertuning, input scaled
- simple_mlp_model_droput - mlp model with 1 hidden layer, hold-out validation, training period from 2018-07-11 to 2018-07-17, 1000 cells in the hidden layer, optimizer adam learning_rate=0.0005, epochs=100, batch_size=64, loss bce, early stopping patience=2, dropout 0.2, no hypertuning, input scaled
- simple_mlp_model_embeddings - mlp model with 1 hidden layer, hold-out validation, training period from 2018-07-11 to 2018-07-17, 1000 cells in the hidden layer, optimizer adam learning_rate=0.0001, epochs=100, batch_size=64, loss bce, early stopping patience=2, dropout 0.2, categorical features: tx_weekday terminal_id, embedding dim=10, no hypertuning, input scaled
- mlp_grid_search_model - mlp model with 2 hidden layers (from hp tuning), prequential split, training period (after hp tuning) from 2018-07-25 to 2018-07-31, 500 cells in the hidden layers from hp tuning, optimizer adam learning_rate=0.001 from hp tuning, epochs=20 from hp tuning, batch_size=64 from hp tuning, loss bce, dropout 0.2 from hp tuning, hp tuning using gridsearchcv, input scaled