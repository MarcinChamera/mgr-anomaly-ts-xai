{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)\n",
    "\n",
    "input_features_length = 15\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../shared_functions.py\n",
    "%run ../my_shared_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  Modules   | Parameters |\n",
      "+------------+------------+\n",
      "| fc1.weight |    7500    |\n",
      "|  fc1.bias  |    500     |\n",
      "| fc2.weight |    500     |\n",
      "|  fc2.bias  |     1      |\n",
      "+------------+------------+\n",
      "Total Trainable Params: 8501\n"
     ]
    }
   ],
   "source": [
    "class FraudMLPHypertuned(torch.nn.Module):\n",
    "    def __init__(self, input_size,hidden_size=500,num_layers=2,p=0.2):\n",
    "        super(FraudMLPHypertuned, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.p = p\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc_hidden=[]\n",
    "        for i in range(num_layers-1):\n",
    "            self.fc_hidden.append(torch.nn.Linear(self.hidden_size, self.hidden_size).to(DEVICE))\n",
    "            self.fc_hidden.append(torch.nn.ReLU())\n",
    "            \n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(self.p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.fc1(x)\n",
    "        hidden = self.relu(hidden)             \n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        for layer in self.fc_hidden:\n",
    "            hidden=layer(hidden)\n",
    "            hidden = self.dropout(hidden)\n",
    "        \n",
    "        output = self.fc2(hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "mlp_model = FraudMLPHypertuned(input_size=input_features_length)\n",
    "_ = count_parameters(mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  Modules   | Parameters |\n",
      "+------------+------------+\n",
      "| fc1.weight |    1500    |\n",
      "|  fc1.bias  |    100     |\n",
      "| fc2.weight |    2000    |\n",
      "|  fc2.bias  |     20     |\n",
      "| fc3.weight |    2000    |\n",
      "|  fc3.bias  |    100     |\n",
      "| fc4.weight |    1500    |\n",
      "|  fc4.bias  |     15     |\n",
      "+------------+------------+\n",
      "Total Trainable Params: 7235\n"
     ]
    }
   ],
   "source": [
    "autoencoder_model = SimpleAutoencoder(input_features_length, 100, 20)\n",
    "autoencoder_parameters = count_parameters(autoencoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  Modules   | Parameters |\n",
      "+------------+------------+\n",
      "| fc1.weight |    1600    |\n",
      "|  fc1.bias  |    100     |\n",
      "| fc2.weight |    100     |\n",
      "|  fc2.bias  |     1      |\n",
      "+------------+------------+\n",
      "Total Trainable Params: 1801\n",
      "9036\n"
     ]
    }
   ],
   "source": [
    "autoencoder_mlp_model = SimpleFraudMLPWithDropout(input_features_length + 1, 100, 0.2)\n",
    "autoencoder_mlp_parameters = count_parameters(autoencoder_mlp_model)\n",
    "print(autoencoder_parameters + autoencoder_mlp_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|   Modules    | Parameters |\n",
      "+--------------+------------+\n",
      "| conv1.weight |    3000    |\n",
      "|  conv1.bias  |    100     |\n",
      "| conv2.weight |   20000    |\n",
      "|  conv2.bias  |    100     |\n",
      "|  fc1.weight  |   50000    |\n",
      "|   fc1.bias   |    500     |\n",
      "|  fc2.weight  |    500     |\n",
      "|   fc2.bias   |     1      |\n",
      "+--------------+------------+\n",
      "Total Trainable Params: 74201\n"
     ]
    }
   ],
   "source": [
    "cnn_model = FraudConvNetWithDropout(input_features_length, hidden_size=500, conv2_params=(100,2), p=0.2)\n",
    "_ = count_parameters(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|      Modules      | Parameters |\n",
      "+-------------------+------------+\n",
      "| lstm.weight_ih_l0 |    6000    |\n",
      "| lstm.weight_hh_l0 |   40000    |\n",
      "|  lstm.bias_ih_l0  |    400     |\n",
      "|  lstm.bias_hh_l0  |    400     |\n",
      "|     fc1.weight    |   50000    |\n",
      "|      fc1.bias     |    500     |\n",
      "|     fc2.weight    |    500     |\n",
      "|      fc2.bias     |     1      |\n",
      "+-------------------+------------+\n",
      "Total Trainable Params: 97801\n"
     ]
    }
   ],
   "source": [
    "lstm_model = FraudLSTM(input_features_length, hidden_size=500, dropout_lstm=0.2)\n",
    "_ = count_parameters(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+\n",
      "|           Modules           | Parameters |\n",
      "+-----------------------------+------------+\n",
      "|      lstm.weight_ih_l0      |    6000    |\n",
      "|      lstm.weight_hh_l0      |   40000    |\n",
      "|       lstm.bias_ih_l0       |    400     |\n",
      "|       lstm.bias_hh_l0       |    400     |\n",
      "|          ff.weight          |    1500    |\n",
      "|           ff.bias           |    100     |\n",
      "| attention.linear_out.weight |   20000    |\n",
      "|  attention.linear_out.bias  |    100     |\n",
      "|          fc1.weight         |   50000    |\n",
      "|           fc1.bias          |    500     |\n",
      "|          fc2.weight         |    500     |\n",
      "|           fc2.bias          |     1      |\n",
      "+-----------------------------+------------+\n",
      "Total Trainable Params: 119501\n"
     ]
    }
   ],
   "source": [
    "lstm_attention_model = FraudLSTMWithAttention(input_features_length, hidden_size = 500, dropout_lstm=0.2)\n",
    "_ = count_parameters(lstm_attention_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
