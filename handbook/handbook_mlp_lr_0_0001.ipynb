{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../my_shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 444 ms\n",
      "919767 transactions loaded, containing 8195 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT = '../../fraud-detection-handbook/simulated-data-transformed/data/'\n",
    "\n",
    "BEGIN_DATE = \"2018-06-11\"\n",
    "END_DATE = \"2018-09-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "output_feature=\"TX_FRAUD\"\n",
    "\n",
    "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "start_date_training = datetime.datetime.strptime(\"2018-07-25\", \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7\n",
    "delta_valid = delta_test\n",
    "\n",
    "start_date_training_with_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "\n",
    "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training+datetime.timedelta(days=delta_train+delta_delay),\n",
    "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df, input_features)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_valid = torch.FloatTensor(valid_df[output_feature].values)\n",
    "\n",
    "training_set = FraudDataset(x_train.to(DEVICE), y_train.to(DEVICE))\n",
    "valid_set = FraudDataset(x_valid.to(DEVICE), y_valid.to(DEVICE))\n",
    "\n",
    "training_generator,valid_generator = prepare_generators(training_set,valid_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchamera\u001b[0m (\u001b[33mmgr-anomaly-tsxai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e3c2576a7f4dc3bb0266f7755fb402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666687282, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\handbook\\wandb\\run-20230806_163450-z5xx0i4m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mgr-anomaly-tsxai/mgr-anomaly-tsxai-project/runs/z5xx0i4m\" target=\"_blank\">drawn-eon-1166</a></strong> to <a href=\"https://wandb.ai/mgr-anomaly-tsxai/mgr-anomaly-tsxai-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = SEED,\n",
    "    begin_date = '2018-08-08',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0001,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered', 'constant_lr_0_0001'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.09847772872302721\n",
      "valid loss: 0.04099373452839229\n",
      "New best score: 0.04099373452839229\n",
      "\n",
      "Epoch 1: train loss: 0.03530663097529425\n",
      "valid loss: 0.02798019776022748\n",
      "New best score: 0.02798019776022748\n",
      "\n",
      "Epoch 2: train loss: 0.027268765705306477\n",
      "valid loss: 0.023891059650882278\n",
      "New best score: 0.023891059650882278\n",
      "\n",
      "Epoch 3: train loss: 0.024753110152354076\n",
      "valid loss: 0.022424466867896467\n",
      "New best score: 0.022424466867896467\n",
      "\n",
      "Epoch 4: train loss: 0.023589085577343464\n",
      "valid loss: 0.02170663549571927\n",
      "New best score: 0.02170663549571927\n",
      "\n",
      "Epoch 5: train loss: 0.022813489372079377\n",
      "valid loss: 0.021437506668709767\n",
      "New best score: 0.021437506668709767\n",
      "\n",
      "Epoch 6: train loss: 0.022218593811745922\n",
      "valid loss: 0.021000493722998437\n",
      "New best score: 0.021000493722998437\n",
      "\n",
      "Epoch 7: train loss: 0.021828248174758973\n",
      "valid loss: 0.020933409555015572\n",
      "New best score: 0.020933409555015572\n",
      "\n",
      "Epoch 8: train loss: 0.021417166018841683\n",
      "valid loss: 0.021208055898202013\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 9: train loss: 0.021190618671449318\n",
      "valid loss: 0.02053996451465074\n",
      "New best score: 0.02053996451465074\n",
      "\n",
      "Epoch 10: train loss: 0.020834833407420327\n",
      "valid loss: 0.020253828644630982\n",
      "New best score: 0.020253828644630982\n",
      "\n",
      "Epoch 11: train loss: 0.020573592118854973\n",
      "valid loss: 0.019863494727152948\n",
      "New best score: 0.019863494727152948\n",
      "\n",
      "Epoch 12: train loss: 0.020359772028449435\n",
      "valid loss: 0.019973500662002295\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 13: train loss: 0.020483050980690417\n",
      "valid loss: 0.01992873251749603\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 14: train loss: 0.019935294848825034\n",
      "valid loss: 0.01968494794288601\n",
      "New best score: 0.01968494794288601\n",
      "\n",
      "Epoch 15: train loss: 0.019805884512773476\n",
      "valid loss: 0.01970050052669218\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 16: train loss: 0.019640246718709984\n",
      "valid loss: 0.01960835462663805\n",
      "New best score: 0.01960835462663805\n",
      "\n",
      "Epoch 17: train loss: 0.019537579037162082\n",
      "valid loss: 0.019554553193238303\n",
      "New best score: 0.019554553193238303\n",
      "\n",
      "Epoch 18: train loss: 0.019370881613970756\n",
      "valid loss: 0.01976363862412287\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 19: train loss: 0.019228760163265114\n",
      "valid loss: 0.019179689818293696\n",
      "New best score: 0.019179689818293696\n",
      "\n",
      "Epoch 20: train loss: 0.01911566312878958\n",
      "valid loss: 0.019431100886753195\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 21: train loss: 0.019000697764124122\n",
      "valid loss: 0.01908344969617023\n",
      "New best score: 0.01908344969617023\n",
      "\n",
      "Epoch 22: train loss: 0.01891961108135846\n",
      "valid loss: 0.01920316641534507\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 23: train loss: 0.01872156898036521\n",
      "valid loss: 0.019181185390998348\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 24: train loss: 0.018662203325369744\n",
      "valid loss: 0.019261205047593665\n",
      "3  iterations since best score.\n",
      "Early stopping\n",
      "98.31799721717834\n",
      "0.004999637603759766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\..\\models\\DL\\mlp_adam)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.889</td></tr><tr><td>Average precision</td><td>0.661</td></tr><tr><td>Card Precision@100</td><td>0.303</td></tr><tr><td>F1 score</td><td>0.646</td></tr><tr><td>Prediction execution time</td><td>0.005</td></tr><tr><td>Training execution time</td><td>98.318</td></tr><tr><td>train loss</td><td>0.01866</td></tr><tr><td>val loss</td><td>0.01926</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">drawn-eon-1166</strong>: <a href=\"https://wandb.ai/mgr-anomaly-tsxai/mgr-anomaly-tsxai-project/runs/z5xx0i4m\" target=\"_blank\">https://wandb.ai/mgr-anomaly-tsxai/mgr-anomaly-tsxai-project/runs/z5xx0i4m</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230806_163450-z5xx0i4m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SimpleFraudMLP(len(input_features), config_mlp.hidden_size).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = config_mlp.lr)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "model,training_execution_time,train_losses_adam,valid_losses_adam = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=True,\n",
    "                                                                        save_path='../models/DL/mlp_adam/simple_mlp_model_adam_lr_0_0001.pt')\n",
    "\n",
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "start_time=time.time()\n",
    "# no need to set model in eval mode since there are no BN, Dropout layers\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "\n",
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_adam_lr_0_0001', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and adam optimizer, lr=0.0001')\n",
    "mlp_artifact.add_dir('../models/DL/mlp_adam')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
