{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "import shap\n",
    "import wandb\n",
    "import tsai.all\n",
    "import imblearn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = f'logs/ts_coin_ts_shap_generate_output_{datetime.now():%Y_%m_%d_%H_%M}.log',\n",
    "                    level = logging.INFO,\n",
    "                    format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../shared_functions.py\n",
    "%run ../my_shared_functions.py\n",
    "\n",
    "DIR_INPUT = '../../fraud-detection-handbook/simulated-data-transformed/data/'\n",
    "END_DATE = \"2018-09-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, \"2018-06-11\", END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),\n",
    "                                                                    transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "output_feature=\"TX_FRAUD\"\n",
    "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']\n",
    "\n",
    "BEGIN_DATE = \"2018-08-08\"\n",
    "start_date_training = datetime.datetime.strptime(BEGIN_DATE, \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7\n",
    "delta_valid = delta_test\n",
    "\n",
    "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training,\n",
    "                            delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "SEQ_LEN = 5\n",
    "\n",
    "# By default, scales input data\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df,input_features)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    { # 0\n",
    "        'artifact_type': 'cnn',\n",
    "        'artifact_name': 'cnn:v1',\n",
    "        'artifact_path': 'cnn-v1/cnn_model.pt',\n",
    "        'model_instance': FraudConvNet(len(input_features), SEQ_LEN).to(DEVICE),\n",
    "        'model_type': 'handbook'\n",
    "    },\n",
    "    { # 1\n",
    "        'artifact_type': 'lstm',\n",
    "        'artifact_name': 'lstm:v1',\n",
    "        'artifact_path': 'lstm-v1/lstm_model.pt',\n",
    "        'model_instance': FraudLSTM(len(input_features)).to(DEVICE),\n",
    "        'model_type': 'handbook'\n",
    "    },\n",
    "    { # 2\n",
    "        'artifact_type': 'lstm',\n",
    "        'artifact_name': 'lstm_attention:v1',\n",
    "        'artifact_path': 'lstm_attention-v1/lstm_attention_model.pt',\n",
    "        'model_instance': FraudLSTMWithAttention(len(input_features)).to(DEVICE),\n",
    "        'model_type': 'handbook'\n",
    "    },\n",
    "    { # 3\n",
    "        'artifact_type': 'cnn',\n",
    "        'artifact_name': 'cnn_hypertuned:v1',\n",
    "        'artifact_path': 'cnn_hypertuned-v1/cnn_hypertuned_model.pt',\n",
    "        'model_instance': FraudConvNetWithDropout(len(input_features), hidden_size=500,\n",
    "                                                  conv2_params=(100,2), p=0.2).to(DEVICE),\n",
    "        'model_type': 'handbook'    \n",
    "    },\n",
    "    { # 4\n",
    "        'artifact_type': 'lstm',\n",
    "        'artifact_name': 'lstm_hypertuned:v1',\n",
    "        'artifact_path': 'lstm_hypertuned-v1/lstm_hypertuned_model.pt',\n",
    "        'model_instance': FraudLSTM(len(input_features), hidden_size=500, dropout_lstm=0.2).to(DEVICE),\n",
    "        'model_type': 'handbook'\n",
    "    },\n",
    "    { # 5\n",
    "        'artifact_type': 'lstm',\n",
    "        'artifact_name': 'lstm_attention_hypertuned:v1',\n",
    "        'artifact_path': 'lstm_attention_hypertuned-v1/lstm_attention_hypertuned_model.pt',\n",
    "        'model_instance': FraudLSTMWithAttention(len(input_features), hidden_size=500,\n",
    "                                                 dropout_lstm=0.2).to(DEVICE),\n",
    "        'model_type': 'handbook'\n",
    "    },\n",
    "    { # 6\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'lstm_fit_one_cycle:v0',\n",
    "        'artifact_path': 'lstm_fit_one_cycle-v0/lstm_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.LSTM(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 7\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'fcn_fit_one_cycle:v0',\n",
    "        'artifact_path': 'fcn_fit_one_cycle-v0/fcn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.FCN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 8\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'gmlp_fit_one_cycle:v0',\n",
    "        'artifact_path': 'gmlp_fit_one_cycle-v0/gmlp_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.gMLP(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 9\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'gru_fcn_fit_one_cycle:v0',\n",
    "        'artifact_path': 'gru_fcn_fit_one_cycle-v0/gru_fcn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.GRU_FCN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 10\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'gru_fit_one_cycle:v0',\n",
    "        'artifact_path': 'gru_fit_one_cycle-v0/gru_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.GRU(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 11\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'inceptiontime_fit_one_cycle:v0',\n",
    "        'artifact_path': 'inceptiontime_fit_one_cycle-v0/inceptiontime_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.InceptionTime(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 12\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'lstm_fcn_fit_one_cycle:v1',\n",
    "        'artifact_path': 'lstm_fcn_fit_one_cycle-v1/lstm_fcn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.LSTM_FCN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 13\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'mlstm_fcn_fit_one_cycle:v0',\n",
    "        'artifact_path': 'mlstm_fcn_fit_one_cycle-v0/mlstm_fcn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.MLSTM_FCN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 14\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'omniscalecnn_fit_one_cycle:v1',\n",
    "        'artifact_path': 'omniscalecnn_fit_one_cycle-v1/omniscalecnn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.OmniScaleCNN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 15\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'rescnn_fit_one_cycle:v1',\n",
    "        'artifact_path': 'rescnn_fit_one_cycle-v1/rescnn_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.ResCNN(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 16\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'resnet_fit_one_cycle:v0',\n",
    "        'artifact_path': 'resnet_fit_one_cycle-v0/resnet_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.ResNet(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 17\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'tsit_fit_one_cycle:v0',\n",
    "        'artifact_path': 'tsit_fit_one_cycle-v0/tsit_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.TSiT(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN,\n",
    "            use_token=False\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 18\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'tst_fit_one_cycle:v0',\n",
    "        'artifact_path': 'tst_fit_one_cycle-v0/tst_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.TST(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    },\n",
    "    { # 19\n",
    "        'artifact_type': 'model',\n",
    "        'artifact_name': 'xcm_fit_one_cycle:v0',\n",
    "        'artifact_path': 'xcm_fit_one_cycle-v0/xcm_fit_one_cycle.pth',\n",
    "        'model_instance': tsai.all.XCM(\n",
    "            c_in=len(input_features),\n",
    "            c_out=1,\n",
    "            seq_len=SEQ_LEN\n",
    "        ).to(DEVICE),\n",
    "        'model_type': 'tsai'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_model_input_preparation(train_df, valid_df, input_features, output_feature, batch_size=64):\n",
    "    x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "    x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
    "    y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "    y_valid = torch.FloatTensor(valid_df[output_feature].values)\n",
    "\n",
    "    training_set = FraudSequenceDataset(x_train, \n",
    "                                        y_train,train_df['CUSTOMER_ID'].values, \n",
    "                                        train_df['TX_DATETIME'].values,\n",
    "                                        SEQ_LEN,\n",
    "                                        padding_mode = \"zeros\")\n",
    "\n",
    "    valid_set = FraudSequenceDataset(x_valid, \n",
    "                                    y_valid,\n",
    "                                    valid_df['CUSTOMER_ID'].values, \n",
    "                                    valid_df['TX_DATETIME'].values,\n",
    "                                    SEQ_LEN,\n",
    "                                    padding_mode = \"zeros\")\n",
    "\n",
    "    training_generator, valid_generator = prepare_generators(training_set, valid_set, batch_size=batch_size)\n",
    "\n",
    "    return training_set, valid_set, training_generator, valid_generator\n",
    "\n",
    "def tsai_model_input_preparation(train_df, valid_df, input_features, output_feature, batch_size=64):\n",
    "    x_train, y_train = prepare_sequenced_X_y(train_df, SEQ_LEN, input_features, output_feature)\n",
    "    x_valid, y_valid = prepare_sequenced_X_y(valid_df, SEQ_LEN, input_features, output_feature)\n",
    "    X, y, splits = tsai.all.combine_split_data([x_train.numpy(), x_valid.numpy()], [y_train.numpy(), y_valid.numpy()])\n",
    "    dsets = tsai.all.TSDatasets(X, y, splits=splits, inplace=True)\n",
    "    dls = tsai.all.TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=batch_size, drop_last=False, device=DEVICE)\n",
    "    return dsets, dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(outlier, nearest_neighbors, inliers):\n",
    "    neighbors_index = nearest_neighbors.kneighbors([outlier], return_distance=False).tolist()[0]\n",
    "    return inliers[neighbors_index]\n",
    "\n",
    "def replicate_upsampling(outlier, context):\n",
    "    return np.array([outlier for _ in range(len(context))])\n",
    "\n",
    "def synthetic_upsampling(C_i_l, o_i, trim_small_values=True): # C_i_l.shape = (x, 5, 15), o_i.shape = (5, 15)\n",
    "    # transformation matrix for synthetic sampling\n",
    "    num_c0_new = C_i_l.shape[0] - 1\n",
    "    coeff_c0_new = np.random.rand(num_c0_new, C_i_l.shape[0]) \n",
    "\n",
    "    # use knn to find the closest distance between the outlier and its context\n",
    "    nbrs_local = KNeighborsTimeSeries(n_neighbors=1).fit(C_i_l) \n",
    "    min_dist_to_nbr = nbrs_local.kneighbors([o_i])[0][0, 0]\n",
    "    min_dist_to_nbr /= C_i_l.shape[2] # normalize distance\n",
    "\n",
    "    # normalize each row of the coeff_c0_new array so that the sum of each row is equal to 1\n",
    "    for r in range(coeff_c0_new.shape[0]):\n",
    "        coeff_c0_new[r, :] /= sum(coeff_c0_new[r, :])\n",
    "\n",
    "    # flatten last 2 dims of C_i_l otherwise it is impossible to receive insts_c0_new with\n",
    "    # (cluster_length - 1, 5, 15) shape out of dot product where C_i_l.shape = (cluster_length, 5, 15)\n",
    "    insts_c0_new = np.dot(coeff_c0_new,\n",
    "                          C_i_l.reshape(-1, C_i_l.shape[1] * C_i_l.shape[2]) - np.dot(np.ones((C_i_l.shape[0], 1)),\n",
    "                                                                                      [o_i.flatten()]))\n",
    "\n",
    "    for r in range(insts_c0_new.shape[0]):                      # shrink to prevent overlap\n",
    "        insts_c0_new[r, :] *= (0.2 * np.random.rand(1)[0] * min_dist_to_nbr)\n",
    "    insts_c0_new += np.dot(np.ones((num_c0_new, 1)), [o_i.flatten()])    # origin + shift\n",
    "\n",
    "    if trim_small_values:\n",
    "        insts_c0_new[insts_c0_new < 0.001] = 0\n",
    "\n",
    "    # bring back the 3d shape\n",
    "    insts_c0 = np.vstack(([o_i], insts_c0_new.reshape(-1, o_i.shape[0], o_i.shape[1])))        \n",
    "\n",
    "    return insts_c0\n",
    "\n",
    "def mixed_upsampling(C_i_l, o_i, trim_small_values=True, minimal_outlier_num=4):\n",
    "    # use ts coin sampling to generate outliers\n",
    "    O_i_l = synthetic_upsampling(C_i_l, o_i, trim_small_values=trim_small_values)\n",
    "    # context length must be bigger than number of outliers before SMOTE, otherwise ValueError is raised\n",
    "    minimal_outlier_num = min(minimal_outlier_num, C_i_l.shape[0] - 1)\n",
    "    # select minimal number of outliers and include the original outlier at index 0\n",
    "    outliers_subset = O_i_l[:minimal_outlier_num]\n",
    "    \n",
    "    # side note: shrinkage parameter in RandomOversampler does not influence upsampled outliers values\n",
    "    #\n",
    "    # k_neighbors must be set to minimal_outlier_num - 1, otherwise ValueError during fit_resample is raised\n",
    "    ROS = imblearn.over_sampling.SMOTE(sampling_strategy=1, k_neighbors=minimal_outlier_num - 1, random_state=SEED)\n",
    "    X = np.vstack((outliers_subset.reshape(-1, outliers_subset.shape[1] * outliers_subset.shape[2]),\n",
    "                   C_i_l.reshape(-1, C_i_l.shape[1] * C_i_l.shape[2])))\n",
    "    y = np.hstack((\n",
    "        np.ones((1, outliers_subset.shape[0])),\n",
    "        np.zeros((1, C_i_l.shape[0]))\n",
    "    )).T\n",
    "    X_resampled, Y_resampled = ROS.fit_resample(X, y) # X has to be 2d\n",
    "    O_i_indices = [i for i in range(len(Y_resampled)) if Y_resampled[i] == 1]\n",
    "    return X_resampled.reshape(-1, C_i_l.shape[1], C_i_l.shape[2])[O_i_indices]\n",
    "\n",
    "def cluster(C_i, increase_cluster_num=True):\n",
    "    kmeans = TimeSeriesKMeans(n_clusters=2, metric='dtw', random_state=SEED).fit(C_i)\n",
    "    best_silhouette_score = silhouette_score(C_i, kmeans.predict(C_i), metric='dtw')\n",
    "    L = 2\n",
    "    for n in range(3, 6):\n",
    "        new_kmeans = TimeSeriesKMeans(n_clusters=n, metric='dtw', random_state=SEED).fit(C_i)\n",
    "        new_silhouette_score = silhouette_score(C_i, kmeans.predict(C_i), metric='dtw')\n",
    "        if new_silhouette_score > best_silhouette_score:\n",
    "            best_silhouette_score = new_silhouette_score\n",
    "            kmeans = new_kmeans\n",
    "            L = n\n",
    "\n",
    "    if increase_cluster_num:\n",
    "        L += 1\n",
    "\n",
    "    C_i_l = np.array([C_i[kmeans.labels_ == i] for i in range(L)])\n",
    "    return C_i_l    \n",
    "\n",
    "def abandon_small_clusters(clustered, not_clustered, ratio):\n",
    "    return np.array([cluster_i for cluster_i in clustered if len(cluster_i) >= ratio * len(not_clustered)])\n",
    "\n",
    "def create_local_classifier(C_i_l, O_i_l):\n",
    "    clf = TimeSeriesSVC(kernel='linear', random_state=SEED, gamma=1.) # kernel='gak'\n",
    "    X = np.vstack((C_i_l, O_i_l))\n",
    "    y = np.hstack((np.zeros(C_i_l.shape[0]), np.ones(O_i_l.shape[0])))\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def calculate_all_abnormal_attribute_scores(context_results):\n",
    "    # LinearTimeSeriesSVC has 75 weights, 1 weight per feature for all sequence elements\n",
    "    abnormal_attribute_scores = np.zeros(context_results[0]['clf'].svm_estimator_.coef_[0].shape)\n",
    "    for context_cluster_results in context_results:\n",
    "        abnormal_attribute_scores += len(context_cluster_results['context_cluster']) * \\\n",
    "            np.abs(context_cluster_results['clf'].svm_estimator_.coef_[0])\n",
    "    abnormal_attribute_scores /= np.sum([len(context_result['context_cluster']) for context_result in context_results])\n",
    "    abnormal_attribute_scores /= np.sum(abnormal_attribute_scores)\n",
    "    return abnormal_attribute_scores\n",
    "\n",
    "def calculate_outlierness_score(context_results, outlier):\n",
    "    outlierness_score = 0\n",
    "    for context_cluster_results in context_results:\n",
    "        outlierness_score_cluster = abs(context_cluster_results['clf'].decision_function(outlier)) /\\\n",
    "            np.linalg.norm(context_cluster_results['clf'].svm_estimator_.coef_[0], ord=2)\n",
    "        outlierness_score += np.linalg.norm(len(context_cluster_results['context_cluster']) * \\\n",
    "            outlierness_score_cluster * context_cluster_results['clf'].svm_estimator_.coef_[0] /\\\n",
    "            np.linalg.norm(context_cluster_results['clf'].svm_estimator_.coef_[0]))\n",
    "    outlierness_score /= sum([len(context_result['context_cluster']) for context_result in context_results])\n",
    "    return outlierness_score\n",
    "\n",
    "def tscoin_for_outlier(o_i, inliers, upsampling_technique, abandon_ratio, nearest_neighbors):\n",
    "    C_i = get_context(o_i, nearest_neighbors, inliers)\n",
    "    C_i_clusters = cluster(C_i)\n",
    "    C_i_clusters = abandon_small_clusters(C_i_clusters, C_i, abandon_ratio)\n",
    "    context_results = []\n",
    "    for C_i_l in C_i_clusters:\n",
    "        if upsampling_technique == 'synthetic':\n",
    "            O_i_l = synthetic_upsampling(C_i_l, o_i)\n",
    "        elif upsampling_technique == 'replicate':\n",
    "            O_i_l = replicate_upsampling(o_i, C_i_l)\n",
    "        else:\n",
    "            O_i_l = mixed_upsampling(C_i_l, o_i)\n",
    "        clf = create_local_classifier(C_i_l, O_i_l)\n",
    "        context_results.append({\n",
    "            'clf': clf, \n",
    "            'context_cluster': C_i_l,\n",
    "            'upsampled_outlier': O_i_l,\n",
    "            })\n",
    "    abnormal_attributes_scores = calculate_all_abnormal_attribute_scores(context_results)\n",
    "    outlierness_score = calculate_outlierness_score(context_results, o_i.reshape(1, o_i.shape[0], o_i.shape[1]))\n",
    "    return abnormal_attributes_scores, outlierness_score\n",
    "\n",
    "def tscoin(inliers, outliers, upsampling_technique='synthetic', abandon_ratio=0.05):\n",
    "    tscoin_start_time=time.time()\n",
    "    context_neighbors = int(np.sqrt(len(inliers)))\n",
    "    nearest_neighbors = KNeighborsTimeSeries(n_neighbors=context_neighbors, metric='dtw') #  metric='euclidean'\n",
    "    # n_jobs=-1 doesn't seem to decrease execution time\n",
    "    knn_start_time=time.time()\n",
    "    # array of shape (n_ts, sz, d) <=> (num_samples, seq_len, num_features)\n",
    "    nearest_neighbors = nearest_neighbors.fit(inliers)\n",
    "    knn_end_time = time.time()\n",
    "    logging.info(f'KNeighbors fitting time: {round(knn_end_time - knn_start_time, 2)} seconds')\n",
    "    parallel_job_results = Parallel(n_jobs=-2)(delayed(tscoin_for_outlier)(o_i, inliers, upsampling_technique, abandon_ratio,\n",
    "                                                                           nearest_neighbors) for o_i in outliers)\n",
    "    tscoin_end_time = time.time()\n",
    "    outlierness_score_list = [item[0] for item in parallel_job_results]\n",
    "    abnormal_attributes_scores_list = [item[1] for item in parallel_job_results]\n",
    "    logging.info(f'TS COIN end, execution time: {round(tscoin_end_time - tscoin_start_time, 2)} seconds')\n",
    "    return outlierness_score_list, abnormal_attributes_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tscoin_outliers = True\n",
    "calculate_shap_outliers = True\n",
    "save_predictions = True\n",
    "save_explainers_expected_value = True\n",
    "generate_outliers_summary_plots_flattened = True\n",
    "generate_outliers_summary_plots_timestep_aggregated = True\n",
    "calculate_tscoin_inliers_sample = True\n",
    "calculate_shap_inliers_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('START')\n",
    "for model_index in tqdm(range(len(models))):\n",
    "    model_type = models[model_index]['model_type']\n",
    "    logging.info(f'NEW MODEL: {model_type} {models[model_index][\"artifact_name\"].split(\":\")[0]}')\n",
    "    if model_type == 'handbook':\n",
    "        torch_train_set, torch_valid_set, torch_train_generator, torch_valid_generator = \\\n",
    "            pytorch_model_input_preparation(train_df, valid_df, input_features, output_feature)\n",
    "    elif model_type == 'tsai':\n",
    "        tsai_dsets, tsai_dls = tsai_model_input_preparation(train_df, valid_df, input_features, output_feature)\n",
    "\n",
    "    # uncomment if 1st time running particular models\n",
    "    #\n",
    "    # run = wandb.init()\n",
    "    # artifact = run.use_artifact(f'mgr-anomaly-tsxai/mgr-anomaly-tsxai-project/{models[model_index][\"artifact_name\"]}',\n",
    "    #                             type=models[model_index]['artifact_type'])\n",
    "    # artifact_dir = artifact.download()\n",
    "    model = models[model_index]['model_instance']\n",
    "    model.load_state_dict(torch.load(f'artifacts/{models[model_index][\"artifact_path\"]}'))\n",
    "    model.eval()\n",
    "    logging.info('Weights loaded successfully')\n",
    "    valid_predictions = get_predictions_sequential(model, torch_valid_generator if model_type == 'handbook'\n",
    "                                                   else tsai_dls.valid)\n",
    "    if model_type == 'tsai':\n",
    "        valid_predictions = torch.nn.Sigmoid()(torch.FloatTensor(valid_predictions)).numpy()\n",
    "    valid_predictions_int = np.round(valid_predictions).astype(int)\n",
    "    logging.info('Get predictions finished')\n",
    "    if save_predictions:\n",
    "        np.save(\n",
    "            f'generator_output/outliers_indices/{model_type}/{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "            np.where(valid_predictions_int == 1)[0]\n",
    "            )\n",
    "    if model_type == 'handbook':\n",
    "        torch_valid_set.output = False\n",
    "        predicted_inliers = torch_valid_set.features[torch_valid_set.sequences_ids[valid_predictions_int == 0],:]\n",
    "        predicted_outliers = torch_valid_set.features[torch_valid_set.sequences_ids[valid_predictions_int == 1],:]\n",
    "        torch_valid_set.output = True\n",
    "    elif model_type == 'tsai':\n",
    "        predicted_inliers = tsai_dls.valid.dataset[valid_predictions_int == 0][0]\n",
    "        predicted_outliers = tsai_dls.valid.dataset[valid_predictions_int == 1][0]\n",
    "        predicted_inliers = predicted_inliers.reshape(predicted_inliers.shape[0], predicted_inliers.shape[2],\n",
    "                                                      predicted_inliers.shape[1])\n",
    "        predicted_outliers = predicted_outliers.reshape(predicted_outliers.shape[0], predicted_outliers.shape[2],\n",
    "                                                        predicted_outliers.shape[1])\n",
    "        \n",
    "    if calculate_tscoin_outliers:\n",
    "        for upsampling_technique in ['synthetic', 'replicate', 'mixed']:\n",
    "            logging.info(f'TS COIN on outliers start, upsampling technique: {upsampling_technique}')\n",
    "            outlierness_score, abnormal_attributes_scores = tscoin(predicted_inliers.numpy(),\n",
    "                                                                   predicted_outliers.numpy()[:10], upsampling_technique)\n",
    "            logging.info('TS COIN on outliers finished')\n",
    "            np.save(\n",
    "                f'generator_output/outliers_attribute_scores/{model_type}/{upsampling_technique}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "                np.array(abnormal_attributes_scores)\n",
    "                )\n",
    "            np.save(\n",
    "                f'generator_output/outliers_outlierness_scores/{model_type}/{upsampling_technique}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "                np.array(outlierness_score)\n",
    "                )\n",
    "    else:\n",
    "        logging.info('TS COIN on outliers skipped')\n",
    "\n",
    "    if calculate_tscoin_inliers_sample:\n",
    "        for upsampling_technique in ['synthetic', 'replicate', 'mixed']:\n",
    "            logging.info(f'TS COIN on inliers sample start, upsampling technique: {upsampling_technique}')\n",
    "            outlierness_score, abnormal_attributes_scores = tscoin(predicted_inliers[len(predicted_outliers):].numpy(),\n",
    "                                                                   predicted_inliers[:len(predicted_outliers)].numpy(),\n",
    "                                                                   upsampling_technique)\n",
    "            logging.info('TS COIN on inliers sample finished')\n",
    "            np.save(\n",
    "                f'generator_output/inliers_attribute_scores/{model_type}/{upsampling_technique}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy', \n",
    "                np.array(abnormal_attributes_scores)\n",
    "                )\n",
    "            np.save(\n",
    "                f'generator_output/inliers_outlierness_scores/{model_type}/{upsampling_technique}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "                np.array(outlierness_score))\n",
    "    else:\n",
    "        logging.info('TS COIN on inliers sample skipped')\n",
    "\n",
    "    if calculate_shap_outliers or calculate_shap_inliers_sample or save_explainers_expected_value:\n",
    "        if model_type == 'handbook':\n",
    "            _, _, torch_shap_generator, _ = pytorch_model_input_preparation(train_df, valid_df, input_features,\n",
    "                                                                            output_feature, 1000)\n",
    "        elif model_type == 'tsai':\n",
    "            _, tsai_shap_dls = tsai_model_input_preparation(train_df, valid_df, input_features, output_feature, 1000)\n",
    "\n",
    "        model.train()\n",
    "        if model_type == 'handbook':\n",
    "            shap_background_data = next(iter(torch_shap_generator))[0]\n",
    "        elif model_type == 'tsai':\n",
    "            shap_background_data = next(iter(tsai_shap_dls.train))[0]\n",
    "        try:\n",
    "            explainer = shap.DeepExplainer(model, shap_background_data)\n",
    "        except Exception as e:\n",
    "            logging.error('SHAP background data step failed')\n",
    "            logging.error(f'Exception: {e}')\n",
    "            logging.info(f'SHAP background data on {shap_background_data.device}')\n",
    "        else:\n",
    "            logging.info('SHAP background data step finished')\n",
    "            if save_explainers_expected_value:\n",
    "                np.save(\n",
    "                    f'generator_output/shap_explainer_expected_value/{model_type}/'\n",
    "                    f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "                    explainer.expected_value\n",
    "                    )\n",
    "        if model_type == 'handbook':\n",
    "            shap_values_dataset = torch_valid_set\n",
    "        elif model_type == 'tsai':\n",
    "            shap_values_dataset = tsai_dsets.valid\n",
    "\n",
    "        if calculate_shap_outliers:\n",
    "            model.train()\n",
    "            try:\n",
    "                outliers_shap_values = explainer.shap_values(predicted_outliers.to(DEVICE)\\\n",
    "                                                             .reshape(predicted_outliers.shape[0], len(input_features),\n",
    "                                                                      SEQ_LEN))\n",
    "                clear_output(wait=True)\n",
    "            except Exception as e:\n",
    "                logging.error('SHAP values generation on outliers failed')\n",
    "                logging.error(f'Exception: {e}')\n",
    "                logging.info(f'SHAP values on {predicted_outliers.to(DEVICE).device}')\n",
    "            else:\n",
    "                logging.info('SHAP values generation on outliers finished')\n",
    "                np.save(\n",
    "                    f'generator_output/outliers_shap_values/{model_type}/'\n",
    "                    f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy', \n",
    "                    np.array(outliers_shap_values)\n",
    "                    )\n",
    "            model.eval()\n",
    "        else:\n",
    "            logging.info('SHAP on outliers skipped')\n",
    "\n",
    "        if calculate_shap_inliers_sample:\n",
    "            model.train()\n",
    "            try:\n",
    "                inliers_shap_values = explainer.shap_values(predicted_inliers[:len(predicted_outliers)].to(DEVICE)\\\n",
    "                                                            .reshape(predicted_inliers[:len(predicted_outliers)]\\\n",
    "                                                                     .shape[0], len(input_features), SEQ_LEN))\n",
    "                clear_output(wait=True)\n",
    "            except Exception as e:\n",
    "                logging.error('SHAP values generation on inliers sample failed')\n",
    "                logging.error(f'Exception: {e}')\n",
    "                logging.info(f'SHAP values on {predicted_inliers.to(DEVICE).device}')\n",
    "            else:\n",
    "                logging.info('SHAP values generation on inliers finished')\n",
    "                np.save(\n",
    "                    f'generator_output/inliers_shap_values/{model_type}/'\n",
    "                    f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy',\n",
    "                    np.array(inliers_shap_values)\n",
    "                    )\n",
    "            model.eval()\n",
    "        else:\n",
    "            logging.info('SHAP on inliers sample skipped')\n",
    "    else:\n",
    "        logging.info('SHAP on outliers skipped')\n",
    "        logging.info('SHAP on inliers sample skipped')\n",
    "\n",
    "    if generate_outliers_summary_plots_flattened:\n",
    "        try:\n",
    "            outliers_shap_values = np.load(\n",
    "                f'generator_output/outliers_shap_values/{model_type}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logging.error('Outliers SHAP values load failed')\n",
    "            logging.error(f'Exception: {e}')\n",
    "        else:\n",
    "            outliers_flattened_shap_values = outliers_shap_values.reshape(outliers_shap_values.shape[0], -1)\n",
    "            shap.summary_plot(outliers_flattened_shap_values,\n",
    "                              feature_names=[f'{input_feature} (t-{t})'\n",
    "                                             for t in range(SEQ_LEN - 1, -1, -1)\n",
    "                                             for input_feature in input_features],\n",
    "                                             features=predicted_outliers.reshape(predicted_outliers.shape[0], -1),\n",
    "                                             max_display=30, sort=True, show=False)\n",
    "            plt.savefig(\n",
    "                f'generator_output/outliers_summary_plots_flattened/{model_type}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.png'\n",
    "                )\n",
    "            # clear plot each time to prevent overlapping\n",
    "            plt.clf()\n",
    "            logging.info('Outliers summary plot flattened generation finished')\n",
    "    else:\n",
    "        logging.info('Outliers summary plot flattened skipped')\n",
    "    \n",
    "    if generate_outliers_summary_plots_timestep_aggregated:\n",
    "        try:\n",
    "            outliers_shap_values = np.load(\n",
    "                f'generator_output/outliers_shap_values/{model_type}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.npy'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logging.error('Outliers SHAP values load failed')\n",
    "            logging.error(f'Exception: {e}')\n",
    "        else:\n",
    "            shap_features = np.array([np.sum(predicted_outlier, axis=1) for predicted_outlier in predicted_outliers\\\n",
    "                                      .reshape(predicted_outliers.shape[0], predicted_outliers.shape[2],\n",
    "                                               predicted_outliers.shape[1]).numpy()])\n",
    "            outliers_aggregated_shap_values = np.array([np.sum(outlier_shap_values, axis=1) for outlier_shap_values\n",
    "                                                        in outliers_shap_values])\n",
    "            shap.summary_plot(outliers_aggregated_shap_values, feature_names=input_features, features=shap_features,\n",
    "                              show=False)\n",
    "            plt.savefig(\n",
    "                f'generator_output/outliers_summary_plots_timestep_aggregated/{model_type}/'\n",
    "                f'{models[model_index][\"artifact_name\"].split(\":\")[0]}.png'\n",
    "                )\n",
    "            plt.clf()\n",
    "            logging.info('Outliers summary plot timestep aggregated generation finished')\n",
    "    else:\n",
    "        logging.info('Outliers summary plot timestep aggregated skipped')\n",
    "\n",
    "logging.info('END')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
