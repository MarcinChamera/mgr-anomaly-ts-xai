{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "\n",
    "@book{leborgne2022fraud,\n",
    "\n",
    "title={Reproducible Machine Learning for Credit Card Fraud Detection - Practical Handbook},\n",
    "\n",
    "author={Le Borgne, Yann-A{\\\"e}l and Siblini, Wissam and Lebichot, Bertrand and Bontempi, Gianluca},\n",
    "\n",
    "url={https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook},\n",
    "\n",
    "year={2022},\n",
    "\n",
    "publisher={Universit{\\'e} Libre de Bruxelles}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covered subchapters:\n",
    "* 7.2.3+ Feed-forward neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO został pierwszy model earlystopping do wytrenowania na W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from skorch import NeuralNetClassifier\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 63257  100 63257    0     0   154k      0 --:--:-- --:--:-- --:--:--  155k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.py\n",
    "%run shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run my_shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 809 ms\n",
      "919767 transactions loaded, containing 8195 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT = '../fraud-detection-handbook/simulated-data-transformed/data/'\n",
    "\n",
    "BEGIN_DATE = \"2018-06-11\"\n",
    "END_DATE = \"2018-09-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "output_feature=\"TX_FRAUD\"\n",
    "\n",
    "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_training = datetime.datetime.strptime(\"2018-07-25\", \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7\n",
    "delta_valid = delta_test\n",
    "\n",
    "start_date_training_with_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "\n",
    "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training,\n",
    "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
    "\n",
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_valid = torch.FloatTensor(valid_df[output_feature].values)\n",
    "\n",
    "training_set = FraudDatasetToDevice(x_train, y_train, DEVICE)\n",
    "valid_set = FraudDatasetToDevice(x_valid, y_valid, DEVICE)\n",
    "\n",
    "training_generator,valid_generator = prepare_generators(training_set,valid_set,batch_size=64)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983141728b0446adbf20537804dc9645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_201538-14evlvba</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/14evlvba\" target=\"_blank\">electric-sky-69</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='sgd',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=500,\n",
    "    scale=True\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-ts-xai\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.18642424632547924\n",
      "valid loss: 0.09570385298756096\n",
      "New best score: 0.09570385298756096\n",
      "\n",
      "Epoch 1: train loss: 0.09381635585971722\n",
      "valid loss: 0.06922256800169217\n",
      "New best score: 0.06922256800169217\n",
      "\n",
      "Epoch 2: train loss: 0.07276558290660154\n",
      "valid loss: 0.055213734098637536\n",
      "New best score: 0.055213734098637536\n",
      "\n",
      "Epoch 3: train loss: 0.06264895755446717\n",
      "valid loss: 0.048663044495571454\n",
      "New best score: 0.048663044495571454\n",
      "\n",
      "Epoch 4: train loss: 0.05774921684975175\n",
      "valid loss: 0.04489710263327239\n",
      "New best score: 0.04489710263327239\n",
      "\n",
      "Epoch 5: train loss: 0.05460530012530531\n",
      "valid loss: 0.04248667272517272\n",
      "New best score: 0.04248667272517272\n",
      "\n",
      "Epoch 6: train loss: 0.05225943442572309\n",
      "valid loss: 0.040681906911400355\n",
      "New best score: 0.040681906911400355\n",
      "\n",
      "Epoch 7: train loss: 0.0503840179871112\n",
      "valid loss: 0.03929845856361574\n",
      "New best score: 0.03929845856361574\n",
      "\n",
      "Epoch 8: train loss: 0.04878798552871204\n",
      "valid loss: 0.038220367122453795\n",
      "New best score: 0.038220367122453795\n",
      "\n",
      "Epoch 9: train loss: 0.047366804891681526\n",
      "valid loss: 0.03727523159873636\n",
      "New best score: 0.03727523159873636\n",
      "\n",
      "Epoch 10: train loss: 0.04610713655226549\n",
      "valid loss: 0.03645069052748439\n",
      "New best score: 0.03645069052748439\n",
      "\n",
      "Epoch 11: train loss: 0.04495479661411297\n",
      "valid loss: 0.035776861592141745\n",
      "New best score: 0.035776861592141745\n",
      "\n",
      "Epoch 12: train loss: 0.04391288617049848\n",
      "valid loss: 0.035094262131425406\n",
      "New best score: 0.035094262131425406\n",
      "\n",
      "Epoch 13: train loss: 0.042912311466511775\n",
      "valid loss: 0.03453659709455071\n",
      "New best score: 0.03453659709455071\n",
      "\n",
      "Epoch 14: train loss: 0.04201154673665223\n",
      "valid loss: 0.03401482802622519\n",
      "New best score: 0.03401482802622519\n",
      "\n",
      "Epoch 15: train loss: 0.04122357160122365\n",
      "valid loss: 0.03358762741189053\n",
      "New best score: 0.03358762741189053\n",
      "\n",
      "Epoch 16: train loss: 0.040399551325141815\n",
      "valid loss: 0.033154370380784295\n",
      "New best score: 0.033154370380784295\n",
      "\n",
      "Epoch 17: train loss: 0.0396674395965095\n",
      "valid loss: 0.03276893284640037\n",
      "New best score: 0.03276893284640037\n",
      "\n",
      "Epoch 18: train loss: 0.03898883128067307\n",
      "valid loss: 0.0323789675533976\n",
      "New best score: 0.0323789675533976\n",
      "\n",
      "Epoch 19: train loss: 0.03837233934361846\n",
      "valid loss: 0.03200849381139508\n",
      "New best score: 0.03200849381139508\n",
      "\n",
      "Epoch 20: train loss: 0.037757067914988375\n",
      "valid loss: 0.0316364096966291\n",
      "New best score: 0.0316364096966291\n",
      "\n",
      "Epoch 21: train loss: 0.037190497712410286\n",
      "valid loss: 0.0313369292125891\n",
      "New best score: 0.0313369292125891\n",
      "\n",
      "Epoch 22: train loss: 0.036643483088955243\n",
      "valid loss: 0.031026588739064633\n",
      "New best score: 0.031026588739064633\n",
      "\n",
      "Epoch 23: train loss: 0.036139996092302194\n",
      "valid loss: 0.03076994803639049\n",
      "New best score: 0.03076994803639049\n",
      "\n",
      "Epoch 24: train loss: 0.0356861949915139\n",
      "valid loss: 0.03048822548880388\n",
      "New best score: 0.03048822548880388\n",
      "\n",
      "Epoch 25: train loss: 0.035257889904687936\n",
      "valid loss: 0.030160199298240188\n",
      "New best score: 0.030160199298240188\n",
      "\n",
      "Epoch 26: train loss: 0.03485448427540875\n",
      "valid loss: 0.0298974184747724\n",
      "New best score: 0.0298974184747724\n",
      "\n",
      "Epoch 27: train loss: 0.034477825417005226\n",
      "valid loss: 0.029637016583966044\n",
      "New best score: 0.029637016583966044\n",
      "\n",
      "Epoch 28: train loss: 0.03412909742992382\n",
      "valid loss: 0.02937452470511607\n",
      "New best score: 0.02937452470511607\n",
      "\n",
      "Epoch 29: train loss: 0.03380329065870221\n",
      "valid loss: 0.02912267711847759\n",
      "New best score: 0.02912267711847759\n",
      "\n",
      "Epoch 30: train loss: 0.03349705454986454\n",
      "valid loss: 0.028897105183035267\n",
      "New best score: 0.028897105183035267\n",
      "\n",
      "Epoch 31: train loss: 0.033215835851330935\n",
      "valid loss: 0.028677733841993153\n",
      "New best score: 0.028677733841993153\n",
      "\n",
      "Epoch 32: train loss: 0.03293270138105631\n",
      "valid loss: 0.028449804556344428\n",
      "New best score: 0.028449804556344428\n",
      "\n",
      "Epoch 33: train loss: 0.03271796911348767\n",
      "valid loss: 0.0282679153979438\n",
      "New best score: 0.0282679153979438\n",
      "\n",
      "Epoch 34: train loss: 0.032448975404336354\n",
      "valid loss: 0.0280762855341795\n",
      "New best score: 0.0280762855341795\n",
      "\n",
      "Epoch 35: train loss: 0.03223248126586061\n",
      "valid loss: 0.027891730281915904\n",
      "New best score: 0.027891730281915904\n",
      "\n",
      "Epoch 36: train loss: 0.03200539700300579\n",
      "valid loss: 0.027704269313442588\n",
      "New best score: 0.027704269313442588\n",
      "\n",
      "Epoch 37: train loss: 0.03184455145791981\n",
      "valid loss: 0.027536419790716582\n",
      "New best score: 0.027536419790716582\n",
      "\n",
      "Epoch 38: train loss: 0.031613160136973684\n",
      "valid loss: 0.02737271251886854\n",
      "New best score: 0.02737271251886854\n",
      "\n",
      "Epoch 39: train loss: 0.0314322058430436\n",
      "valid loss: 0.027228526137237623\n",
      "New best score: 0.027228526137237623\n",
      "\n",
      "Epoch 40: train loss: 0.031255673299890034\n",
      "valid loss: 0.027088161808889438\n",
      "New best score: 0.027088161808889438\n",
      "\n",
      "Epoch 41: train loss: 0.031090503405659088\n",
      "valid loss: 0.02694703684710825\n",
      "New best score: 0.02694703684710825\n",
      "\n",
      "Epoch 42: train loss: 0.030933457160887835\n",
      "valid loss: 0.02680355019797903\n",
      "New best score: 0.02680355019797903\n",
      "\n",
      "Epoch 43: train loss: 0.03080063949088317\n",
      "valid loss: 0.026677000930821104\n",
      "New best score: 0.026677000930821104\n",
      "\n",
      "Epoch 44: train loss: 0.030633690080239476\n",
      "valid loss: 0.026541338160605416\n",
      "New best score: 0.026541338160605416\n",
      "\n",
      "Epoch 45: train loss: 0.030498022000907038\n",
      "valid loss: 0.026417409861613676\n",
      "New best score: 0.026417409861613676\n",
      "\n",
      "Epoch 46: train loss: 0.030379768116727515\n",
      "valid loss: 0.026302857057658627\n",
      "New best score: 0.026302857057658627\n",
      "\n",
      "Epoch 47: train loss: 0.03023467306490743\n",
      "valid loss: 0.026196376169069734\n",
      "New best score: 0.026196376169069734\n",
      "\n",
      "Epoch 48: train loss: 0.0301098074625777\n",
      "valid loss: 0.02608170430968249\n",
      "New best score: 0.02608170430968249\n",
      "\n",
      "Epoch 49: train loss: 0.02999530012075117\n",
      "valid loss: 0.025975046440668642\n",
      "New best score: 0.025975046440668642\n",
      "\n",
      "Epoch 50: train loss: 0.02994209298550317\n",
      "valid loss: 0.025877537409254605\n",
      "New best score: 0.025877537409254605\n",
      "\n",
      "Epoch 51: train loss: 0.029777372202753567\n",
      "valid loss: 0.025781390525024428\n",
      "New best score: 0.025781390525024428\n",
      "\n",
      "Epoch 52: train loss: 0.029666862378320474\n",
      "valid loss: 0.025681753242367922\n",
      "New best score: 0.025681753242367922\n",
      "\n",
      "Epoch 53: train loss: 0.029566774181543107\n",
      "valid loss: 0.025595256940918666\n",
      "New best score: 0.025595256940918666\n",
      "\n",
      "Epoch 54: train loss: 0.029462852613197605\n",
      "valid loss: 0.025511935125815587\n",
      "New best score: 0.025511935125815587\n",
      "\n",
      "Epoch 55: train loss: 0.029371156626169195\n",
      "valid loss: 0.025419860540017176\n",
      "New best score: 0.025419860540017176\n",
      "\n",
      "Epoch 56: train loss: 0.029276499080318395\n",
      "valid loss: 0.02534175091438429\n",
      "New best score: 0.02534175091438429\n",
      "\n",
      "Epoch 57: train loss: 0.02919050917555742\n",
      "valid loss: 0.025259614278052234\n",
      "New best score: 0.025259614278052234\n",
      "\n",
      "Epoch 58: train loss: 0.02910261360448134\n",
      "valid loss: 0.025181173695543724\n",
      "New best score: 0.025181173695543724\n",
      "\n",
      "Epoch 59: train loss: 0.02908191741971418\n",
      "valid loss: 0.025104221553591814\n",
      "New best score: 0.025104221553591814\n",
      "\n",
      "Epoch 60: train loss: 0.028936990650173997\n",
      "valid loss: 0.025034321125230775\n",
      "New best score: 0.025034321125230775\n",
      "\n",
      "Epoch 61: train loss: 0.02886316629386304\n",
      "valid loss: 0.024962476073865474\n",
      "New best score: 0.024962476073865474\n",
      "\n",
      "Epoch 62: train loss: 0.028782984840645537\n",
      "valid loss: 0.02489911371169348\n",
      "New best score: 0.02489911371169348\n",
      "\n",
      "Epoch 63: train loss: 0.028707516318732973\n",
      "valid loss: 0.024830173057249445\n",
      "New best score: 0.024830173057249445\n",
      "\n",
      "Epoch 64: train loss: 0.02863251603807506\n",
      "valid loss: 0.024761779718898425\n",
      "New best score: 0.024761779718898425\n",
      "\n",
      "Epoch 65: train loss: 0.028603430788434126\n",
      "valid loss: 0.02470178480932753\n",
      "New best score: 0.02470178480932753\n",
      "\n",
      "Epoch 66: train loss: 0.028498735237196725\n",
      "valid loss: 0.02464270303853911\n",
      "New best score: 0.02464270303853911\n",
      "\n",
      "Epoch 67: train loss: 0.02842607084714004\n",
      "valid loss: 0.024583060718136384\n",
      "New best score: 0.024583060718136384\n",
      "\n",
      "Epoch 68: train loss: 0.028363393687002438\n",
      "valid loss: 0.02452471538965154\n",
      "New best score: 0.02452471538965154\n",
      "\n",
      "Epoch 69: train loss: 0.028297161677680364\n",
      "valid loss: 0.02446769317458706\n",
      "New best score: 0.02446769317458706\n",
      "\n",
      "Epoch 70: train loss: 0.028234523703316698\n",
      "valid loss: 0.024412781592053654\n",
      "New best score: 0.024412781592053654\n",
      "\n",
      "Epoch 71: train loss: 0.028174227260435968\n",
      "valid loss: 0.024360726148393068\n",
      "New best score: 0.024360726148393068\n",
      "\n",
      "Epoch 72: train loss: 0.028111750599161697\n",
      "valid loss: 0.02431224207506279\n",
      "New best score: 0.02431224207506279\n",
      "\n",
      "Epoch 73: train loss: 0.028056705762436355\n",
      "valid loss: 0.024257555523474264\n",
      "New best score: 0.024257555523474264\n",
      "\n",
      "Epoch 74: train loss: 0.028002186038342616\n",
      "valid loss: 0.024210599995331695\n",
      "New best score: 0.024210599995331695\n",
      "\n",
      "Epoch 75: train loss: 0.027945094431292346\n",
      "valid loss: 0.024158125954632095\n",
      "New best score: 0.024158125954632095\n",
      "\n",
      "Epoch 76: train loss: 0.027889055545290278\n",
      "valid loss: 0.024113574177466342\n",
      "New best score: 0.024113574177466342\n",
      "\n",
      "Epoch 77: train loss: 0.02784044973305883\n",
      "valid loss: 0.02406711986903858\n",
      "New best score: 0.02406711986903858\n",
      "\n",
      "Epoch 78: train loss: 0.02778577738504442\n",
      "valid loss: 0.02402170111770761\n",
      "New best score: 0.02402170111770761\n",
      "\n",
      "Epoch 79: train loss: 0.027734552781238486\n",
      "valid loss: 0.02397444979011661\n",
      "New best score: 0.02397444979011661\n",
      "\n",
      "Epoch 80: train loss: 0.027684484458050836\n",
      "valid loss: 0.023932626185188342\n",
      "New best score: 0.023932626185188342\n",
      "\n",
      "Epoch 81: train loss: 0.027637337968480037\n",
      "valid loss: 0.023888108947975755\n",
      "New best score: 0.023888108947975755\n",
      "\n",
      "Epoch 82: train loss: 0.027588702809618525\n",
      "valid loss: 0.02384648133405616\n",
      "New best score: 0.02384648133405616\n",
      "\n",
      "Epoch 83: train loss: 0.027543696162594812\n",
      "valid loss: 0.023805229613581995\n",
      "New best score: 0.023805229613581995\n",
      "\n",
      "Epoch 84: train loss: 0.027500285525991448\n",
      "valid loss: 0.023765289776474973\n",
      "New best score: 0.023765289776474973\n",
      "\n",
      "Epoch 85: train loss: 0.02745385737589017\n",
      "valid loss: 0.02373005689375315\n",
      "New best score: 0.02373005689375315\n",
      "\n",
      "Epoch 86: train loss: 0.027409458240285005\n",
      "valid loss: 0.023692009221705168\n",
      "New best score: 0.023692009221705168\n",
      "\n",
      "Epoch 87: train loss: 0.0273696111451363\n",
      "valid loss: 0.023652475248791202\n",
      "New best score: 0.023652475248791202\n",
      "\n",
      "Epoch 88: train loss: 0.027321811602384446\n",
      "valid loss: 0.02361561587713168\n",
      "New best score: 0.02361561587713168\n",
      "\n",
      "Epoch 89: train loss: 0.02728665359211921\n",
      "valid loss: 0.02357677618551766\n",
      "New best score: 0.02357677618551766\n",
      "\n",
      "Epoch 90: train loss: 0.027243670321368284\n",
      "valid loss: 0.02354471427802524\n",
      "New best score: 0.02354471427802524\n",
      "\n",
      "Epoch 91: train loss: 0.027204758517548697\n",
      "valid loss: 0.023510190126926737\n",
      "New best score: 0.023510190126926737\n",
      "\n",
      "Epoch 92: train loss: 0.027184970666043865\n",
      "valid loss: 0.02347618762713486\n",
      "New best score: 0.02347618762713486\n",
      "\n",
      "Epoch 93: train loss: 0.027128193979164838\n",
      "valid loss: 0.02344051238535837\n",
      "New best score: 0.02344051238535837\n",
      "\n",
      "Epoch 94: train loss: 0.027106599708563595\n",
      "valid loss: 0.02340938976140842\n",
      "New best score: 0.02340938976140842\n",
      "\n",
      "Epoch 95: train loss: 0.027046362589424448\n",
      "valid loss: 0.023377246501202296\n",
      "New best score: 0.023377246501202296\n",
      "\n",
      "Epoch 96: train loss: 0.02701559467582328\n",
      "valid loss: 0.023345236246331988\n",
      "New best score: 0.023345236246331988\n",
      "\n",
      "Epoch 97: train loss: 0.027004278725505825\n",
      "valid loss: 0.023315920934089258\n",
      "New best score: 0.023315920934089258\n",
      "\n",
      "Epoch 98: train loss: 0.026946303341404795\n",
      "valid loss: 0.023285357450246482\n",
      "New best score: 0.023285357450246482\n",
      "\n",
      "Epoch 99: train loss: 0.026902645563823723\n",
      "valid loss: 0.023260102160816806\n",
      "New best score: 0.023260102160816806\n",
      "\n",
      "Epoch 100: train loss: 0.026877907373066053\n",
      "valid loss: 0.023229007467343323\n",
      "New best score: 0.023229007467343323\n",
      "\n",
      "Epoch 101: train loss: 0.026845937000621257\n",
      "valid loss: 0.023197135898518722\n",
      "New best score: 0.023197135898518722\n",
      "\n",
      "Epoch 102: train loss: 0.02681269786554565\n",
      "valid loss: 0.023171809621556006\n",
      "New best score: 0.023171809621556006\n",
      "\n",
      "Epoch 103: train loss: 0.02682063580321398\n",
      "valid loss: 0.02314574412007854\n",
      "New best score: 0.02314574412007854\n",
      "\n",
      "Epoch 104: train loss: 0.02674734627351264\n",
      "valid loss: 0.023117638893351857\n",
      "New best score: 0.023117638893351857\n",
      "\n",
      "Epoch 105: train loss: 0.026715778811062935\n",
      "valid loss: 0.023091339106230898\n",
      "New best score: 0.023091339106230898\n",
      "\n",
      "Epoch 106: train loss: 0.026684021901341066\n",
      "valid loss: 0.023066842048832765\n",
      "New best score: 0.023066842048832765\n",
      "\n",
      "Epoch 107: train loss: 0.026656578721161346\n",
      "valid loss: 0.02303775147873291\n",
      "New best score: 0.02303775147873291\n",
      "\n",
      "Epoch 108: train loss: 0.026623378246384614\n",
      "valid loss: 0.023015050791279627\n",
      "New best score: 0.023015050791279627\n",
      "\n",
      "Epoch 109: train loss: 0.026596662824792693\n",
      "valid loss: 0.022989791752279835\n",
      "New best score: 0.022989791752279835\n",
      "\n",
      "Epoch 110: train loss: 0.02656890714756407\n",
      "valid loss: 0.022964643409811\n",
      "New best score: 0.022964643409811\n",
      "\n",
      "Epoch 111: train loss: 0.02659942564659962\n",
      "valid loss: 0.022942212766289924\n",
      "New best score: 0.022942212766289924\n",
      "\n",
      "Epoch 112: train loss: 0.02650888628555864\n",
      "valid loss: 0.022919404236078425\n",
      "New best score: 0.022919404236078425\n",
      "\n",
      "Epoch 113: train loss: 0.02648512972186503\n",
      "valid loss: 0.02289482843668561\n",
      "New best score: 0.02289482843668561\n",
      "\n",
      "Epoch 114: train loss: 0.026455053954618273\n",
      "valid loss: 0.022874409148856\n",
      "New best score: 0.022874409148856\n",
      "\n",
      "Epoch 115: train loss: 0.026429220715929336\n",
      "valid loss: 0.022852105358165703\n",
      "New best score: 0.022852105358165703\n",
      "\n",
      "Epoch 116: train loss: 0.026401991923813744\n",
      "valid loss: 0.022826740254456857\n",
      "New best score: 0.022826740254456857\n",
      "\n",
      "Epoch 117: train loss: 0.026375775348854123\n",
      "valid loss: 0.022805018968211587\n",
      "New best score: 0.022805018968211587\n",
      "\n",
      "Epoch 118: train loss: 0.026351093589509884\n",
      "valid loss: 0.022784711155656043\n",
      "New best score: 0.022784711155656043\n",
      "\n",
      "Epoch 119: train loss: 0.026325996560651585\n",
      "valid loss: 0.022765248353745375\n",
      "New best score: 0.022765248353745375\n",
      "\n",
      "Epoch 120: train loss: 0.026302598219641354\n",
      "valid loss: 0.02274405328034496\n",
      "New best score: 0.02274405328034496\n",
      "\n",
      "Epoch 121: train loss: 0.02627499704758728\n",
      "valid loss: 0.02272408037435194\n",
      "New best score: 0.02272408037435194\n",
      "\n",
      "Epoch 122: train loss: 0.02625181942714731\n",
      "valid loss: 0.022703228865184798\n",
      "New best score: 0.022703228865184798\n",
      "\n",
      "Epoch 123: train loss: 0.026224286204423724\n",
      "valid loss: 0.022682135343514834\n",
      "New best score: 0.022682135343514834\n",
      "\n",
      "Epoch 124: train loss: 0.026199887062269632\n",
      "valid loss: 0.02266546511745569\n",
      "New best score: 0.02266546511745569\n",
      "\n",
      "Epoch 125: train loss: 0.026192434318851036\n",
      "valid loss: 0.02264374750615205\n",
      "New best score: 0.02264374750615205\n",
      "\n",
      "Epoch 126: train loss: 0.026162304320673038\n",
      "valid loss: 0.02262632863756584\n",
      "New best score: 0.02262632863756584\n",
      "\n",
      "Epoch 127: train loss: 0.026133843175455446\n",
      "valid loss: 0.02260473484232711\n",
      "New best score: 0.02260473484232711\n",
      "\n",
      "Epoch 128: train loss: 0.026112909058372994\n",
      "valid loss: 0.022587620979072742\n",
      "New best score: 0.022587620979072742\n",
      "\n",
      "Epoch 129: train loss: 0.026088774505538332\n",
      "valid loss: 0.022568769493824203\n",
      "New best score: 0.022568769493824203\n",
      "\n",
      "Epoch 130: train loss: 0.026068026343932984\n",
      "valid loss: 0.02254896634846157\n",
      "New best score: 0.02254896634846157\n",
      "\n",
      "Epoch 131: train loss: 0.026082473794409526\n",
      "valid loss: 0.022533383967741325\n",
      "New best score: 0.022533383967741325\n",
      "\n",
      "Epoch 132: train loss: 0.02602513499919513\n",
      "valid loss: 0.02251642675759551\n",
      "New best score: 0.02251642675759551\n",
      "\n",
      "Epoch 133: train loss: 0.02600384113905184\n",
      "valid loss: 0.022501060521152406\n",
      "New best score: 0.022501060521152406\n",
      "\n",
      "Epoch 134: train loss: 0.02598155145316281\n",
      "valid loss: 0.02248370519897499\n",
      "New best score: 0.02248370519897499\n",
      "\n",
      "Epoch 135: train loss: 0.02601962847915167\n",
      "valid loss: 0.022468898160143885\n",
      "New best score: 0.022468898160143885\n",
      "\n",
      "Epoch 136: train loss: 0.025941508837871885\n",
      "valid loss: 0.022452146888063438\n",
      "New best score: 0.022452146888063438\n",
      "\n",
      "Epoch 137: train loss: 0.025917847815823063\n",
      "valid loss: 0.022435859751986784\n",
      "New best score: 0.022435859751986784\n",
      "\n",
      "Epoch 138: train loss: 0.025902947025474597\n",
      "valid loss: 0.02242038199733268\n",
      "New best score: 0.02242038199733268\n",
      "\n",
      "Epoch 139: train loss: 0.025880843421114658\n",
      "valid loss: 0.022404737253013986\n",
      "New best score: 0.022404737253013986\n",
      "\n",
      "Epoch 140: train loss: 0.025885682903517296\n",
      "valid loss: 0.0223911289497801\n",
      "New best score: 0.0223911289497801\n",
      "\n",
      "Epoch 141: train loss: 0.025841780454381152\n",
      "valid loss: 0.022379414421740334\n",
      "New best score: 0.022379414421740334\n",
      "\n",
      "Epoch 142: train loss: 0.025837616112834635\n",
      "valid loss: 0.02236247401808222\n",
      "New best score: 0.02236247401808222\n",
      "\n",
      "Epoch 143: train loss: 0.02580396655094513\n",
      "valid loss: 0.02234736228597879\n",
      "New best score: 0.02234736228597879\n",
      "\n",
      "Epoch 144: train loss: 0.025800603833785022\n",
      "valid loss: 0.022336563238317942\n",
      "New best score: 0.022336563238317942\n",
      "\n",
      "Epoch 145: train loss: 0.025767751012155192\n",
      "valid loss: 0.022321166170855042\n",
      "New best score: 0.022321166170855042\n",
      "\n",
      "Epoch 146: train loss: 0.025766641274404728\n",
      "valid loss: 0.0223060422686862\n",
      "New best score: 0.0223060422686862\n",
      "\n",
      "Epoch 147: train loss: 0.025730960377687847\n",
      "valid loss: 0.022294106269633535\n",
      "New best score: 0.022294106269633535\n",
      "\n",
      "Epoch 148: train loss: 0.025711454850728743\n",
      "valid loss: 0.022283422591473444\n",
      "New best score: 0.022283422591473444\n",
      "\n",
      "Epoch 149: train loss: 0.025697087647515632\n",
      "valid loss: 0.022262921238892622\n",
      "New best score: 0.022262921238892622\n",
      "\n",
      "Epoch 150: train loss: 0.025677387355618555\n",
      "valid loss: 0.02225074021715127\n",
      "New best score: 0.02225074021715127\n",
      "\n",
      "Epoch 151: train loss: 0.025664389643642687\n",
      "valid loss: 0.022235200145669307\n",
      "New best score: 0.022235200145669307\n",
      "\n",
      "Epoch 152: train loss: 0.0256411914319796\n",
      "valid loss: 0.022223734268233687\n",
      "New best score: 0.022223734268233687\n",
      "\n",
      "Epoch 153: train loss: 0.02562664854685846\n",
      "valid loss: 0.022207733506948147\n",
      "New best score: 0.022207733506948147\n",
      "\n",
      "Epoch 154: train loss: 0.02561191625013189\n",
      "valid loss: 0.022195905610064348\n",
      "New best score: 0.022195905610064348\n",
      "\n",
      "Epoch 155: train loss: 0.025591150422418378\n",
      "valid loss: 0.02217830963454257\n",
      "New best score: 0.02217830963454257\n",
      "\n",
      "Epoch 156: train loss: 0.025632521341848128\n",
      "valid loss: 0.022171504095781485\n",
      "New best score: 0.022171504095781485\n",
      "\n",
      "Epoch 157: train loss: 0.025560164219122928\n",
      "valid loss: 0.02215518746381926\n",
      "New best score: 0.02215518746381926\n",
      "\n",
      "Epoch 158: train loss: 0.02554334885995484\n",
      "valid loss: 0.02214173835361812\n",
      "New best score: 0.02214173835361812\n",
      "\n",
      "Epoch 159: train loss: 0.025525262342060567\n",
      "valid loss: 0.022128477180172187\n",
      "New best score: 0.022128477180172187\n",
      "\n",
      "Epoch 160: train loss: 0.025513359405087938\n",
      "valid loss: 0.02211627805540103\n",
      "New best score: 0.02211627805540103\n",
      "\n",
      "Epoch 161: train loss: 0.025493933324109264\n",
      "valid loss: 0.022110118915589626\n",
      "New best score: 0.022110118915589626\n",
      "\n",
      "Epoch 162: train loss: 0.025481024769433445\n",
      "valid loss: 0.022093862162761872\n",
      "New best score: 0.022093862162761872\n",
      "\n",
      "Epoch 163: train loss: 0.025464615366775207\n",
      "valid loss: 0.022083711040867687\n",
      "New best score: 0.022083711040867687\n",
      "\n",
      "Epoch 164: train loss: 0.025449990624514054\n",
      "valid loss: 0.022071608553592494\n",
      "New best score: 0.022071608553592494\n",
      "\n",
      "Epoch 165: train loss: 0.02543347954442574\n",
      "valid loss: 0.022059257727005937\n",
      "New best score: 0.022059257727005937\n",
      "\n",
      "Epoch 166: train loss: 0.02542076352252656\n",
      "valid loss: 0.022049720102612393\n",
      "New best score: 0.022049720102612393\n",
      "\n",
      "Epoch 167: train loss: 0.025406965627011575\n",
      "valid loss: 0.022038756992921446\n",
      "New best score: 0.022038756992921446\n",
      "\n",
      "Epoch 168: train loss: 0.025389208932665063\n",
      "valid loss: 0.02202610487581469\n",
      "New best score: 0.02202610487581469\n",
      "\n",
      "Epoch 169: train loss: 0.02541625089425023\n",
      "valid loss: 0.022014778754525663\n",
      "New best score: 0.022014778754525663\n",
      "\n",
      "Epoch 170: train loss: 0.02536148096427683\n",
      "valid loss: 0.02200510940677052\n",
      "New best score: 0.02200510940677052\n",
      "\n",
      "Epoch 171: train loss: 0.025347690973040973\n",
      "valid loss: 0.021992577322422394\n",
      "New best score: 0.021992577322422394\n",
      "\n",
      "Epoch 172: train loss: 0.02533156275868834\n",
      "valid loss: 0.021985773768472258\n",
      "New best score: 0.021985773768472258\n",
      "\n",
      "Epoch 173: train loss: 0.025317901805252348\n",
      "valid loss: 0.021972343384653214\n",
      "New best score: 0.021972343384653214\n",
      "\n",
      "Epoch 174: train loss: 0.025305075114584047\n",
      "valid loss: 0.02196213547763968\n",
      "New best score: 0.02196213547763968\n",
      "\n",
      "Epoch 175: train loss: 0.025288324526862926\n",
      "valid loss: 0.02195230351902062\n",
      "New best score: 0.02195230351902062\n",
      "\n",
      "Epoch 176: train loss: 0.02527366501884402\n",
      "valid loss: 0.02193881880029889\n",
      "New best score: 0.02193881880029889\n",
      "\n",
      "Epoch 177: train loss: 0.025261055144457437\n",
      "valid loss: 0.021928706515118333\n",
      "New best score: 0.021928706515118333\n",
      "\n",
      "Epoch 178: train loss: 0.02524642125046772\n",
      "valid loss: 0.021922243370303452\n",
      "New best score: 0.021922243370303452\n",
      "\n",
      "Epoch 179: train loss: 0.02528252177667954\n",
      "valid loss: 0.02191011734259786\n",
      "New best score: 0.02191011734259786\n",
      "\n",
      "Epoch 180: train loss: 0.02521884326599751\n",
      "valid loss: 0.021902418091972936\n",
      "New best score: 0.021902418091972936\n",
      "\n",
      "Epoch 181: train loss: 0.02520622375339702\n",
      "valid loss: 0.02189403142867984\n",
      "New best score: 0.02189403142867984\n",
      "\n",
      "Epoch 182: train loss: 0.025192612022264273\n",
      "valid loss: 0.021886538128643558\n",
      "New best score: 0.021886538128643558\n",
      "\n",
      "Epoch 183: train loss: 0.02518880529330067\n",
      "valid loss: 0.021879198108355053\n",
      "New best score: 0.021879198108355053\n",
      "\n",
      "Epoch 184: train loss: 0.02523205580644409\n",
      "valid loss: 0.021869019821181925\n",
      "New best score: 0.021869019821181925\n",
      "\n",
      "Epoch 185: train loss: 0.025221607645842414\n",
      "valid loss: 0.021859427095921435\n",
      "New best score: 0.021859427095921435\n",
      "\n",
      "Epoch 186: train loss: 0.025143738577078557\n",
      "valid loss: 0.02184891151560779\n",
      "New best score: 0.02184891151560779\n",
      "\n",
      "Epoch 187: train loss: 0.025129682686823206\n",
      "valid loss: 0.02183626602919063\n",
      "New best score: 0.02183626602919063\n",
      "\n",
      "Epoch 188: train loss: 0.025116997641811658\n",
      "valid loss: 0.021827769068066688\n",
      "New best score: 0.021827769068066688\n",
      "\n",
      "Epoch 189: train loss: 0.02510254187146104\n",
      "valid loss: 0.02181633071857377\n",
      "New best score: 0.02181633071857377\n",
      "\n",
      "Epoch 190: train loss: 0.025089835737221348\n",
      "valid loss: 0.02180305206928398\n",
      "New best score: 0.02180305206928398\n",
      "\n",
      "Epoch 191: train loss: 0.02507719608665321\n",
      "valid loss: 0.021796328554381784\n",
      "New best score: 0.021796328554381784\n",
      "\n",
      "Epoch 192: train loss: 0.025074883942783843\n",
      "valid loss: 0.021794376952080904\n",
      "New best score: 0.021794376952080904\n",
      "\n",
      "Epoch 193: train loss: 0.025053680953051324\n",
      "valid loss: 0.02178019637412685\n",
      "New best score: 0.02178019637412685\n",
      "\n",
      "Epoch 194: train loss: 0.02503977278353811\n",
      "valid loss: 0.02177531711381996\n",
      "New best score: 0.02177531711381996\n",
      "\n",
      "Epoch 195: train loss: 0.025028715481924924\n",
      "valid loss: 0.021764344290925268\n",
      "New best score: 0.021764344290925268\n",
      "\n",
      "Epoch 196: train loss: 0.02501627084662647\n",
      "valid loss: 0.02175286292347906\n",
      "New best score: 0.02175286292347906\n",
      "\n",
      "Epoch 197: train loss: 0.025001551057402573\n",
      "valid loss: 0.021752976878616718\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 198: train loss: 0.024994037210228866\n",
      "valid loss: 0.02174267361838162\n",
      "New best score: 0.02174267361838162\n",
      "\n",
      "Epoch 199: train loss: 0.024980100437327086\n",
      "valid loss: 0.021727840757328695\n",
      "New best score: 0.021727840757328695\n",
      "\n",
      "Epoch 200: train loss: 0.02503158639452742\n",
      "valid loss: 0.02172294829416975\n",
      "New best score: 0.02172294829416975\n",
      "\n",
      "Epoch 201: train loss: 0.024959311447284412\n",
      "valid loss: 0.021712309533922343\n",
      "New best score: 0.021712309533922343\n",
      "\n",
      "Epoch 202: train loss: 0.024947354647567536\n",
      "valid loss: 0.021703752885602258\n",
      "New best score: 0.021703752885602258\n",
      "\n",
      "Epoch 203: train loss: 0.024934945137795594\n",
      "valid loss: 0.021697206836210157\n",
      "New best score: 0.021697206836210157\n",
      "\n",
      "Epoch 204: train loss: 0.024921195273182144\n",
      "valid loss: 0.021687102953378935\n",
      "New best score: 0.021687102953378935\n",
      "\n",
      "Epoch 205: train loss: 0.024912548149323584\n",
      "valid loss: 0.02168348219662774\n",
      "New best score: 0.02168348219662774\n",
      "\n",
      "Epoch 206: train loss: 0.0248996957035099\n",
      "valid loss: 0.02167228329517806\n",
      "New best score: 0.02167228329517806\n",
      "\n",
      "Epoch 207: train loss: 0.02489316864621486\n",
      "valid loss: 0.021665069259014125\n",
      "New best score: 0.021665069259014125\n",
      "\n",
      "Epoch 208: train loss: 0.024953006394561928\n",
      "valid loss: 0.021661596293750286\n",
      "New best score: 0.021661596293750286\n",
      "\n",
      "Epoch 209: train loss: 0.024865511637340548\n",
      "valid loss: 0.021651579202055668\n",
      "New best score: 0.021651579202055668\n",
      "\n",
      "Epoch 210: train loss: 0.024859148808723967\n",
      "valid loss: 0.021641373104527264\n",
      "New best score: 0.021641373104527264\n",
      "\n",
      "Epoch 211: train loss: 0.024917554358626536\n",
      "valid loss: 0.02163408458785948\n",
      "New best score: 0.02163408458785948\n",
      "\n",
      "Epoch 212: train loss: 0.024833711033215574\n",
      "valid loss: 0.021627237333309898\n",
      "New best score: 0.021627237333309898\n",
      "\n",
      "Epoch 213: train loss: 0.02482070368479806\n",
      "valid loss: 0.021624466272696655\n",
      "New best score: 0.021624466272696655\n",
      "\n",
      "Epoch 214: train loss: 0.02480996680511543\n",
      "valid loss: 0.021609866712037903\n",
      "New best score: 0.021609866712037903\n",
      "\n",
      "Epoch 215: train loss: 0.0248004721254323\n",
      "valid loss: 0.021601244573390396\n",
      "New best score: 0.021601244573390396\n",
      "\n",
      "Epoch 216: train loss: 0.0247893121355363\n",
      "valid loss: 0.02159233510204919\n",
      "New best score: 0.02159233510204919\n",
      "\n",
      "Epoch 217: train loss: 0.02477753150509892\n",
      "valid loss: 0.021594051260176054\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 218: train loss: 0.024769285515431033\n",
      "valid loss: 0.02158147289108947\n",
      "New best score: 0.02158147289108947\n",
      "\n",
      "Epoch 219: train loss: 0.02475950324736876\n",
      "valid loss: 0.021578400753107244\n",
      "New best score: 0.021578400753107244\n",
      "\n",
      "Epoch 220: train loss: 0.02478400365135798\n",
      "valid loss: 0.021573269992512696\n",
      "New best score: 0.021573269992512696\n",
      "\n",
      "Epoch 221: train loss: 0.024741339743833857\n",
      "valid loss: 0.021565240815243022\n",
      "New best score: 0.021565240815243022\n",
      "\n",
      "Epoch 222: train loss: 0.02473593046924154\n",
      "valid loss: 0.021551784473746345\n",
      "New best score: 0.021551784473746345\n",
      "\n",
      "Epoch 223: train loss: 0.02479219487642286\n",
      "valid loss: 0.021548721463696543\n",
      "New best score: 0.021548721463696543\n",
      "\n",
      "Epoch 224: train loss: 0.024708772719049955\n",
      "valid loss: 0.021538767855283195\n",
      "New best score: 0.021538767855283195\n",
      "\n",
      "Epoch 225: train loss: 0.02470225510568483\n",
      "valid loss: 0.021530790848690202\n",
      "New best score: 0.021530790848690202\n",
      "\n",
      "Epoch 226: train loss: 0.02469122903306324\n",
      "valid loss: 0.021527893372027266\n",
      "New best score: 0.021527893372027266\n",
      "\n",
      "Epoch 227: train loss: 0.024674488334109266\n",
      "valid loss: 0.02151899811638975\n",
      "New best score: 0.02151899811638975\n",
      "\n",
      "Epoch 228: train loss: 0.02466693644138934\n",
      "valid loss: 0.021517573417758347\n",
      "New best score: 0.021517573417758347\n",
      "\n",
      "Epoch 229: train loss: 0.024655795691146752\n",
      "valid loss: 0.021505236994468994\n",
      "New best score: 0.021505236994468994\n",
      "\n",
      "Epoch 230: train loss: 0.024646569904716212\n",
      "valid loss: 0.02149774568537751\n",
      "New best score: 0.02149774568537751\n",
      "\n",
      "Epoch 231: train loss: 0.024638196849552197\n",
      "valid loss: 0.02149211789857551\n",
      "New best score: 0.02149211789857551\n",
      "\n",
      "Epoch 232: train loss: 0.024681297888745377\n",
      "valid loss: 0.02148882708278138\n",
      "New best score: 0.02148882708278138\n",
      "\n",
      "Epoch 233: train loss: 0.024616681375398127\n",
      "valid loss: 0.02148217352288116\n",
      "New best score: 0.02148217352288116\n",
      "\n",
      "Epoch 234: train loss: 0.024653244128479107\n",
      "valid loss: 0.021478287676557833\n",
      "New best score: 0.021478287676557833\n",
      "\n",
      "Epoch 235: train loss: 0.02459849139075588\n",
      "valid loss: 0.02146821399296007\n",
      "New best score: 0.02146821399296007\n",
      "\n",
      "Epoch 236: train loss: 0.024582991819805708\n",
      "valid loss: 0.02145342433413145\n",
      "New best score: 0.02145342433413145\n",
      "\n",
      "Epoch 237: train loss: 0.02457822367485141\n",
      "valid loss: 0.021454093303596836\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 238: train loss: 0.024631201428495043\n",
      "valid loss: 0.02144779938270065\n",
      "New best score: 0.02144779938270065\n",
      "\n",
      "Epoch 239: train loss: 0.0245587749574617\n",
      "valid loss: 0.021445217885590197\n",
      "New best score: 0.021445217885590197\n",
      "\n",
      "Epoch 240: train loss: 0.024548377623723076\n",
      "valid loss: 0.02143153913996901\n",
      "New best score: 0.02143153913996901\n",
      "\n",
      "Epoch 241: train loss: 0.024553274636340286\n",
      "valid loss: 0.02142920109707122\n",
      "New best score: 0.02142920109707122\n",
      "\n",
      "Epoch 242: train loss: 0.02453101322375642\n",
      "valid loss: 0.021426041521342587\n",
      "New best score: 0.021426041521342587\n",
      "\n",
      "Epoch 243: train loss: 0.024538348744588805\n",
      "valid loss: 0.02141333003216612\n",
      "New best score: 0.02141333003216612\n",
      "\n",
      "Epoch 244: train loss: 0.024559481877596122\n",
      "valid loss: 0.0214129901681766\n",
      "New best score: 0.0214129901681766\n",
      "\n",
      "Epoch 245: train loss: 0.02454862345123467\n",
      "valid loss: 0.021403396985688862\n",
      "New best score: 0.021403396985688862\n",
      "\n",
      "Epoch 246: train loss: 0.02449437862569617\n",
      "valid loss: 0.021399710507057485\n",
      "New best score: 0.021399710507057485\n",
      "\n",
      "Epoch 247: train loss: 0.02447926982033016\n",
      "valid loss: 0.02138408256002572\n",
      "New best score: 0.02138408256002572\n",
      "\n",
      "Epoch 248: train loss: 0.02447759553096253\n",
      "valid loss: 0.021384760221153595\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 249: train loss: 0.02451972035623796\n",
      "valid loss: 0.021383708107778978\n",
      "New best score: 0.021383708107778978\n",
      "\n",
      "Epoch 250: train loss: 0.0244572473923409\n",
      "valid loss: 0.021373101186035887\n",
      "New best score: 0.021373101186035887\n",
      "\n",
      "Epoch 251: train loss: 0.02444957479919625\n",
      "valid loss: 0.021370174855752374\n",
      "New best score: 0.021370174855752374\n",
      "\n",
      "Epoch 252: train loss: 0.024439141289424062\n",
      "valid loss: 0.02135779511982945\n",
      "New best score: 0.02135779511982945\n",
      "\n",
      "Epoch 253: train loss: 0.024478702835400157\n",
      "valid loss: 0.02135470213494279\n",
      "New best score: 0.02135470213494279\n",
      "\n",
      "Epoch 254: train loss: 0.024421908139923745\n",
      "valid loss: 0.021350776568631734\n",
      "New best score: 0.021350776568631734\n",
      "\n",
      "Epoch 255: train loss: 0.02441321756291096\n",
      "valid loss: 0.021347218424410987\n",
      "New best score: 0.021347218424410987\n",
      "\n",
      "Epoch 256: train loss: 0.02440380957531437\n",
      "valid loss: 0.021343336018665185\n",
      "New best score: 0.021343336018665185\n",
      "\n",
      "Epoch 257: train loss: 0.0243971234543708\n",
      "valid loss: 0.021334473084061355\n",
      "New best score: 0.021334473084061355\n",
      "\n",
      "Epoch 258: train loss: 0.02438712861015861\n",
      "valid loss: 0.021329384240138822\n",
      "New best score: 0.021329384240138822\n",
      "\n",
      "Epoch 259: train loss: 0.024385117190880268\n",
      "valid loss: 0.021330366494097266\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 260: train loss: 0.024370398293989227\n",
      "valid loss: 0.021315385161211016\n",
      "New best score: 0.021315385161211016\n",
      "\n",
      "Epoch 261: train loss: 0.024361230032456476\n",
      "valid loss: 0.021313991932030745\n",
      "New best score: 0.021313991932030745\n",
      "\n",
      "Epoch 262: train loss: 0.024353487250952495\n",
      "valid loss: 0.021302609043768336\n",
      "New best score: 0.021302609043768336\n",
      "\n",
      "Epoch 263: train loss: 0.02434515457606558\n",
      "valid loss: 0.021298080223482653\n",
      "New best score: 0.021298080223482653\n",
      "\n",
      "Epoch 264: train loss: 0.02433531188590365\n",
      "valid loss: 0.021293240011762724\n",
      "New best score: 0.021293240011762724\n",
      "\n",
      "Epoch 265: train loss: 0.02432692770116903\n",
      "valid loss: 0.021284011400148323\n",
      "New best score: 0.021284011400148323\n",
      "\n",
      "Epoch 266: train loss: 0.024320082801910777\n",
      "valid loss: 0.02128192225474206\n",
      "New best score: 0.02128192225474206\n",
      "\n",
      "Epoch 267: train loss: 0.024311186917450503\n",
      "valid loss: 0.02127923278989624\n",
      "New best score: 0.02127923278989624\n",
      "\n",
      "Epoch 268: train loss: 0.024302474639115307\n",
      "valid loss: 0.02126792692001725\n",
      "New best score: 0.02126792692001725\n",
      "\n",
      "Epoch 269: train loss: 0.024305690981121602\n",
      "valid loss: 0.021265654199277218\n",
      "New best score: 0.021265654199277218\n",
      "\n",
      "Epoch 270: train loss: 0.0242857733082565\n",
      "valid loss: 0.02126476808188383\n",
      "New best score: 0.02126476808188383\n",
      "\n",
      "Epoch 271: train loss: 0.02427712937778874\n",
      "valid loss: 0.021257581990508673\n",
      "New best score: 0.021257581990508673\n",
      "\n",
      "Epoch 272: train loss: 0.024272595043980882\n",
      "valid loss: 0.021253213204615203\n",
      "New best score: 0.021253213204615203\n",
      "\n",
      "Epoch 273: train loss: 0.024262231320486563\n",
      "valid loss: 0.021246821108927418\n",
      "New best score: 0.021246821108927418\n",
      "\n",
      "Epoch 274: train loss: 0.024260711345110944\n",
      "valid loss: 0.021244957046348163\n",
      "New best score: 0.021244957046348163\n",
      "\n",
      "Epoch 275: train loss: 0.02424511743233593\n",
      "valid loss: 0.021238694013465446\n",
      "New best score: 0.021238694013465446\n",
      "\n",
      "Epoch 276: train loss: 0.024288055258355572\n",
      "valid loss: 0.02123432173945713\n",
      "New best score: 0.02123432173945713\n",
      "\n",
      "Epoch 277: train loss: 0.024227965825621027\n",
      "valid loss: 0.021219709183986\n",
      "New best score: 0.021219709183986\n",
      "\n",
      "Epoch 278: train loss: 0.02422094012040833\n",
      "valid loss: 0.021214640250278208\n",
      "New best score: 0.021214640250278208\n",
      "\n",
      "Epoch 279: train loss: 0.02425003684653974\n",
      "valid loss: 0.021214173869797234\n",
      "New best score: 0.021214173869797234\n",
      "\n",
      "Epoch 280: train loss: 0.02419985030896494\n",
      "valid loss: 0.021200374269481792\n",
      "New best score: 0.021200374269481792\n",
      "\n",
      "Epoch 281: train loss: 0.02419898506399283\n",
      "valid loss: 0.021200285317441683\n",
      "New best score: 0.021200285317441683\n",
      "\n",
      "Epoch 282: train loss: 0.024187979526989707\n",
      "valid loss: 0.02119362197430962\n",
      "New best score: 0.02119362197430962\n",
      "\n",
      "Epoch 283: train loss: 0.024182104250171588\n",
      "valid loss: 0.021191826508340603\n",
      "New best score: 0.021191826508340603\n",
      "\n",
      "Epoch 284: train loss: 0.024176165210543915\n",
      "valid loss: 0.021187916379135703\n",
      "New best score: 0.021187916379135703\n",
      "\n",
      "Epoch 285: train loss: 0.024222749554118822\n",
      "valid loss: 0.021182486959186134\n",
      "New best score: 0.021182486959186134\n",
      "\n",
      "Epoch 286: train loss: 0.024158735518085884\n",
      "valid loss: 0.02117850516823608\n",
      "New best score: 0.02117850516823608\n",
      "\n",
      "Epoch 287: train loss: 0.02415205450014315\n",
      "valid loss: 0.02117243811051626\n",
      "New best score: 0.02117243811051626\n",
      "\n",
      "Epoch 288: train loss: 0.024143926063809478\n",
      "valid loss: 0.02117047065042949\n",
      "New best score: 0.02117047065042949\n",
      "\n",
      "Epoch 289: train loss: 0.0241364879100774\n",
      "valid loss: 0.021164575393611215\n",
      "New best score: 0.021164575393611215\n",
      "\n",
      "Epoch 290: train loss: 0.024146675886754673\n",
      "valid loss: 0.021159142559152976\n",
      "New best score: 0.021159142559152976\n",
      "\n",
      "Epoch 291: train loss: 0.02412027913056222\n",
      "valid loss: 0.02115565350352818\n",
      "New best score: 0.02115565350352818\n",
      "\n",
      "Epoch 292: train loss: 0.024112385397929377\n",
      "valid loss: 0.02114806739096126\n",
      "New best score: 0.02114806739096126\n",
      "\n",
      "Epoch 293: train loss: 0.024105305966214904\n",
      "valid loss: 0.021142451836841037\n",
      "New best score: 0.021142451836841037\n",
      "\n",
      "Epoch 294: train loss: 0.02409593797342889\n",
      "valid loss: 0.02113808211297999\n",
      "New best score: 0.02113808211297999\n",
      "\n",
      "Epoch 295: train loss: 0.024089633072677693\n",
      "valid loss: 0.02113756819159962\n",
      "New best score: 0.02113756819159962\n",
      "\n",
      "Epoch 296: train loss: 0.024082672774710195\n",
      "valid loss: 0.021127939754565433\n",
      "New best score: 0.021127939754565433\n",
      "\n",
      "Epoch 297: train loss: 0.024079816301373656\n",
      "valid loss: 0.021128572613812582\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 298: train loss: 0.024163746828395876\n",
      "valid loss: 0.02112099131273819\n",
      "New best score: 0.02112099131273819\n",
      "\n",
      "Epoch 299: train loss: 0.024059576728734784\n",
      "valid loss: 0.021117992804443993\n",
      "New best score: 0.021117992804443993\n",
      "\n",
      "Epoch 300: train loss: 0.02405383781490973\n",
      "valid loss: 0.021113575818518054\n",
      "New best score: 0.021113575818518054\n",
      "\n",
      "Epoch 301: train loss: 0.02404644419474582\n",
      "valid loss: 0.02111067272606178\n",
      "New best score: 0.02111067272606178\n",
      "\n",
      "Epoch 302: train loss: 0.02403783839865881\n",
      "valid loss: 0.021104318464970222\n",
      "New best score: 0.021104318464970222\n",
      "\n",
      "Epoch 303: train loss: 0.024035229861158542\n",
      "valid loss: 0.021101245006421656\n",
      "New best score: 0.021101245006421656\n",
      "\n",
      "Epoch 304: train loss: 0.024053803671436735\n",
      "valid loss: 0.021095515685012865\n",
      "New best score: 0.021095515685012865\n",
      "\n",
      "Epoch 305: train loss: 0.024016664251791477\n",
      "valid loss: 0.02108685008984194\n",
      "New best score: 0.02108685008984194\n",
      "\n",
      "Epoch 306: train loss: 0.02400965775211619\n",
      "valid loss: 0.02108899643619133\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 307: train loss: 0.024004438084077893\n",
      "valid loss: 0.02108944739055596\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 308: train loss: 0.023994702191183347\n",
      "valid loss: 0.02107654928723524\n",
      "New best score: 0.02107654928723524\n",
      "\n",
      "Epoch 309: train loss: 0.02398772872067265\n",
      "valid loss: 0.021077160194207446\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 310: train loss: 0.023985680183626137\n",
      "valid loss: 0.02107244005511616\n",
      "New best score: 0.02107244005511616\n",
      "\n",
      "Epoch 311: train loss: 0.023974384939728692\n",
      "valid loss: 0.021068227422422637\n",
      "New best score: 0.021068227422422637\n",
      "\n",
      "Epoch 312: train loss: 0.023966610730296933\n",
      "valid loss: 0.021061115192032687\n",
      "New best score: 0.021061115192032687\n",
      "\n",
      "Epoch 313: train loss: 0.023996961331189913\n",
      "valid loss: 0.021052650402941048\n",
      "New best score: 0.021052650402941048\n",
      "\n",
      "Epoch 314: train loss: 0.023952136648735554\n",
      "valid loss: 0.021045273203866885\n",
      "New best score: 0.021045273203866885\n",
      "\n",
      "Epoch 315: train loss: 0.023949374466049854\n",
      "valid loss: 0.021044104093477196\n",
      "New best score: 0.021044104093477196\n",
      "\n",
      "Epoch 316: train loss: 0.023939502664746894\n",
      "valid loss: 0.02104298023171682\n",
      "New best score: 0.02104298023171682\n",
      "\n",
      "Epoch 317: train loss: 0.023932936950697502\n",
      "valid loss: 0.02104115826522463\n",
      "New best score: 0.02104115826522463\n",
      "\n",
      "Epoch 318: train loss: 0.023923984791668792\n",
      "valid loss: 0.021041016404145896\n",
      "New best score: 0.021041016404145896\n",
      "\n",
      "Epoch 319: train loss: 0.02391742616444365\n",
      "valid loss: 0.02102950470763311\n",
      "New best score: 0.02102950470763311\n",
      "\n",
      "Epoch 320: train loss: 0.023915823923865178\n",
      "valid loss: 0.02101971027538442\n",
      "New best score: 0.02101971027538442\n",
      "\n",
      "Epoch 321: train loss: 0.023905628898910306\n",
      "valid loss: 0.021017438503857355\n",
      "New best score: 0.021017438503857355\n",
      "\n",
      "Epoch 322: train loss: 0.02389681504150546\n",
      "valid loss: 0.021012097183598612\n",
      "New best score: 0.021012097183598612\n",
      "\n",
      "Epoch 323: train loss: 0.02388944425691029\n",
      "valid loss: 0.021009399835224766\n",
      "New best score: 0.021009399835224766\n",
      "\n",
      "Epoch 324: train loss: 0.02388414205428564\n",
      "valid loss: 0.021011002651854335\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 325: train loss: 0.023877218924817246\n",
      "valid loss: 0.021008546851088754\n",
      "New best score: 0.021008546851088754\n",
      "\n",
      "Epoch 326: train loss: 0.02387193265503058\n",
      "valid loss: 0.021001635499617533\n",
      "New best score: 0.021001635499617533\n",
      "\n",
      "Epoch 327: train loss: 0.023865656897321088\n",
      "valid loss: 0.021000356712193524\n",
      "New best score: 0.021000356712193524\n",
      "\n",
      "Epoch 328: train loss: 0.023856881523875256\n",
      "valid loss: 0.020996884148185572\n",
      "New best score: 0.020996884148185572\n",
      "\n",
      "Epoch 329: train loss: 0.023848394035440573\n",
      "valid loss: 0.020997623912734516\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 330: train loss: 0.023870378233568582\n",
      "valid loss: 0.02098058576416531\n",
      "New best score: 0.02098058576416531\n",
      "\n",
      "Epoch 331: train loss: 0.023840249306500156\n",
      "valid loss: 0.020980552702980642\n",
      "New best score: 0.020980552702980642\n",
      "\n",
      "Epoch 332: train loss: 0.023831630352048026\n",
      "valid loss: 0.02098100511592319\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 333: train loss: 0.023824100799550198\n",
      "valid loss: 0.02097386511453682\n",
      "New best score: 0.02097386511453682\n",
      "\n",
      "Epoch 334: train loss: 0.02386297094817322\n",
      "valid loss: 0.02096915674374391\n",
      "New best score: 0.02096915674374391\n",
      "\n",
      "Epoch 335: train loss: 0.023840850303755927\n",
      "valid loss: 0.020966926782824403\n",
      "New best score: 0.020966926782824403\n",
      "\n",
      "Epoch 336: train loss: 0.02380759821346049\n",
      "valid loss: 0.02096587904492796\n",
      "New best score: 0.02096587904492796\n",
      "\n",
      "Epoch 337: train loss: 0.0237980312169339\n",
      "valid loss: 0.020959164093133024\n",
      "New best score: 0.020959164093133024\n",
      "\n",
      "Epoch 338: train loss: 0.023792401790445214\n",
      "valid loss: 0.02095471698566903\n",
      "New best score: 0.02095471698566903\n",
      "\n",
      "Epoch 339: train loss: 0.023785505795605007\n",
      "valid loss: 0.020949531619771332\n",
      "New best score: 0.020949531619771332\n",
      "\n",
      "Epoch 340: train loss: 0.02377985277989406\n",
      "valid loss: 0.020942809233497152\n",
      "New best score: 0.020942809233497152\n",
      "\n",
      "Epoch 341: train loss: 0.023773511931287756\n",
      "valid loss: 0.020939035298973403\n",
      "New best score: 0.020939035298973403\n",
      "\n",
      "Epoch 342: train loss: 0.023765357302946634\n",
      "valid loss: 0.02094549046620792\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 343: train loss: 0.023760573256168902\n",
      "valid loss: 0.020935805807090504\n",
      "New best score: 0.020935805807090504\n",
      "\n",
      "Epoch 344: train loss: 0.023753687611991075\n",
      "valid loss: 0.02093292783069681\n",
      "New best score: 0.02093292783069681\n",
      "\n",
      "Epoch 345: train loss: 0.023747453017011425\n",
      "valid loss: 0.020929876499493433\n",
      "New best score: 0.020929876499493433\n",
      "\n",
      "Epoch 346: train loss: 0.023739050580215217\n",
      "valid loss: 0.020932380814651184\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 347: train loss: 0.02373478363811275\n",
      "valid loss: 0.02091889672669348\n",
      "New best score: 0.02091889672669348\n",
      "\n",
      "Epoch 348: train loss: 0.023773791514368963\n",
      "valid loss: 0.020911160422004475\n",
      "New best score: 0.020911160422004475\n",
      "\n",
      "Epoch 349: train loss: 0.023718634783356788\n",
      "valid loss: 0.020904566729993333\n",
      "New best score: 0.020904566729993333\n",
      "\n",
      "Epoch 350: train loss: 0.023715017221648278\n",
      "valid loss: 0.020913757085309462\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 351: train loss: 0.02379463119990592\n",
      "valid loss: 0.020909357993351346\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 352: train loss: 0.023703444816684308\n",
      "valid loss: 0.02090394666135254\n",
      "New best score: 0.02090394666135254\n",
      "\n",
      "Epoch 353: train loss: 0.023697565068011406\n",
      "valid loss: 0.02090300704377093\n",
      "New best score: 0.02090300704377093\n",
      "\n",
      "Epoch 354: train loss: 0.02368754578721519\n",
      "valid loss: 0.020907431906835472\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 355: train loss: 0.023773987279304583\n",
      "valid loss: 0.020898158371685693\n",
      "New best score: 0.020898158371685693\n",
      "\n",
      "Epoch 356: train loss: 0.023710260617650414\n",
      "valid loss: 0.020890596393254534\n",
      "New best score: 0.020890596393254534\n",
      "\n",
      "Epoch 357: train loss: 0.023675812595966073\n",
      "valid loss: 0.020885888050830668\n",
      "New best score: 0.020885888050830668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model,training_execution_time,train_losses,valid_losses \u001b[39m=\u001b[39m training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,\n\u001b[0;32m      2\u001b[0m                                                             max_epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, save_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodels/DL/mlp_earlystop/simple_mlp_model_earlystop.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mD:\\repos\\mgr-anomaly-ts-xai\\my_shared_functions.py:271\u001b[0m, in \u001b[0;36mtraining_loop_and_saving_best_wandb\u001b[1;34m(model, training_generator, valid_generator, optimizer, criterion, max_epochs, apply_early_stopping, patience, verbose, save_path)\u001b[0m\n\u001b[0;32m    269\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    270\u001b[0m train_loss\u001b[39m=\u001b[39m[]\n\u001b[1;32m--> 271\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m training_generator:\n\u001b[0;32m    272\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    273\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x_batch)\n",
      "File \u001b[1;32mc:\\Users\\mcham\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\mcham\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mcham\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mcham\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\repos\\mgr-anomaly-ts-xai\\my_shared_functions.py:216\u001b[0m, in \u001b[0;36mFraudDatasetToDevice.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39m'\u001b[39m\u001b[39mGenerates one sample of data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx[index]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDEVICE), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEVICE)\n\u001b[0;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEVICE)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,\n",
    "                                                            max_epochs=500,verbose=True, save_path='models/DL/mlp_earlystop/simple_mlp_model_earlystop.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445.7395100593567\n"
     ]
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "start_time=time.time()\n",
    "# no need to set model in eval mode since there are no BN, Dropout layers\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.832              0.555     0.609                0.26"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_earlystop)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.832</td></tr><tr><td>Average precision</td><td>0.555</td></tr><tr><td>Card Precision@100</td><td>0.26</td></tr><tr><td>F1 score</td><td>0.609</td></tr><tr><td>Prediction execution time</td><td>0.013</td></tr><tr><td>Training execution time</td><td>2445.73951</td></tr><tr><td>train loss</td><td>0.02771</td></tr><tr><td>val loss</td><td>0.02278</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">comic-fog-58</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/3qi0g7zo\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/3qi0g7zo</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_155704-3qi0g7zo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_earlystop', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and early stopping on')\n",
    "mlp_artifact.add_dir('models/DL/mlp_earlystop')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14evlvba) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>0.02368</td></tr><tr><td>val loss</td><td>0.02089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sky-69</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/14evlvba\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/14evlvba</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_201538-14evlvba\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14evlvba). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b51016581d432dbca0d322f17189c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_220042-8bxfytzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz\" target=\"_blank\">vivid-voice-70</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-ts-xai\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.04304343708135103\n",
      "valid loss: 0.022782339930685666\n",
      "New best score: 0.022782339930685666\n",
      "\n",
      "Epoch 1: train loss: 0.024800907236614312\n",
      "valid loss: 0.020320492451187555\n",
      "New best score: 0.020320492451187555\n",
      "\n",
      "Epoch 2: train loss: 0.022864724756315968\n",
      "valid loss: 0.019964506324946996\n",
      "New best score: 0.019964506324946996\n",
      "\n",
      "Epoch 3: train loss: 0.02190224805079865\n",
      "valid loss: 0.020164376673846823\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 4: train loss: 0.021138214083414146\n",
      "valid loss: 0.019264124811276317\n",
      "New best score: 0.019264124811276317\n",
      "\n",
      "Epoch 5: train loss: 0.020224218546979124\n",
      "valid loss: 0.01902117400453656\n",
      "New best score: 0.01902117400453656\n",
      "\n",
      "Epoch 6: train loss: 0.01973835375529052\n",
      "valid loss: 0.019091747153019298\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 7: train loss: 0.019350004048692814\n",
      "valid loss: 0.018914142219476508\n",
      "New best score: 0.018914142219476508\n",
      "\n",
      "Epoch 8: train loss: 0.018840550201722144\n",
      "valid loss: 0.01943170726620035\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 9: train loss: 0.018541907965013763\n",
      "valid loss: 0.018915812917356303\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 10: train loss: 0.01831518779661627\n",
      "valid loss: 0.018900157220157643\n",
      "New best score: 0.018900157220157643\n",
      "\n",
      "Epoch 11: train loss: 0.01792869847016427\n",
      "valid loss: 0.02006839476507893\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 12: train loss: 0.01787691179751113\n",
      "valid loss: 0.018873483220712844\n",
      "New best score: 0.018873483220712844\n",
      "\n",
      "Epoch 13: train loss: 0.017319376305938404\n",
      "valid loss: 0.018961347108503647\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 14: train loss: 0.01706070191824816\n",
      "valid loss: 0.018536283771272824\n",
      "New best score: 0.018536283771272824\n",
      "\n",
      "Epoch 15: train loss: 0.01708758957149039\n",
      "valid loss: 0.01888867689207918\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 16: train loss: 0.01702439983524462\n",
      "valid loss: 0.019173046517641817\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 17: train loss: 0.016672237222499393\n",
      "valid loss: 0.019898832550744653\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "model,training_execution_time,train_losses_adam,valid_losses_adam = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=True,\n",
    "                                                                        save_path='models/DL/mlp_adam/simple_mlp_model_adam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320.8815701007843\n",
      "0.002000093460083008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0     0.86               0.62     0.672               0.271"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "start_time=time.time()\n",
    "# no need to set model in eval mode since there are no BN, Dropout layers\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_adam)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▃▄▂▂▂▂▂▂▂▄▂▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.86</td></tr><tr><td>Average precision</td><td>0.62</td></tr><tr><td>Card Precision@100</td><td>0.271</td></tr><tr><td>F1 score</td><td>0.672</td></tr><tr><td>Prediction execution time</td><td>0.002</td></tr><tr><td>Training execution time</td><td>320.88157</td></tr><tr><td>train loss</td><td>0.01667</td></tr><tr><td>val loss</td><td>0.0199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vivid-voice-70</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_220042-8bxfytzz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_adam', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and adam optimizer')\n",
    "mlp_artifact.add_dir('models/DL/mlp_adam')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f30bf511ee44a98cf74d8b1d6bf2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_220628-23rnyhzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs\" target=\"_blank\">neat-fog-71</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    dropout=0.2\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-ts-xai\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "model = SimpleFraudMLPWithDropout(len(input_features), 1000,0.2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=False,\n",
    "                                                                                                        save_path='models/DL/mlp_dropout/simple_mlp_model_dropout.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196.85899806022644\n",
      "0.0030002593994140625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.876              0.635     0.644                0.28"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_dropout)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▄▃▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.876</td></tr><tr><td>Average precision</td><td>0.635</td></tr><tr><td>Card Precision@100</td><td>0.28</td></tr><tr><td>F1 score</td><td>0.644</td></tr><tr><td>Prediction execution time</td><td>0.003</td></tr><tr><td>Training execution time</td><td>196.859</td></tr><tr><td>train loss</td><td>0.01917</td></tr><tr><td>val loss</td><td>0.019</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">neat-fog-71</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_220628-23rnyhzs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_dropout', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and dropout')\n",
    "mlp_artifact.add_dir('models/DL/mlp_dropout')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TX_AMOUNT',\n",
       " 'TX_DURING_WEEKEND',\n",
       " 'TX_DURING_NIGHT',\n",
       " 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW',\n",
       " 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW',\n",
       " 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_1DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_7DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_30DAY_WINDOW']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday(tx_datetime):\n",
    "    \n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    \n",
    "    return int(weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['TX_WEEKDAY'] = train_df.TX_DATETIME.apply(weekday)\n",
    "valid_df['TX_WEEKDAY'] = valid_df.TX_DATETIME.apply(weekday)\n",
    "input_categorical_features = ['TX_WEEKDAY','TERMINAL_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938f68555b0c4d969c626f890fa2ef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_221001-30lvhzr6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6\" target=\"_blank\">autumn-field-72</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed-extended',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0001,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    dropout=0.2,\n",
    "    embedding_size=10\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-ts-xai\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.08502790267285948\n",
      "valid loss: 0.029979193843052643\n",
      "New best score: 0.029979193843052643\n",
      "\n",
      "Epoch 1: train loss: 0.03248775874922047\n",
      "valid loss: 0.024811104128242528\n",
      "New best score: 0.024811104128242528\n",
      "\n",
      "Epoch 2: train loss: 0.02725523496405545\n",
      "valid loss: 0.02281399695258935\n",
      "New best score: 0.02281399695258935\n",
      "\n",
      "Epoch 3: train loss: 0.025421618136376157\n",
      "valid loss: 0.021918508669915564\n",
      "New best score: 0.021918508669915564\n",
      "\n",
      "Epoch 4: train loss: 0.02447954625011795\n",
      "valid loss: 0.02162936261819796\n",
      "New best score: 0.02162936261819796\n",
      "\n",
      "Epoch 5: train loss: 0.02352804730419906\n",
      "valid loss: 0.021723798720363393\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 6: train loss: 0.023122566236075826\n",
      "valid loss: 0.021430913876725768\n",
      "New best score: 0.021430913876725768\n",
      "\n",
      "Epoch 7: train loss: 0.0225583291919388\n",
      "valid loss: 0.021607249014746116\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 8: train loss: 0.022182895909626212\n",
      "valid loss: 0.021199198402019687\n",
      "New best score: 0.021199198402019687\n",
      "\n",
      "Epoch 9: train loss: 0.02162063721362253\n",
      "valid loss: 0.021264854739225878\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 10: train loss: 0.02121260527113309\n",
      "valid loss: 0.02138334601651366\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 11: train loss: 0.021083767425719043\n",
      "valid loss: 0.021322607970949873\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "training_generator,valid_generator,categorical_inputs_modalities = prepare_generators_with_categorical_features(train_df,valid_df, input_features, input_categorical_features, output_feature, DEVICE, batch_size=64)\n",
    "\n",
    "embedding_sizes = [10]*len(categorical_inputs_modalities)\n",
    "\n",
    "model = FraudMLPWithEmbedding(categorical_inputs_modalities,len(input_features),embedding_sizes, 1000,0.2, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "model,training_execution_time,train_losses_embedding,valid_losses_embedding = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=True,\n",
    "                                                                                            save_path='models/DL/mlp_embeddings/simple_mlp_model_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.27499628067017\n",
      "0.004000186920166016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.838              0.602     0.605                0.28"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "x_valid_with_cat = prepare_x_valid_with_categorical_features(train_df, valid_df, input_features, input_categorical_features)\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_valid_with_cat.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_embeddings)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.838</td></tr><tr><td>Average precision</td><td>0.602</td></tr><tr><td>Card Precision@100</td><td>0.28</td></tr><tr><td>F1 score</td><td>0.605</td></tr><tr><td>Prediction execution time</td><td>0.004</td></tr><tr><td>Training execution time</td><td>183.275</td></tr><tr><td>train loss</td><td>0.02108</td></tr><tr><td>val loss</td><td>0.02132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-field-72</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_221001-30lvhzr6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_embeddings', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and embedding layers')\n",
    "mlp_artifact.add_dir('models/DL/mlp_embeddings')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prequential grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudMLP(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, hidden_size=100,num_layers=1,p=0, input_size=len(input_features)):\n",
    "            super(FraudMLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.p = p\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            \n",
    "            self.fc_hidden=[]\n",
    "            for _ in range(num_layers-1):\n",
    "                self.fc_hidden.append(torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "                self.fc_hidden.append(torch.nn.ReLU())\n",
    "                \n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 2)\n",
    "            self.softmax = torch.nn.Softmax()\n",
    "            \n",
    "            self.dropout = torch.nn.Dropout(self.p)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            \n",
    "            hidden = self.fc1(x)\n",
    "            hidden = self.relu(hidden)             \n",
    "            hidden = self.dropout(hidden)\n",
    "            \n",
    "            for layer in self.fc_hidden:\n",
    "                hidden=layer(hidden)\n",
    "                hidden = self.dropout(hidden)\n",
    "            \n",
    "            output = self.fc2(hidden)\n",
    "            output = self.softmax(output)\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDatasetForPipe(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        'Initialization'\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = None\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y.values)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        if self.y is not None:\n",
    "            # DON'T ADD .to(DEVICE) BELOW!!!\n",
    "            # it will slow down training process more than 10 times\n",
    "            # return self.x[index].to(DEVICE), self.y[index].to(DEVICE)\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index], -1       \n",
    "            # return self.x[index].to(DEVICE), -1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.FraudMLP'>,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    FraudMLP,\n",
    "    max_epochs=2,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    "    dataset=FraudDatasetForPipe,\n",
    "    iterator_train__shuffle=True,\n",
    "    # device=DEVICE\n",
    ")\n",
    "net.set_params(train_split=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep columns that are needed as argument to custom scoring function\n",
    "# to reduce serialization time of transaction dataset\n",
    "transactions_df_scorer=transactions_df[['CUSTOMER_ID', 'TX_FRAUD','TX_TIME_DAYS']]\n",
    "\n",
    "card_precision_top_100 = sklearn.metrics.make_scorer(card_precision_top_k_custom, \n",
    "                                                     needs_proba=True, \n",
    "                                                     top_k=100, \n",
    "                                                     transactions_df=transactions_df_scorer)\n",
    "\n",
    "n_folds=4\n",
    "start_date_training_for_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "start_date_training_for_test = start_date_training+datetime.timedelta(days=(n_folds-1)*delta_test)\n",
    "delta_assessment = delta_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performance before the proper hp search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Total execution time: 82.48s\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "start_time=time.time()\n",
    "\n",
    "parameters = {\n",
    "    'clf__lr': [0.001 ],\n",
    "    'clf__batch_size': [64],\n",
    "    'clf__max_epochs': [10, 20],\n",
    "    'clf__module__hidden_size': [100],\n",
    "    'clf__module__num_layers': [1,2],\n",
    "    'clf__module__p': [0],\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc':'roc_auc',\n",
    "           'average_precision': 'average_precision',\n",
    "           'card_precision@100': card_precision_top_100,\n",
    "           }\n",
    "\n",
    "\n",
    "performance_metrics_list_grid=['roc_auc', 'average_precision', 'card_precision@100']\n",
    "performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100']\n",
    "\n",
    "performances_df_validation=prequential_grid_search(\n",
    "    transactions_df, net, \n",
    "    input_features, output_feature,\n",
    "    parameters, scoring, \n",
    "    start_date_training=start_date_training_with_valid,\n",
    "    n_folds=n_folds,\n",
    "    expe_type='Validation',\n",
    "    delta_train=delta_train, \n",
    "    delta_delay=delta_delay, \n",
    "    delta_assessment=delta_valid,\n",
    "    performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "    performance_metrics_list=performance_metrics_list)\n",
    "\n",
    "print(\"Validation: Total execution time: \"+str(round(time.time()-start_time,2))+\"s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My execution time on GPU: ~70-80s\n",
    "\n",
    "Handbook's execution time: 37.16s\n",
    "\n",
    "Hp search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'clf__lr': [0.001 , 0.0001, 0.0002],\n",
    "    'clf__batch_size': [64,128,256],\n",
    "    'clf__max_epochs': [10,20,40],\n",
    "    'clf__module__hidden_size': [500],\n",
    "    'clf__module__num_layers': [1,2],\n",
    "    'clf__module__p': [0,0.2,0.4],\n",
    "    'clf__module__input_size': [int(len(input_features))],\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc':'roc_auc',\n",
    "           'average_precision': 'average_precision',\n",
    "           'card_precision@100': card_precision_top_100,\n",
    "           }\n",
    "           \n",
    "performance_metrics_list_grid=['roc_auc', 'average_precision', 'card_precision@100']\n",
    "performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100']\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "performances_df=model_selection_wrapper(transactions_df, net, \n",
    "                                        input_features, output_feature,\n",
    "                                        parameters, scoring, \n",
    "                                        start_date_training_for_valid,\n",
    "                                        start_date_training_for_test,\n",
    "                                        n_folds=n_folds,\n",
    "                                        delta_train=delta_train, \n",
    "                                        delta_delay=delta_delay, \n",
    "                                        delta_assessment=delta_assessment,\n",
    "                                        performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                                        performance_metrics_list=performance_metrics_list,\n",
    "                                        n_jobs=1)\n",
    "\n",
    "\n",
    "execution_time_nn = time.time()-start_time\n",
    "\n",
    "parameters_dict=dict(performances_df['Parameters'])\n",
    "performances_df['Parameters summary']=[str(parameters_dict[i]['clf__lr'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__batch_size'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__max_epochs'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__module__p'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__module__num_layers'])\n",
    "                                   for i in range(len(parameters_dict))]\n",
    "\n",
    "performances_df_nn=performances_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above search in the handbook took ~120min\n",
    "\n",
    "my execution didn't finalize for over 600min...\n",
    "\n",
    "nvidia-smi shows around 20% GPU usage of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df) = get_train_test_set(transactions_df,start_date_training,\n",
    "                                       delta_train=7,delta_delay=7,delta_test=7)\n",
    "(train_df, test_df)=scaleData(train_df, test_df, input_features)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_test = torch.FloatTensor(test_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_test = torch.FloatTensor(test_df[output_feature].values)\n",
    "\n",
    "training_set = FraudDatasetToDevice(x_train, y_train, DEVICE)\n",
    "testing_set = FraudDatasetToDevice(x_test, y_test, DEVICE)\n",
    "\n",
    "training_generator,testing_generator = prepare_generators(training_set,testing_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudMLPHypertuned(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, input_size,hidden_size=500,num_layers=2,p=0.2):\n",
    "            super(FraudMLPHypertuned, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.p = p\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            \n",
    "            self.fc_hidden=[]\n",
    "            for i in range(num_layers-1):\n",
    "                self.fc_hidden.append(torch.nn.Linear(self.hidden_size, self.hidden_size).to(DEVICE))\n",
    "                self.fc_hidden.append(torch.nn.ReLU())\n",
    "                \n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            \n",
    "            self.dropout = torch.nn.Dropout(self.p)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            \n",
    "            hidden = self.fc1(x)\n",
    "            hidden = self.relu(hidden)             \n",
    "            hidden = self.dropout(hidden)\n",
    "            \n",
    "            for layer in self.fc_hidden:\n",
    "                hidden=layer(hidden)\n",
    "                hidden = self.dropout(hidden)\n",
    "            \n",
    "            output = self.fc2(hidden)\n",
    "            output = self.sigmoid(output)\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fkae5ab) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b057940ea2438c869fac6b1a55b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rosy-blaze-67</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/1fkae5ab\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/1fkae5ab</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_200200-1fkae5ab\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1fkae5ab). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660be5d21754be79382c1163fc9cf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_200531-242hzixy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy\" target=\"_blank\">zesty-gorge-68</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 500,\n",
    "    num_hidden_layers = 2,\n",
    "    optimizer='adam',\n",
    "    lr=0.001,\n",
    "    early_stopping=False,\n",
    "    max_epochs=20,\n",
    "    scale=True,\n",
    "    dropout=0.2\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-ts-xai\", config=config_mlp, tags=['mlp', 'imbalance-not-considered', 'hypertuned'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.05643073652563466\n",
      "valid loss: 0.024018097617004054\n",
      "\n",
      "Epoch 1: train loss: 0.02638936078540988\n",
      "valid loss: 0.02153680467099957\n",
      "\n",
      "Epoch 2: train loss: 0.024881045035415328\n",
      "valid loss: 0.020263867595097056\n",
      "\n",
      "Epoch 3: train loss: 0.02339501065342952\n",
      "valid loss: 0.019642853217953907\n",
      "\n",
      "Epoch 4: train loss: 0.022876052330221473\n",
      "valid loss: 0.019326516251773885\n",
      "\n",
      "Epoch 5: train loss: 0.022160829143681973\n",
      "valid loss: 0.01904257496028656\n",
      "\n",
      "Epoch 6: train loss: 0.02143366295834569\n",
      "valid loss: 0.019655168659777535\n",
      "\n",
      "Epoch 7: train loss: 0.021043319834753355\n",
      "valid loss: 0.018881842694638867\n",
      "\n",
      "Epoch 8: train loss: 0.0206349876358818\n",
      "valid loss: 0.018905052699891872\n",
      "\n",
      "Epoch 9: train loss: 0.020476300606205457\n",
      "valid loss: 0.019224997351209826\n",
      "\n",
      "Epoch 10: train loss: 0.019763412286676326\n",
      "valid loss: 0.01863673846995786\n",
      "\n",
      "Epoch 11: train loss: 0.02069019793411389\n",
      "valid loss: 0.018626018677713935\n",
      "\n",
      "Epoch 12: train loss: 0.019821298708868958\n",
      "valid loss: 0.01858314455561718\n",
      "\n",
      "Epoch 13: train loss: 0.019269375057982284\n",
      "valid loss: 0.018931785450290384\n",
      "\n",
      "Epoch 14: train loss: 0.01947174859678481\n",
      "valid loss: 0.01842006734911246\n",
      "\n",
      "Epoch 15: train loss: 0.019054680917040753\n",
      "valid loss: 0.018433201222851234\n",
      "\n",
      "Epoch 16: train loss: 0.01929063935926739\n",
      "valid loss: 0.0186118850693169\n",
      "\n",
      "Epoch 17: train loss: 0.018696328623975307\n",
      "valid loss: 0.01808142625539166\n",
      "\n",
      "Epoch 18: train loss: 0.018717897763488183\n",
      "valid loss: 0.018053305081554928\n",
      "\n",
      "Epoch 19: train loss: 0.018171166658472824\n",
      "valid loss: 0.018024880583444047\n"
     ]
    }
   ],
   "source": [
    "# Best hps taken from the handbook:\n",
    "# learning_rate=0.001\n",
    "# hidden layers 2\n",
    "# hidden size 500\n",
    "# batch_size 64\n",
    "# max epochs 20\n",
    "# dropout p 0.2\n",
    "\n",
    "model = FraudMLPHypertuned(len(input_features)).to(DEVICE)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,testing_generator,optimizer,criterion,\n",
    "                                                            apply_early_stopping=False, max_epochs=20,verbose=True, save_path='models/DL/mlp_hypertuned/mlp_hypertuned_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353.0805106163025\n",
      "0.0034372806549072266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.876              0.659     0.686               0.284"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_test.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=test_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_hypertuned)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.876</td></tr><tr><td>Average precision</td><td>0.659</td></tr><tr><td>Card Precision@100</td><td>0.284</td></tr><tr><td>F1 score</td><td>0.686</td></tr><tr><td>Prediction execution time</td><td>0.00344</td></tr><tr><td>Training execution time</td><td>353.08051</td></tr><tr><td>train loss</td><td>0.01817</td></tr><tr><td>val loss</td><td>0.01802</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zesty-gorge-68</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_200531-242hzixy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_hypertuned', type='mlp', description='trained hypertuned multilayer perceptron with 2 hidden layers')\n",
    "mlp_artifact.add_dir('models/DL/mlp_hypertuned')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a11e3a7eb51e2483c16a5d7cdfda12389edc17230fd81a6fc823433cff3faa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
