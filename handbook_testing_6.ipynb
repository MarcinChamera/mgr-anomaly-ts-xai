{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:\n",
    "\n",
    "@book{leborgne2022fraud,\n",
    "\n",
    "title={Reproducible Machine Learning for Credit Card Fraud Detection - Practical Handbook},\n",
    "\n",
    "author={Le Borgne, Yann-A{\\\"e}l and Siblini, Wissam and Lebichot, Bertrand and Bontempi, Gianluca},\n",
    "\n",
    "url={https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook},\n",
    "\n",
    "year={2022},\n",
    "\n",
    "publisher={Universit{\\'e} Libre de Bruxelles}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covered subchapters:\n",
    "* 7.2.3+ Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from skorch import NeuralNetClassifier\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  2 63257    2  1371    0     0   3902      0  0:00:16 --:--:--  0:00:16  3905\n",
      "100 63257  100 63257    0     0   162k      0 --:--:-- --:--:-- --:--:--  162k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.py\n",
    "%run shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run my_shared_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 992 ms\n",
      "919767 transactions loaded, containing 8195 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT = '../fraud-detection-handbook/simulated-data-transformed/data/'\n",
    "\n",
    "BEGIN_DATE = \"2018-06-11\"\n",
    "END_DATE = \"2018-09-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "output_feature=\"TX_FRAUD\"\n",
    "\n",
    "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_training = datetime.datetime.strptime(\"2018-07-25\", \"%Y-%m-%d\")\n",
    "delta_train=7\n",
    "delta_delay=7\n",
    "delta_test=7\n",
    "delta_valid = delta_test\n",
    "\n",
    "start_date_training_with_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "\n",
    "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training,\n",
    "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
    "\n",
    "(train_df, valid_df)=scaleData(train_df, valid_df, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\" \n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(\"Selected device is\",DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
    "\n",
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_valid = torch.FloatTensor(valid_df[output_feature].values)\n",
    "\n",
    "training_set = FraudDataset(x_train.to(DEVICE), y_train.to(DEVICE))\n",
    "valid_set = FraudDataset(x_valid.to(DEVICE), y_valid.to(DEVICE))\n",
    "\n",
    "training_generator,valid_generator = prepare_generators(training_set,valid_set,batch_size=64)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7266c54e7941389a3e1f040d0d419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_234955-2ek5yj8y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/2ek5yj8y\" target=\"_blank\">fresh-serenity-73</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='sgd',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=500,\n",
    "    scale=True,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.18884140769858002\n",
      "valid loss: 0.09599164529866104\n",
      "New best score: 0.09599164529866104\n",
      "\n",
      "Epoch 1: train loss: 0.09537467348799605\n",
      "valid loss: 0.07040649621198455\n",
      "New best score: 0.07040649621198455\n",
      "\n",
      "Epoch 2: train loss: 0.07512162991938537\n",
      "valid loss: 0.05619031202526503\n",
      "New best score: 0.05619031202526503\n",
      "\n",
      "Epoch 3: train loss: 0.06351821662434558\n",
      "valid loss: 0.048948103641737434\n",
      "New best score: 0.048948103641737434\n",
      "\n",
      "Epoch 4: train loss: 0.058062408105261996\n",
      "valid loss: 0.04489999113381563\n",
      "New best score: 0.04489999113381563\n",
      "\n",
      "Epoch 5: train loss: 0.05460313476310187\n",
      "valid loss: 0.04229192821049304\n",
      "New best score: 0.04229192821049304\n",
      "\n",
      "Epoch 6: train loss: 0.05212243972213267\n",
      "valid loss: 0.04033646309844389\n",
      "New best score: 0.04033646309844389\n",
      "\n",
      "Epoch 7: train loss: 0.05013998291259374\n",
      "valid loss: 0.03883849721479625\n",
      "New best score: 0.03883849721479625\n",
      "\n",
      "Epoch 8: train loss: 0.04848510026577311\n",
      "valid loss: 0.03766066508426952\n",
      "New best score: 0.03766066508426952\n",
      "\n",
      "Epoch 9: train loss: 0.047036537577129625\n",
      "valid loss: 0.03669081783611184\n",
      "New best score: 0.03669081783611184\n",
      "\n",
      "Epoch 10: train loss: 0.0457393995616461\n",
      "valid loss: 0.035890271051957565\n",
      "New best score: 0.035890271051957565\n",
      "\n",
      "Epoch 11: train loss: 0.044573769187793974\n",
      "valid loss: 0.03516009822794523\n",
      "New best score: 0.03516009822794523\n",
      "\n",
      "Epoch 12: train loss: 0.043501461467056135\n",
      "valid loss: 0.03450205446446864\n",
      "New best score: 0.03450205446446864\n",
      "\n",
      "Epoch 13: train loss: 0.04251794573220891\n",
      "valid loss: 0.033939654129208706\n",
      "New best score: 0.033939654129208706\n",
      "\n",
      "Epoch 14: train loss: 0.041599640399430274\n",
      "valid loss: 0.03342357987147458\n",
      "New best score: 0.03342357987147458\n",
      "\n",
      "Epoch 15: train loss: 0.04074920860112961\n",
      "valid loss: 0.03294656540110756\n",
      "New best score: 0.03294656540110756\n",
      "\n",
      "Epoch 16: train loss: 0.03995566545399907\n",
      "valid loss: 0.03250090173587743\n",
      "New best score: 0.03250090173587743\n",
      "\n",
      "Epoch 17: train loss: 0.03922048305470557\n",
      "valid loss: 0.03209583727391406\n",
      "New best score: 0.03209583727391406\n",
      "\n",
      "Epoch 18: train loss: 0.03853498373737117\n",
      "valid loss: 0.03170005736809171\n",
      "New best score: 0.03170005736809171\n",
      "\n",
      "Epoch 19: train loss: 0.03789579718117624\n",
      "valid loss: 0.031335051649921744\n",
      "New best score: 0.031335051649921744\n",
      "\n",
      "Epoch 20: train loss: 0.037278870401547766\n",
      "valid loss: 0.03099807581693572\n",
      "New best score: 0.03099807581693572\n",
      "\n",
      "Epoch 21: train loss: 0.03671867217067505\n",
      "valid loss: 0.03066315498912521\n",
      "New best score: 0.03066315498912521\n",
      "\n",
      "Epoch 22: train loss: 0.03624108586536559\n",
      "valid loss: 0.030347348081457105\n",
      "New best score: 0.030347348081457105\n",
      "\n",
      "Epoch 23: train loss: 0.03571299882605672\n",
      "valid loss: 0.0300654796326306\n",
      "New best score: 0.0300654796326306\n",
      "\n",
      "Epoch 24: train loss: 0.035295988452114825\n",
      "valid loss: 0.029776152048022798\n",
      "New best score: 0.029776152048022798\n",
      "\n",
      "Epoch 25: train loss: 0.0348616097526506\n",
      "valid loss: 0.029491883948952385\n",
      "New best score: 0.029491883948952385\n",
      "\n",
      "Epoch 26: train loss: 0.03446104326378466\n",
      "valid loss: 0.02920870775301684\n",
      "New best score: 0.02920870775301684\n",
      "\n",
      "Epoch 27: train loss: 0.034099504229329974\n",
      "valid loss: 0.02896354766051427\n",
      "New best score: 0.02896354766051427\n",
      "\n",
      "Epoch 28: train loss: 0.03382016532566154\n",
      "valid loss: 0.02870541340114322\n",
      "New best score: 0.02870541340114322\n",
      "\n",
      "Epoch 29: train loss: 0.033462004034250946\n",
      "valid loss: 0.02847939526149032\n",
      "New best score: 0.02847939526149032\n",
      "\n",
      "Epoch 30: train loss: 0.03319157916276257\n",
      "valid loss: 0.028251789103371512\n",
      "New best score: 0.028251789103371512\n",
      "\n",
      "Epoch 31: train loss: 0.032883544928340204\n",
      "valid loss: 0.02806911204249468\n",
      "New best score: 0.02806911204249468\n",
      "\n",
      "Epoch 32: train loss: 0.03263145480898663\n",
      "valid loss: 0.02785512773191268\n",
      "New best score: 0.02785512773191268\n",
      "\n",
      "Epoch 33: train loss: 0.03240231535823698\n",
      "valid loss: 0.027676069916815843\n",
      "New best score: 0.027676069916815843\n",
      "\n",
      "Epoch 34: train loss: 0.03216057135010197\n",
      "valid loss: 0.02749475129521181\n",
      "New best score: 0.02749475129521181\n",
      "\n",
      "Epoch 35: train loss: 0.031945615677210853\n",
      "valid loss: 0.02732240001218134\n",
      "New best score: 0.02732240001218134\n",
      "\n",
      "Epoch 36: train loss: 0.03174435266326372\n",
      "valid loss: 0.027141551411497017\n",
      "New best score: 0.027141551411497017\n",
      "\n",
      "Epoch 37: train loss: 0.031551498669836774\n",
      "valid loss: 0.026972553603817288\n",
      "New best score: 0.026972553603817288\n",
      "\n",
      "Epoch 38: train loss: 0.03136844916462359\n",
      "valid loss: 0.02682769146996308\n",
      "New best score: 0.02682769146996308\n",
      "\n",
      "Epoch 39: train loss: 0.03119337580678054\n",
      "valid loss: 0.026678752909597463\n",
      "New best score: 0.026678752909597463\n",
      "\n",
      "Epoch 40: train loss: 0.03105650913833395\n",
      "valid loss: 0.026550775978603818\n",
      "New best score: 0.026550775978603818\n",
      "\n",
      "Epoch 41: train loss: 0.03085905070692216\n",
      "valid loss: 0.02639107217063512\n",
      "New best score: 0.02639107217063512\n",
      "\n",
      "Epoch 42: train loss: 0.03071764774742819\n",
      "valid loss: 0.02627537064162276\n",
      "New best score: 0.02627537064162276\n",
      "\n",
      "Epoch 43: train loss: 0.030567301092111105\n",
      "valid loss: 0.02617614301029924\n",
      "New best score: 0.02617614301029924\n",
      "\n",
      "Epoch 44: train loss: 0.030450508854468022\n",
      "valid loss: 0.02604660395641211\n",
      "New best score: 0.02604660395641211\n",
      "\n",
      "Epoch 45: train loss: 0.03033829995295428\n",
      "valid loss: 0.025932384792379994\n",
      "New best score: 0.025932384792379994\n",
      "\n",
      "Epoch 46: train loss: 0.030235025959956034\n",
      "valid loss: 0.025819392036033218\n",
      "New best score: 0.025819392036033218\n",
      "\n",
      "Epoch 47: train loss: 0.03011142463555345\n",
      "valid loss: 0.02572100722812664\n",
      "New best score: 0.02572100722812664\n",
      "\n",
      "Epoch 48: train loss: 0.029938491106079403\n",
      "valid loss: 0.02562013805540264\n",
      "New best score: 0.02562013805540264\n",
      "\n",
      "Epoch 49: train loss: 0.029894571641919075\n",
      "valid loss: 0.025525125677459164\n",
      "New best score: 0.025525125677459164\n",
      "\n",
      "Epoch 50: train loss: 0.029716727191796764\n",
      "valid loss: 0.025437228384700684\n",
      "New best score: 0.025437228384700684\n",
      "\n",
      "Epoch 51: train loss: 0.02961234057182136\n",
      "valid loss: 0.02534002801618088\n",
      "New best score: 0.02534002801618088\n",
      "\n",
      "Epoch 52: train loss: 0.029511563300015982\n",
      "valid loss: 0.025257011955761393\n",
      "New best score: 0.025257011955761393\n",
      "\n",
      "Epoch 53: train loss: 0.02941311411355772\n",
      "valid loss: 0.02517716624645228\n",
      "New best score: 0.02517716624645228\n",
      "\n",
      "Epoch 54: train loss: 0.029320357431862308\n",
      "valid loss: 0.025084164723524933\n",
      "New best score: 0.025084164723524933\n",
      "\n",
      "Epoch 55: train loss: 0.02922975687136418\n",
      "valid loss: 0.025002729976764428\n",
      "New best score: 0.025002729976764428\n",
      "\n",
      "Epoch 56: train loss: 0.0291406313461614\n",
      "valid loss: 0.02493745628407002\n",
      "New best score: 0.02493745628407002\n",
      "\n",
      "Epoch 57: train loss: 0.02905403808216528\n",
      "valid loss: 0.024852179074159014\n",
      "New best score: 0.024852179074159014\n",
      "\n",
      "Epoch 58: train loss: 0.02897651787257118\n",
      "valid loss: 0.024789819965684943\n",
      "New best score: 0.024789819965684943\n",
      "\n",
      "Epoch 59: train loss: 0.028890448679117844\n",
      "valid loss: 0.024711346143343447\n",
      "New best score: 0.024711346143343447\n",
      "\n",
      "Epoch 60: train loss: 0.02881169494451489\n",
      "valid loss: 0.024644802028515697\n",
      "New best score: 0.024644802028515697\n",
      "\n",
      "Epoch 61: train loss: 0.028736853256084372\n",
      "valid loss: 0.024579771333573433\n",
      "New best score: 0.024579771333573433\n",
      "\n",
      "Epoch 62: train loss: 0.028673463452520552\n",
      "valid loss: 0.024512061174002064\n",
      "New best score: 0.024512061174002064\n",
      "\n",
      "Epoch 63: train loss: 0.02859165107048906\n",
      "valid loss: 0.024448462847812442\n",
      "New best score: 0.024448462847812442\n",
      "\n",
      "Epoch 64: train loss: 0.028521148187527476\n",
      "valid loss: 0.024385216621361044\n",
      "New best score: 0.024385216621361044\n",
      "\n",
      "Epoch 65: train loss: 0.028497425557737134\n",
      "valid loss: 0.024329853069566754\n",
      "New best score: 0.024329853069566754\n",
      "\n",
      "Epoch 66: train loss: 0.02838805425454899\n",
      "valid loss: 0.024266438066456404\n",
      "New best score: 0.024266438066456404\n",
      "\n",
      "Epoch 67: train loss: 0.028320575787816073\n",
      "valid loss: 0.024205724226973306\n",
      "New best score: 0.024205724226973306\n",
      "\n",
      "Epoch 68: train loss: 0.028259414228230546\n",
      "valid loss: 0.02415006539174054\n",
      "New best score: 0.02415006539174054\n",
      "\n",
      "Epoch 69: train loss: 0.028196670018627416\n",
      "valid loss: 0.024098266753123748\n",
      "New best score: 0.024098266753123748\n",
      "\n",
      "Epoch 70: train loss: 0.02813848446887146\n",
      "valid loss: 0.024053983195070443\n",
      "New best score: 0.024053983195070443\n",
      "\n",
      "Epoch 71: train loss: 0.028092242509293257\n",
      "valid loss: 0.02399770420762321\n",
      "New best score: 0.02399770420762321\n",
      "\n",
      "Epoch 72: train loss: 0.028062642848147495\n",
      "valid loss: 0.02394984306565166\n",
      "New best score: 0.02394984306565166\n",
      "\n",
      "Epoch 73: train loss: 0.027965475726902812\n",
      "valid loss: 0.023906871662388272\n",
      "New best score: 0.023906871662388272\n",
      "\n",
      "Epoch 74: train loss: 0.027977239043911747\n",
      "valid loss: 0.023860550941756296\n",
      "New best score: 0.023860550941756296\n",
      "\n",
      "Epoch 75: train loss: 0.027862552677916555\n",
      "valid loss: 0.023813003472243688\n",
      "New best score: 0.023813003472243688\n",
      "\n",
      "Epoch 76: train loss: 0.027803645359811245\n",
      "valid loss: 0.023772247066122246\n",
      "New best score: 0.023772247066122246\n",
      "\n",
      "Epoch 77: train loss: 0.027755994493492676\n",
      "valid loss: 0.0237227820990143\n",
      "New best score: 0.0237227820990143\n",
      "\n",
      "Epoch 78: train loss: 0.027705605755906578\n",
      "valid loss: 0.023687468565137634\n",
      "New best score: 0.023687468565137634\n",
      "\n",
      "Epoch 79: train loss: 0.02765672986544527\n",
      "valid loss: 0.023641933910149987\n",
      "New best score: 0.023641933910149987\n",
      "\n",
      "Epoch 80: train loss: 0.02761018901091285\n",
      "valid loss: 0.023601592526312113\n",
      "New best score: 0.023601592526312113\n",
      "\n",
      "Epoch 81: train loss: 0.027561028681156695\n",
      "valid loss: 0.02355857725376846\n",
      "New best score: 0.02355857725376846\n",
      "\n",
      "Epoch 82: train loss: 0.027515498651005506\n",
      "valid loss: 0.023520229353633855\n",
      "New best score: 0.023520229353633855\n",
      "\n",
      "Epoch 83: train loss: 0.027472219304301264\n",
      "valid loss: 0.02347922124097412\n",
      "New best score: 0.02347922124097412\n",
      "\n",
      "Epoch 84: train loss: 0.0274277841745407\n",
      "valid loss: 0.023443994413382494\n",
      "New best score: 0.023443994413382494\n",
      "\n",
      "Epoch 85: train loss: 0.027386445636293017\n",
      "valid loss: 0.023401242290070418\n",
      "New best score: 0.023401242290070418\n",
      "\n",
      "Epoch 86: train loss: 0.02734147968345794\n",
      "valid loss: 0.0233654867324561\n",
      "New best score: 0.0233654867324561\n",
      "\n",
      "Epoch 87: train loss: 0.0272994779730898\n",
      "valid loss: 0.023332087425300373\n",
      "New best score: 0.023332087425300373\n",
      "\n",
      "Epoch 88: train loss: 0.02729735590963244\n",
      "valid loss: 0.02329234217710662\n",
      "New best score: 0.02329234217710662\n",
      "\n",
      "Epoch 89: train loss: 0.02721857617309691\n",
      "valid loss: 0.023259925592893124\n",
      "New best score: 0.023259925592893124\n",
      "\n",
      "Epoch 90: train loss: 0.027180360519388366\n",
      "valid loss: 0.023226531650554123\n",
      "New best score: 0.023226531650554123\n",
      "\n",
      "Epoch 91: train loss: 0.02714139921612839\n",
      "valid loss: 0.023193141085665128\n",
      "New best score: 0.023193141085665128\n",
      "\n",
      "Epoch 92: train loss: 0.027101265223781895\n",
      "valid loss: 0.023162678630386697\n",
      "New best score: 0.023162678630386697\n",
      "\n",
      "Epoch 93: train loss: 0.02707581631892443\n",
      "valid loss: 0.023133440462526845\n",
      "New best score: 0.023133440462526845\n",
      "\n",
      "Epoch 94: train loss: 0.027029507470713286\n",
      "valid loss: 0.02310085617942259\n",
      "New best score: 0.02310085617942259\n",
      "\n",
      "Epoch 95: train loss: 0.026993201237852437\n",
      "valid loss: 0.02306890252718966\n",
      "New best score: 0.02306890252718966\n",
      "\n",
      "Epoch 96: train loss: 0.026959127167436505\n",
      "valid loss: 0.023039018681107252\n",
      "New best score: 0.023039018681107252\n",
      "\n",
      "Epoch 97: train loss: 0.026921761029893507\n",
      "valid loss: 0.02300464199512428\n",
      "New best score: 0.02300464199512428\n",
      "\n",
      "Epoch 98: train loss: 0.026889328984632293\n",
      "valid loss: 0.022976954101608365\n",
      "New best score: 0.022976954101608365\n",
      "\n",
      "Epoch 99: train loss: 0.026855592242620182\n",
      "valid loss: 0.022947355152315305\n",
      "New best score: 0.022947355152315305\n",
      "\n",
      "Epoch 100: train loss: 0.026821344516653883\n",
      "valid loss: 0.022920780044363896\n",
      "New best score: 0.022920780044363896\n",
      "\n",
      "Epoch 101: train loss: 0.02679059246788023\n",
      "valid loss: 0.022896686347015405\n",
      "New best score: 0.022896686347015405\n",
      "\n",
      "Epoch 102: train loss: 0.026758258380297106\n",
      "valid loss: 0.02287070864364791\n",
      "New best score: 0.02287070864364791\n",
      "\n",
      "Epoch 103: train loss: 0.026728946354716916\n",
      "valid loss: 0.022842419093513592\n",
      "New best score: 0.022842419093513592\n",
      "\n",
      "Epoch 104: train loss: 0.02671791365074187\n",
      "valid loss: 0.022818030210612617\n",
      "New best score: 0.022818030210612617\n",
      "\n",
      "Epoch 105: train loss: 0.026664750848575135\n",
      "valid loss: 0.022796312201886746\n",
      "New best score: 0.022796312201886746\n",
      "\n",
      "Epoch 106: train loss: 0.026635397138777626\n",
      "valid loss: 0.02277305370179321\n",
      "New best score: 0.02277305370179321\n",
      "\n",
      "Epoch 107: train loss: 0.026605652173809162\n",
      "valid loss: 0.022748219781135066\n",
      "New best score: 0.022748219781135066\n",
      "\n",
      "Epoch 108: train loss: 0.026576082859069577\n",
      "valid loss: 0.02272394770740007\n",
      "New best score: 0.02272394770740007\n",
      "\n",
      "Epoch 109: train loss: 0.02661702894099716\n",
      "valid loss: 0.022698697370797005\n",
      "New best score: 0.022698697370797005\n",
      "\n",
      "Epoch 110: train loss: 0.026518767835620214\n",
      "valid loss: 0.022673533135783807\n",
      "New best score: 0.022673533135783807\n",
      "\n",
      "Epoch 111: train loss: 0.026492021635412234\n",
      "valid loss: 0.02265164432137321\n",
      "New best score: 0.02265164432137321\n",
      "\n",
      "Epoch 112: train loss: 0.026482544326312932\n",
      "valid loss: 0.022630054902123813\n",
      "New best score: 0.022630054902123813\n",
      "\n",
      "Epoch 113: train loss: 0.026437981229197487\n",
      "valid loss: 0.022605354994286002\n",
      "New best score: 0.022605354994286002\n",
      "\n",
      "Epoch 114: train loss: 0.02640406339102337\n",
      "valid loss: 0.02258776851242733\n",
      "New best score: 0.02258776851242733\n",
      "\n",
      "Epoch 115: train loss: 0.02638583687966163\n",
      "valid loss: 0.0225636675150331\n",
      "New best score: 0.0225636675150331\n",
      "\n",
      "Epoch 116: train loss: 0.02637315612351903\n",
      "valid loss: 0.022540757183136848\n",
      "New best score: 0.022540757183136848\n",
      "\n",
      "Epoch 117: train loss: 0.02633162555781031\n",
      "valid loss: 0.02252013946430498\n",
      "New best score: 0.02252013946430498\n",
      "\n",
      "Epoch 118: train loss: 0.026305504152985496\n",
      "valid loss: 0.0225018474526916\n",
      "New best score: 0.0225018474526916\n",
      "\n",
      "Epoch 119: train loss: 0.026280230807307063\n",
      "valid loss: 0.022480675623507845\n",
      "New best score: 0.022480675623507845\n",
      "\n",
      "Epoch 120: train loss: 0.02625577296148082\n",
      "valid loss: 0.02246055378640964\n",
      "New best score: 0.02246055378640964\n",
      "\n",
      "Epoch 121: train loss: 0.02623248292526654\n",
      "valid loss: 0.02244156839210503\n",
      "New best score: 0.02244156839210503\n",
      "\n",
      "Epoch 122: train loss: 0.02620874354673406\n",
      "valid loss: 0.02242004397119734\n",
      "New best score: 0.02242004397119734\n",
      "\n",
      "Epoch 123: train loss: 0.026185692501538828\n",
      "valid loss: 0.022402463638776775\n",
      "New best score: 0.022402463638776775\n",
      "\n",
      "Epoch 124: train loss: 0.026160209976246563\n",
      "valid loss: 0.022384466506279066\n",
      "New best score: 0.022384466506279066\n",
      "\n",
      "Epoch 125: train loss: 0.026139566820375375\n",
      "valid loss: 0.022364562223987444\n",
      "New best score: 0.022364562223987444\n",
      "\n",
      "Epoch 126: train loss: 0.026116687337691576\n",
      "valid loss: 0.02234826499721982\n",
      "New best score: 0.02234826499721982\n",
      "\n",
      "Epoch 127: train loss: 0.02609388114415733\n",
      "valid loss: 0.022329790375586987\n",
      "New best score: 0.022329790375586987\n",
      "\n",
      "Epoch 128: train loss: 0.026071639892802024\n",
      "valid loss: 0.022312735455032973\n",
      "New best score: 0.022312735455032973\n",
      "\n",
      "Epoch 129: train loss: 0.026048686788891478\n",
      "valid loss: 0.022296670438879212\n",
      "New best score: 0.022296670438879212\n",
      "\n",
      "Epoch 130: train loss: 0.026026383781146353\n",
      "valid loss: 0.022276923069106794\n",
      "New best score: 0.022276923069106794\n",
      "\n",
      "Epoch 131: train loss: 0.026003200864086157\n",
      "valid loss: 0.022262851424713574\n",
      "New best score: 0.022262851424713574\n",
      "\n",
      "Epoch 132: train loss: 0.026009805399710074\n",
      "valid loss: 0.02224734661800114\n",
      "New best score: 0.02224734661800114\n",
      "\n",
      "Epoch 133: train loss: 0.02596215801854059\n",
      "valid loss: 0.02222872475973861\n",
      "New best score: 0.02222872475973861\n",
      "\n",
      "Epoch 134: train loss: 0.025941751819798683\n",
      "valid loss: 0.02221164623102629\n",
      "New best score: 0.02221164623102629\n",
      "\n",
      "Epoch 135: train loss: 0.02592114556813834\n",
      "valid loss: 0.022195195176809387\n",
      "New best score: 0.022195195176809387\n",
      "\n",
      "Epoch 136: train loss: 0.025947994085119153\n",
      "valid loss: 0.022180682710657134\n",
      "New best score: 0.022180682710657134\n",
      "\n",
      "Epoch 137: train loss: 0.02588123822235613\n",
      "valid loss: 0.022163994680848852\n",
      "New best score: 0.022163994680848852\n",
      "\n",
      "Epoch 138: train loss: 0.025909939344190804\n",
      "valid loss: 0.02214986741395604\n",
      "New best score: 0.02214986741395604\n",
      "\n",
      "Epoch 139: train loss: 0.025840035977761848\n",
      "valid loss: 0.0221326884930661\n",
      "New best score: 0.0221326884930661\n",
      "\n",
      "Epoch 140: train loss: 0.025819982042479428\n",
      "valid loss: 0.02212119586004147\n",
      "New best score: 0.02212119586004147\n",
      "\n",
      "Epoch 141: train loss: 0.025801252435713823\n",
      "valid loss: 0.022104804715628274\n",
      "New best score: 0.022104804715628274\n",
      "\n",
      "Epoch 142: train loss: 0.025783932764706984\n",
      "valid loss: 0.022091434476743773\n",
      "New best score: 0.022091434476743773\n",
      "\n",
      "Epoch 143: train loss: 0.025765978619341094\n",
      "valid loss: 0.022076023409539746\n",
      "New best score: 0.022076023409539746\n",
      "\n",
      "Epoch 144: train loss: 0.02574604766737567\n",
      "valid loss: 0.02206156501913511\n",
      "New best score: 0.02206156501913511\n",
      "\n",
      "Epoch 145: train loss: 0.025727651650874702\n",
      "valid loss: 0.02204842684441885\n",
      "New best score: 0.02204842684441885\n",
      "\n",
      "Epoch 146: train loss: 0.02570920489257916\n",
      "valid loss: 0.022034381652244753\n",
      "New best score: 0.022034381652244753\n",
      "\n",
      "Epoch 147: train loss: 0.025710337012864428\n",
      "valid loss: 0.022019716055090535\n",
      "New best score: 0.022019716055090535\n",
      "\n",
      "Epoch 148: train loss: 0.02567235499507013\n",
      "valid loss: 0.02200717495143021\n",
      "New best score: 0.02200717495143021\n",
      "\n",
      "Epoch 149: train loss: 0.025655927393296564\n",
      "valid loss: 0.02199446117126155\n",
      "New best score: 0.02199446117126155\n",
      "\n",
      "Epoch 150: train loss: 0.025636470994932468\n",
      "valid loss: 0.021982041628965873\n",
      "New best score: 0.021982041628965873\n",
      "\n",
      "Epoch 151: train loss: 0.025620247057943003\n",
      "valid loss: 0.02196992489634006\n",
      "New best score: 0.02196992489634006\n",
      "\n",
      "Epoch 152: train loss: 0.02559951759788866\n",
      "valid loss: 0.021954710891267895\n",
      "New best score: 0.021954710891267895\n",
      "\n",
      "Epoch 153: train loss: 0.025598778337998746\n",
      "valid loss: 0.02194232663751041\n",
      "New best score: 0.02194232663751041\n",
      "\n",
      "Epoch 154: train loss: 0.025567573750636174\n",
      "valid loss: 0.021929554745011057\n",
      "New best score: 0.021929554745011057\n",
      "\n",
      "Epoch 155: train loss: 0.025551725511920313\n",
      "valid loss: 0.02191658641040963\n",
      "New best score: 0.02191658641040963\n",
      "\n",
      "Epoch 156: train loss: 0.025535793547880978\n",
      "valid loss: 0.02190369885219578\n",
      "New best score: 0.02190369885219578\n",
      "\n",
      "Epoch 157: train loss: 0.025556117996483062\n",
      "valid loss: 0.021889676257372967\n",
      "New best score: 0.021889676257372967\n",
      "\n",
      "Epoch 158: train loss: 0.025499542079844666\n",
      "valid loss: 0.02187779317119258\n",
      "New best score: 0.02187779317119258\n",
      "\n",
      "Epoch 159: train loss: 0.02548389396626683\n",
      "valid loss: 0.02186556023511827\n",
      "New best score: 0.02186556023511827\n",
      "\n",
      "Epoch 160: train loss: 0.02547098736307069\n",
      "valid loss: 0.021855689300240542\n",
      "New best score: 0.021855689300240542\n",
      "\n",
      "Epoch 161: train loss: 0.02545080445799888\n",
      "valid loss: 0.021844634754812235\n",
      "New best score: 0.021844634754812235\n",
      "\n",
      "Epoch 162: train loss: 0.025438796974972006\n",
      "valid loss: 0.02183366796941293\n",
      "New best score: 0.02183366796941293\n",
      "\n",
      "Epoch 163: train loss: 0.025420889451561034\n",
      "valid loss: 0.0218228606855278\n",
      "New best score: 0.0218228606855278\n",
      "\n",
      "Epoch 164: train loss: 0.025418775916882887\n",
      "valid loss: 0.02181067559781201\n",
      "New best score: 0.02181067559781201\n",
      "\n",
      "Epoch 165: train loss: 0.025390094056854877\n",
      "valid loss: 0.021799392437482606\n",
      "New best score: 0.021799392437482606\n",
      "\n",
      "Epoch 166: train loss: 0.025374589771693386\n",
      "valid loss: 0.02178770587697797\n",
      "New best score: 0.02178770587697797\n",
      "\n",
      "Epoch 167: train loss: 0.02543158170374573\n",
      "valid loss: 0.021776833065195594\n",
      "New best score: 0.021776833065195594\n",
      "\n",
      "Epoch 168: train loss: 0.025345652793081288\n",
      "valid loss: 0.021763569624014975\n",
      "New best score: 0.021763569624014975\n",
      "\n",
      "Epoch 169: train loss: 0.025387311579356524\n",
      "valid loss: 0.021753730218904553\n",
      "New best score: 0.021753730218904553\n",
      "\n",
      "Epoch 170: train loss: 0.025318838626041753\n",
      "valid loss: 0.02174330285712504\n",
      "New best score: 0.02174330285712504\n",
      "\n",
      "Epoch 171: train loss: 0.02529903325441867\n",
      "valid loss: 0.02173359432174905\n",
      "New best score: 0.02173359432174905\n",
      "\n",
      "Epoch 172: train loss: 0.02528462354598884\n",
      "valid loss: 0.021724429470205805\n",
      "New best score: 0.021724429470205805\n",
      "\n",
      "Epoch 173: train loss: 0.025272580039785816\n",
      "valid loss: 0.021711935765855218\n",
      "New best score: 0.021711935765855218\n",
      "\n",
      "Epoch 174: train loss: 0.025255531312093196\n",
      "valid loss: 0.021699873938324375\n",
      "New best score: 0.021699873938324375\n",
      "\n",
      "Epoch 175: train loss: 0.02526427692087716\n",
      "valid loss: 0.021689444255139002\n",
      "New best score: 0.021689444255139002\n",
      "\n",
      "Epoch 176: train loss: 0.025228837800507286\n",
      "valid loss: 0.021681224058599964\n",
      "New best score: 0.021681224058599964\n",
      "\n",
      "Epoch 177: train loss: 0.025213531051876712\n",
      "valid loss: 0.02166953146596706\n",
      "New best score: 0.02166953146596706\n",
      "\n",
      "Epoch 178: train loss: 0.02520131681324448\n",
      "valid loss: 0.0216592134686512\n",
      "New best score: 0.0216592134686512\n",
      "\n",
      "Epoch 179: train loss: 0.025188544320720617\n",
      "valid loss: 0.021651125487203902\n",
      "New best score: 0.021651125487203902\n",
      "\n",
      "Epoch 180: train loss: 0.02517176173617629\n",
      "valid loss: 0.021640545315938287\n",
      "New best score: 0.021640545315938287\n",
      "\n",
      "Epoch 181: train loss: 0.02515805228817127\n",
      "valid loss: 0.021630825151297807\n",
      "New best score: 0.021630825151297807\n",
      "\n",
      "Epoch 182: train loss: 0.025184068359615376\n",
      "valid loss: 0.021618888486978217\n",
      "New best score: 0.021618888486978217\n",
      "\n",
      "Epoch 183: train loss: 0.025137647717914675\n",
      "valid loss: 0.02160904478025137\n",
      "New best score: 0.02160904478025137\n",
      "\n",
      "Epoch 184: train loss: 0.02511987667734585\n",
      "valid loss: 0.02160231342066885\n",
      "New best score: 0.02160231342066885\n",
      "\n",
      "Epoch 185: train loss: 0.02510433579989125\n",
      "valid loss: 0.021592996943472147\n",
      "New best score: 0.021592996943472147\n",
      "\n",
      "Epoch 186: train loss: 0.025092813242395203\n",
      "valid loss: 0.021582480445125934\n",
      "New best score: 0.021582480445125934\n",
      "\n",
      "Epoch 187: train loss: 0.025078307494537093\n",
      "valid loss: 0.021574152409406758\n",
      "New best score: 0.021574152409406758\n",
      "\n",
      "Epoch 188: train loss: 0.025065247000254455\n",
      "valid loss: 0.021564099011721925\n",
      "New best score: 0.021564099011721925\n",
      "\n",
      "Epoch 189: train loss: 0.025117056235160633\n",
      "valid loss: 0.021556313971311067\n",
      "New best score: 0.021556313971311067\n",
      "\n",
      "Epoch 190: train loss: 0.025082274740180807\n",
      "valid loss: 0.02154752498305516\n",
      "New best score: 0.02154752498305516\n",
      "\n",
      "Epoch 191: train loss: 0.025028879128416007\n",
      "valid loss: 0.021538009668744355\n",
      "New best score: 0.021538009668744355\n",
      "\n",
      "Epoch 192: train loss: 0.025012261393062635\n",
      "valid loss: 0.02152985425892927\n",
      "New best score: 0.02152985425892927\n",
      "\n",
      "Epoch 193: train loss: 0.025001781076355905\n",
      "valid loss: 0.021520404266776226\n",
      "New best score: 0.021520404266776226\n",
      "\n",
      "Epoch 194: train loss: 0.02498900767089666\n",
      "valid loss: 0.021509849279301097\n",
      "New best score: 0.021509849279301097\n",
      "\n",
      "Epoch 195: train loss: 0.024977170732439442\n",
      "valid loss: 0.021503023724260807\n",
      "New best score: 0.021503023724260807\n",
      "\n",
      "Epoch 196: train loss: 0.024962768276983235\n",
      "valid loss: 0.021496102062046822\n",
      "New best score: 0.021496102062046822\n",
      "\n",
      "Epoch 197: train loss: 0.02498121338641248\n",
      "valid loss: 0.02148836321947824\n",
      "New best score: 0.02148836321947824\n",
      "\n",
      "Epoch 198: train loss: 0.024940116276927327\n",
      "valid loss: 0.02147904424478313\n",
      "New best score: 0.02147904424478313\n",
      "\n",
      "Epoch 199: train loss: 0.024927785652658623\n",
      "valid loss: 0.021470439234394765\n",
      "New best score: 0.021470439234394765\n",
      "\n",
      "Epoch 200: train loss: 0.02492533128116666\n",
      "valid loss: 0.0214622994431455\n",
      "New best score: 0.0214622994431455\n",
      "\n",
      "Epoch 201: train loss: 0.024901250142251433\n",
      "valid loss: 0.021452430083899954\n",
      "New best score: 0.021452430083899954\n",
      "\n",
      "Epoch 202: train loss: 0.024889891854638888\n",
      "valid loss: 0.021444718241507572\n",
      "New best score: 0.021444718241507572\n",
      "\n",
      "Epoch 203: train loss: 0.024880011944360785\n",
      "valid loss: 0.021435779200419048\n",
      "New best score: 0.021435779200419048\n",
      "\n",
      "Epoch 204: train loss: 0.024868187038883653\n",
      "valid loss: 0.021427393368773497\n",
      "New best score: 0.021427393368773497\n",
      "\n",
      "Epoch 205: train loss: 0.024855902356686662\n",
      "valid loss: 0.02141988398425453\n",
      "New best score: 0.02141988398425453\n",
      "\n",
      "Epoch 206: train loss: 0.02484429044738504\n",
      "valid loss: 0.021411876388162\n",
      "New best score: 0.021411876388162\n",
      "\n",
      "Epoch 207: train loss: 0.024831670671603957\n",
      "valid loss: 0.021401947766489276\n",
      "New best score: 0.021401947766489276\n",
      "\n",
      "Epoch 208: train loss: 0.024821453904564698\n",
      "valid loss: 0.021394105463541233\n",
      "New best score: 0.021394105463541233\n",
      "\n",
      "Epoch 209: train loss: 0.02481060475664839\n",
      "valid loss: 0.021388075152937523\n",
      "New best score: 0.021388075152937523\n",
      "\n",
      "Epoch 210: train loss: 0.0247978291298829\n",
      "valid loss: 0.021380102367965354\n",
      "New best score: 0.021380102367965354\n",
      "\n",
      "Epoch 211: train loss: 0.02478691729447573\n",
      "valid loss: 0.021375711799755642\n",
      "New best score: 0.021375711799755642\n",
      "\n",
      "Epoch 212: train loss: 0.024775164465257786\n",
      "valid loss: 0.021365987344488552\n",
      "New best score: 0.021365987344488552\n",
      "\n",
      "Epoch 213: train loss: 0.024764659175636878\n",
      "valid loss: 0.02135850619662729\n",
      "New best score: 0.02135850619662729\n",
      "\n",
      "Epoch 214: train loss: 0.024751066840337954\n",
      "valid loss: 0.021354454691428795\n",
      "New best score: 0.021354454691428795\n",
      "\n",
      "Epoch 215: train loss: 0.02480173870463892\n",
      "valid loss: 0.02134677039742527\n",
      "New best score: 0.02134677039742527\n",
      "\n",
      "Epoch 216: train loss: 0.02473218577579869\n",
      "valid loss: 0.021337412746263305\n",
      "New best score: 0.021337412746263305\n",
      "\n",
      "Epoch 217: train loss: 0.02475521405830937\n",
      "valid loss: 0.021328263755572875\n",
      "New best score: 0.021328263755572875\n",
      "\n",
      "Epoch 218: train loss: 0.02476132079625731\n",
      "valid loss: 0.021322648649734703\n",
      "New best score: 0.021322648649734703\n",
      "\n",
      "Epoch 219: train loss: 0.0246985773143884\n",
      "valid loss: 0.02131454424925443\n",
      "New best score: 0.02131454424925443\n",
      "\n",
      "Epoch 220: train loss: 0.02468941033519645\n",
      "valid loss: 0.021308674773510082\n",
      "New best score: 0.021308674773510082\n",
      "\n",
      "Epoch 221: train loss: 0.024677543313624418\n",
      "valid loss: 0.021303178894340698\n",
      "New best score: 0.021303178894340698\n",
      "\n",
      "Epoch 222: train loss: 0.024664215890639288\n",
      "valid loss: 0.021299509171848353\n",
      "New best score: 0.021299509171848353\n",
      "\n",
      "Epoch 223: train loss: 0.024655310388346098\n",
      "valid loss: 0.02128836611269449\n",
      "New best score: 0.02128836611269449\n",
      "\n",
      "Epoch 224: train loss: 0.024645131066278487\n",
      "valid loss: 0.02128142523773538\n",
      "New best score: 0.02128142523773538\n",
      "\n",
      "Epoch 225: train loss: 0.024634966464601563\n",
      "valid loss: 0.021274750076723005\n",
      "New best score: 0.021274750076723005\n",
      "\n",
      "Epoch 226: train loss: 0.02462454996918896\n",
      "valid loss: 0.02126852043369697\n",
      "New best score: 0.02126852043369697\n",
      "\n",
      "Epoch 227: train loss: 0.024629988069588462\n",
      "valid loss: 0.02126063492622443\n",
      "New best score: 0.02126063492622443\n",
      "\n",
      "Epoch 228: train loss: 0.024601581597480954\n",
      "valid loss: 0.021252487306140617\n",
      "New best score: 0.021252487306140617\n",
      "\n",
      "Epoch 229: train loss: 0.024589002304768046\n",
      "valid loss: 0.021252437241162807\n",
      "New best score: 0.021252437241162807\n",
      "\n",
      "Epoch 230: train loss: 0.024583024216938115\n",
      "valid loss: 0.021243270724024276\n",
      "New best score: 0.021243270724024276\n",
      "\n",
      "Epoch 231: train loss: 0.024571706687758627\n",
      "valid loss: 0.021237487991112483\n",
      "New best score: 0.021237487991112483\n",
      "\n",
      "Epoch 232: train loss: 0.024562947085653597\n",
      "valid loss: 0.02123491466096495\n",
      "New best score: 0.02123491466096495\n",
      "\n",
      "Epoch 233: train loss: 0.024552966135737445\n",
      "valid loss: 0.021222659383627506\n",
      "New best score: 0.021222659383627506\n",
      "\n",
      "Epoch 234: train loss: 0.024542024838724853\n",
      "valid loss: 0.02121369815257743\n",
      "New best score: 0.02121369815257743\n",
      "\n",
      "Epoch 235: train loss: 0.02453226217274314\n",
      "valid loss: 0.021206258360907827\n",
      "New best score: 0.021206258360907827\n",
      "\n",
      "Epoch 236: train loss: 0.024521654855449295\n",
      "valid loss: 0.021202422958583\n",
      "New best score: 0.021202422958583\n",
      "\n",
      "Epoch 237: train loss: 0.024513197357861147\n",
      "valid loss: 0.02119626773267772\n",
      "New best score: 0.02119626773267772\n",
      "\n",
      "Epoch 238: train loss: 0.02450074524916425\n",
      "valid loss: 0.021186141084325545\n",
      "New best score: 0.021186141084325545\n",
      "\n",
      "Epoch 239: train loss: 0.024493672932851724\n",
      "valid loss: 0.021182379679208465\n",
      "New best score: 0.021182379679208465\n",
      "\n",
      "Epoch 240: train loss: 0.02448368672774192\n",
      "valid loss: 0.02117454734299655\n",
      "New best score: 0.02117454734299655\n",
      "\n",
      "Epoch 241: train loss: 0.024519354744116952\n",
      "valid loss: 0.021168608404491526\n",
      "New best score: 0.021168608404491526\n",
      "\n",
      "Epoch 242: train loss: 0.024577925204353444\n",
      "valid loss: 0.021161747539700158\n",
      "New best score: 0.021161747539700158\n",
      "\n",
      "Epoch 243: train loss: 0.02445517597896367\n",
      "valid loss: 0.02115685400414014\n",
      "New best score: 0.02115685400414014\n",
      "\n",
      "Epoch 244: train loss: 0.024445173445137327\n",
      "valid loss: 0.02115016634617937\n",
      "New best score: 0.02115016634617937\n",
      "\n",
      "Epoch 245: train loss: 0.02443359797881592\n",
      "valid loss: 0.021148988717408394\n",
      "New best score: 0.021148988717408394\n",
      "\n",
      "Epoch 246: train loss: 0.02442705567255368\n",
      "valid loss: 0.02114134172895672\n",
      "New best score: 0.02114134172895672\n",
      "\n",
      "Epoch 247: train loss: 0.02441635524316183\n",
      "valid loss: 0.021134022852553117\n",
      "New best score: 0.021134022852553117\n",
      "\n",
      "Epoch 248: train loss: 0.024408545480202794\n",
      "valid loss: 0.021126237914117482\n",
      "New best score: 0.021126237914117482\n",
      "\n",
      "Epoch 249: train loss: 0.024399116552690954\n",
      "valid loss: 0.021122175852822306\n",
      "New best score: 0.021122175852822306\n",
      "\n",
      "Epoch 250: train loss: 0.024389895864166\n",
      "valid loss: 0.021115417314670842\n",
      "New best score: 0.021115417314670842\n",
      "\n",
      "Epoch 251: train loss: 0.024446437738350212\n",
      "valid loss: 0.02110621503668783\n",
      "New best score: 0.02110621503668783\n",
      "\n",
      "Epoch 252: train loss: 0.024371780529768905\n",
      "valid loss: 0.021101115955628708\n",
      "New best score: 0.021101115955628708\n",
      "\n",
      "Epoch 253: train loss: 0.024361136743575547\n",
      "valid loss: 0.02109596345233235\n",
      "New best score: 0.02109596345233235\n",
      "\n",
      "Epoch 254: train loss: 0.024389697067558298\n",
      "valid loss: 0.021091780040779128\n",
      "New best score: 0.021091780040779128\n",
      "\n",
      "Epoch 255: train loss: 0.024341412681242635\n",
      "valid loss: 0.02108413859860385\n",
      "New best score: 0.02108413859860385\n",
      "\n",
      "Epoch 256: train loss: 0.02433392606772213\n",
      "valid loss: 0.021077687461562987\n",
      "New best score: 0.021077687461562987\n",
      "\n",
      "Epoch 257: train loss: 0.024324224808108908\n",
      "valid loss: 0.021073978191597384\n",
      "New best score: 0.021073978191597384\n",
      "\n",
      "Epoch 258: train loss: 0.024322172173154456\n",
      "valid loss: 0.02106795713364767\n",
      "New best score: 0.02106795713364767\n",
      "\n",
      "Epoch 259: train loss: 0.024307718308869356\n",
      "valid loss: 0.021062007862420264\n",
      "New best score: 0.021062007862420264\n",
      "\n",
      "Epoch 260: train loss: 0.0243205848795944\n",
      "valid loss: 0.02105607321249724\n",
      "New best score: 0.02105607321249724\n",
      "\n",
      "Epoch 261: train loss: 0.02428843554722537\n",
      "valid loss: 0.021052238356084874\n",
      "New best score: 0.021052238356084874\n",
      "\n",
      "Epoch 262: train loss: 0.024280846289285483\n",
      "valid loss: 0.021046569290531045\n",
      "New best score: 0.021046569290531045\n",
      "\n",
      "Epoch 263: train loss: 0.02427144968314194\n",
      "valid loss: 0.02104014119477871\n",
      "New best score: 0.02104014119477871\n",
      "\n",
      "Epoch 264: train loss: 0.02426506148063738\n",
      "valid loss: 0.021035175936135058\n",
      "New best score: 0.021035175936135058\n",
      "\n",
      "Epoch 265: train loss: 0.024258959339045476\n",
      "valid loss: 0.021031666606609455\n",
      "New best score: 0.021031666606609455\n",
      "\n",
      "Epoch 266: train loss: 0.024246746434924946\n",
      "valid loss: 0.021022859021037833\n",
      "New best score: 0.021022859021037833\n",
      "\n",
      "Epoch 267: train loss: 0.02423712149901328\n",
      "valid loss: 0.021015712770282746\n",
      "New best score: 0.021015712770282746\n",
      "\n",
      "Epoch 268: train loss: 0.02422661461568766\n",
      "valid loss: 0.02100996811323187\n",
      "New best score: 0.02100996811323187\n",
      "\n",
      "Epoch 269: train loss: 0.02421890152706902\n",
      "valid loss: 0.021004172863181104\n",
      "New best score: 0.021004172863181104\n",
      "\n",
      "Epoch 270: train loss: 0.024211156174458997\n",
      "valid loss: 0.02100070770707239\n",
      "New best score: 0.02100070770707239\n",
      "\n",
      "Epoch 271: train loss: 0.024200823045044173\n",
      "valid loss: 0.020993658917026776\n",
      "New best score: 0.020993658917026776\n",
      "\n",
      "Epoch 272: train loss: 0.024193885759649876\n",
      "valid loss: 0.020993622734735828\n",
      "New best score: 0.020993622734735828\n",
      "\n",
      "Epoch 273: train loss: 0.024285561399805502\n",
      "valid loss: 0.02098238159739847\n",
      "New best score: 0.02098238159739847\n",
      "\n",
      "Epoch 274: train loss: 0.024174896440824066\n",
      "valid loss: 0.02097658607643609\n",
      "New best score: 0.02097658607643609\n",
      "\n",
      "Epoch 275: train loss: 0.024169626931742287\n",
      "valid loss: 0.02097327510200637\n",
      "New best score: 0.02097327510200637\n",
      "\n",
      "Epoch 276: train loss: 0.024160597404486346\n",
      "valid loss: 0.020971855383548316\n",
      "New best score: 0.020971855383548316\n",
      "\n",
      "Epoch 277: train loss: 0.024152291289793876\n",
      "valid loss: 0.020968300904301695\n",
      "New best score: 0.020968300904301695\n",
      "\n",
      "Epoch 278: train loss: 0.02414386090844671\n",
      "valid loss: 0.020963316966046534\n",
      "New best score: 0.020963316966046534\n",
      "\n",
      "Epoch 279: train loss: 0.024140505228266048\n",
      "valid loss: 0.020958252576242162\n",
      "New best score: 0.020958252576242162\n",
      "\n",
      "Epoch 280: train loss: 0.02417656657645757\n",
      "valid loss: 0.020954049667742197\n",
      "New best score: 0.020954049667742197\n",
      "\n",
      "Epoch 281: train loss: 0.02411970965959465\n",
      "valid loss: 0.02094851623332224\n",
      "New best score: 0.02094851623332224\n",
      "\n",
      "Epoch 282: train loss: 0.02411247791410499\n",
      "valid loss: 0.02094210703851109\n",
      "New best score: 0.02094210703851109\n",
      "\n",
      "Epoch 283: train loss: 0.024102891383508306\n",
      "valid loss: 0.020942788901580366\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 284: train loss: 0.02409426094857811\n",
      "valid loss: 0.020935720978785228\n",
      "New best score: 0.020935720978785228\n",
      "\n",
      "Epoch 285: train loss: 0.02413952938740901\n",
      "valid loss: 0.02092655575742207\n",
      "New best score: 0.02092655575742207\n",
      "\n",
      "Epoch 286: train loss: 0.024077640139683016\n",
      "valid loss: 0.020925446502142613\n",
      "New best score: 0.020925446502142613\n",
      "\n",
      "Epoch 287: train loss: 0.02406827424348046\n",
      "valid loss: 0.020921152540248473\n",
      "New best score: 0.020921152540248473\n",
      "\n",
      "Epoch 288: train loss: 0.02406262423396536\n",
      "valid loss: 0.020916337869360094\n",
      "New best score: 0.020916337869360094\n",
      "\n",
      "Epoch 289: train loss: 0.024055220636642838\n",
      "valid loss: 0.020908828970692996\n",
      "New best score: 0.020908828970692996\n",
      "\n",
      "Epoch 290: train loss: 0.02405595867677488\n",
      "valid loss: 0.02090178631832895\n",
      "New best score: 0.02090178631832895\n",
      "\n",
      "Epoch 291: train loss: 0.0240413946748448\n",
      "valid loss: 0.020897300202148004\n",
      "New best score: 0.020897300202148004\n",
      "\n",
      "Epoch 292: train loss: 0.02402990554671178\n",
      "valid loss: 0.020896845409016887\n",
      "New best score: 0.020896845409016887\n",
      "\n",
      "Epoch 293: train loss: 0.024023418465840118\n",
      "valid loss: 0.020891795458034924\n",
      "New best score: 0.020891795458034924\n",
      "\n",
      "Epoch 294: train loss: 0.024014614561003775\n",
      "valid loss: 0.020883109526346824\n",
      "New best score: 0.020883109526346824\n",
      "\n",
      "Epoch 295: train loss: 0.024022934634901308\n",
      "valid loss: 0.020884605685839865\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 296: train loss: 0.024043012010314627\n",
      "valid loss: 0.02087929837495645\n",
      "New best score: 0.02087929837495645\n",
      "\n",
      "Epoch 297: train loss: 0.023992301479068813\n",
      "valid loss: 0.02086944916795706\n",
      "New best score: 0.02086944916795706\n",
      "\n",
      "Epoch 298: train loss: 0.023984824233714112\n",
      "valid loss: 0.020864874186632187\n",
      "New best score: 0.020864874186632187\n",
      "\n",
      "Epoch 299: train loss: 0.023977021872820057\n",
      "valid loss: 0.02085734182829702\n",
      "New best score: 0.02085734182829702\n",
      "\n",
      "Epoch 300: train loss: 0.023967692165278415\n",
      "valid loss: 0.020855977326737867\n",
      "New best score: 0.020855977326737867\n",
      "\n",
      "Epoch 301: train loss: 0.023970817176972494\n",
      "valid loss: 0.02084788893560064\n",
      "New best score: 0.02084788893560064\n",
      "\n",
      "Epoch 302: train loss: 0.02395434337349114\n",
      "valid loss: 0.02084987776933604\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 303: train loss: 0.023947207205313\n",
      "valid loss: 0.020844098971032104\n",
      "New best score: 0.020844098971032104\n",
      "\n",
      "Epoch 304: train loss: 0.02393752714746643\n",
      "valid loss: 0.020836200475903344\n",
      "New best score: 0.020836200475903344\n",
      "\n",
      "Epoch 305: train loss: 0.02393100401129641\n",
      "valid loss: 0.020834676480497326\n",
      "New best score: 0.020834676480497326\n",
      "\n",
      "Epoch 306: train loss: 0.023922429437555025\n",
      "valid loss: 0.020835473467457864\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 307: train loss: 0.023918066779683236\n",
      "valid loss: 0.02082996638864892\n",
      "New best score: 0.02082996638864892\n",
      "\n",
      "Epoch 308: train loss: 0.023921949156674314\n",
      "valid loss: 0.020822666064579484\n",
      "New best score: 0.020822666064579484\n",
      "\n",
      "Epoch 309: train loss: 0.023909091550925948\n",
      "valid loss: 0.02082042034784931\n",
      "New best score: 0.02082042034784931\n",
      "\n",
      "Epoch 310: train loss: 0.02389366818249906\n",
      "valid loss: 0.020809291884697705\n",
      "New best score: 0.020809291884697705\n",
      "\n",
      "Epoch 311: train loss: 0.02389009109145291\n",
      "valid loss: 0.020807967107215514\n",
      "New best score: 0.020807967107215514\n",
      "\n",
      "Epoch 312: train loss: 0.023886184348325137\n",
      "valid loss: 0.020803743066587183\n",
      "New best score: 0.020803743066587183\n",
      "\n",
      "Epoch 313: train loss: 0.023872887159429215\n",
      "valid loss: 0.0208008414963475\n",
      "New best score: 0.0208008414963475\n",
      "\n",
      "Epoch 314: train loss: 0.023864553422958623\n",
      "valid loss: 0.02080023011579112\n",
      "New best score: 0.02080023011579112\n",
      "\n",
      "Epoch 315: train loss: 0.023858769510790252\n",
      "valid loss: 0.020798190334314966\n",
      "New best score: 0.020798190334314966\n",
      "\n",
      "Epoch 316: train loss: 0.023849231506663276\n",
      "valid loss: 0.020792114071222893\n",
      "New best score: 0.020792114071222893\n",
      "\n",
      "Epoch 317: train loss: 0.023844304559690315\n",
      "valid loss: 0.020783926792489825\n",
      "New best score: 0.020783926792489825\n",
      "\n",
      "Epoch 318: train loss: 0.023905429555338892\n",
      "valid loss: 0.020780479329465414\n",
      "New best score: 0.020780479329465414\n",
      "\n",
      "Epoch 319: train loss: 0.023828746291492436\n",
      "valid loss: 0.020778061263109878\n",
      "New best score: 0.020778061263109878\n",
      "\n",
      "Epoch 320: train loss: 0.02382343564294172\n",
      "valid loss: 0.02077162750351573\n",
      "New best score: 0.02077162750351573\n",
      "\n",
      "Epoch 321: train loss: 0.023861186959019568\n",
      "valid loss: 0.02076508105056755\n",
      "New best score: 0.02076508105056755\n",
      "\n",
      "Epoch 322: train loss: 0.023809695911002685\n",
      "valid loss: 0.02076119083029951\n",
      "New best score: 0.02076119083029951\n",
      "\n",
      "Epoch 323: train loss: 0.023800484786256723\n",
      "valid loss: 0.02075982219401571\n",
      "New best score: 0.02075982219401571\n",
      "\n",
      "Epoch 324: train loss: 0.0237932160178148\n",
      "valid loss: 0.02075778483566506\n",
      "New best score: 0.02075778483566506\n",
      "\n",
      "Epoch 325: train loss: 0.02378735587321626\n",
      "valid loss: 0.020749982116993367\n",
      "New best score: 0.020749982116993367\n",
      "\n",
      "Epoch 326: train loss: 0.023779731903129483\n",
      "valid loss: 0.020743520349227343\n",
      "New best score: 0.020743520349227343\n",
      "\n",
      "Epoch 327: train loss: 0.02377358156271442\n",
      "valid loss: 0.02073999674419303\n",
      "New best score: 0.02073999674419303\n",
      "\n",
      "Epoch 328: train loss: 0.023772520890865265\n",
      "valid loss: 0.02073166111783706\n",
      "New best score: 0.02073166111783706\n",
      "\n",
      "Epoch 329: train loss: 0.023759859388111093\n",
      "valid loss: 0.020731638473204035\n",
      "New best score: 0.020731638473204035\n",
      "\n",
      "Epoch 330: train loss: 0.023751894383603523\n",
      "valid loss: 0.02073069254849331\n",
      "New best score: 0.02073069254849331\n",
      "\n",
      "Epoch 331: train loss: 0.02374446820901914\n",
      "valid loss: 0.020722142268385244\n",
      "New best score: 0.020722142268385244\n",
      "\n",
      "Epoch 332: train loss: 0.023738934894842932\n",
      "valid loss: 0.02071935940778208\n",
      "New best score: 0.02071935940778208\n",
      "\n",
      "Epoch 333: train loss: 0.023732536331707032\n",
      "valid loss: 0.020716323736547998\n",
      "New best score: 0.020716323736547998\n",
      "\n",
      "Epoch 334: train loss: 0.023724971179381635\n",
      "valid loss: 0.020712886142118147\n",
      "New best score: 0.020712886142118147\n",
      "\n",
      "Epoch 335: train loss: 0.023718432492295636\n",
      "valid loss: 0.020710579857473495\n",
      "New best score: 0.020710579857473495\n",
      "\n",
      "Epoch 336: train loss: 0.023713838374623222\n",
      "valid loss: 0.020706573475671588\n",
      "New best score: 0.020706573475671588\n",
      "\n",
      "Epoch 337: train loss: 0.023704893648009446\n",
      "valid loss: 0.020702758923318118\n",
      "New best score: 0.020702758923318118\n",
      "\n",
      "Epoch 338: train loss: 0.023698371147637662\n",
      "valid loss: 0.020695776708779853\n",
      "New best score: 0.020695776708779853\n",
      "\n",
      "Epoch 339: train loss: 0.023693182867872826\n",
      "valid loss: 0.020691592558353535\n",
      "New best score: 0.020691592558353535\n",
      "\n",
      "Epoch 340: train loss: 0.023698191457542348\n",
      "valid loss: 0.020689335042655747\n",
      "New best score: 0.020689335042655747\n",
      "\n",
      "Epoch 341: train loss: 0.02367967573574059\n",
      "valid loss: 0.020686785104703957\n",
      "New best score: 0.020686785104703957\n",
      "\n",
      "Epoch 342: train loss: 0.023672045803993062\n",
      "valid loss: 0.02068225314593276\n",
      "New best score: 0.02068225314593276\n",
      "\n",
      "Epoch 343: train loss: 0.023665274243507638\n",
      "valid loss: 0.020676281927541944\n",
      "New best score: 0.020676281927541944\n",
      "\n",
      "Epoch 344: train loss: 0.02365375157347309\n",
      "valid loss: 0.020668121354049225\n",
      "New best score: 0.020668121354049225\n",
      "\n",
      "Epoch 345: train loss: 0.023653302011394664\n",
      "valid loss: 0.02066950954146209\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 346: train loss: 0.02368704973521521\n",
      "valid loss: 0.020663212824297984\n",
      "New best score: 0.020663212824297984\n",
      "\n",
      "Epoch 347: train loss: 0.023638386570659777\n",
      "valid loss: 0.020662837220676872\n",
      "New best score: 0.020662837220676872\n",
      "\n",
      "Epoch 348: train loss: 0.023632030683099076\n",
      "valid loss: 0.020658110138069754\n",
      "New best score: 0.020658110138069754\n",
      "\n",
      "Epoch 349: train loss: 0.023626720042966596\n",
      "valid loss: 0.020654599672248822\n",
      "New best score: 0.020654599672248822\n",
      "\n",
      "Epoch 350: train loss: 0.023617188317070294\n",
      "valid loss: 0.020646480549255674\n",
      "New best score: 0.020646480549255674\n",
      "\n",
      "Epoch 351: train loss: 0.02360917164874994\n",
      "valid loss: 0.020651651217687046\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 352: train loss: 0.023607377999450667\n",
      "valid loss: 0.02064227808162603\n",
      "New best score: 0.02064227808162603\n",
      "\n",
      "Epoch 353: train loss: 0.023600720862045416\n",
      "valid loss: 0.020639266534304628\n",
      "New best score: 0.020639266534304628\n",
      "\n",
      "Epoch 354: train loss: 0.023594465933928\n",
      "valid loss: 0.02063536946022828\n",
      "New best score: 0.02063536946022828\n",
      "\n",
      "Epoch 355: train loss: 0.02358706192699191\n",
      "valid loss: 0.020634858295756967\n",
      "New best score: 0.020634858295756967\n",
      "\n",
      "Epoch 356: train loss: 0.02363136715674817\n",
      "valid loss: 0.020628861904054323\n",
      "New best score: 0.020628861904054323\n",
      "\n",
      "Epoch 357: train loss: 0.023575173182941115\n",
      "valid loss: 0.020627950987769404\n",
      "New best score: 0.020627950987769404\n",
      "\n",
      "Epoch 358: train loss: 0.023568474714219797\n",
      "valid loss: 0.02062347793182855\n",
      "New best score: 0.02062347793182855\n",
      "\n",
      "Epoch 359: train loss: 0.023561355568597946\n",
      "valid loss: 0.020616983596652615\n",
      "New best score: 0.020616983596652615\n",
      "\n",
      "Epoch 360: train loss: 0.023557337721168185\n",
      "valid loss: 0.020615279173599118\n",
      "New best score: 0.020615279173599118\n",
      "\n",
      "Epoch 361: train loss: 0.023552325136381797\n",
      "valid loss: 0.020615332295277802\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 362: train loss: 0.023557903025552746\n",
      "valid loss: 0.02061193313349435\n",
      "New best score: 0.02061193313349435\n",
      "\n",
      "Epoch 363: train loss: 0.023536913885117344\n",
      "valid loss: 0.020608406493738785\n",
      "New best score: 0.020608406493738785\n",
      "\n",
      "Epoch 364: train loss: 0.02360386294237454\n",
      "valid loss: 0.020602501860822825\n",
      "New best score: 0.020602501860822825\n",
      "\n",
      "Epoch 365: train loss: 0.02352528777558332\n",
      "valid loss: 0.020597358924795747\n",
      "New best score: 0.020597358924795747\n",
      "\n",
      "Epoch 366: train loss: 0.023580964175066232\n",
      "valid loss: 0.02059413757227345\n",
      "New best score: 0.02059413757227345\n",
      "\n",
      "Epoch 367: train loss: 0.023509756972097024\n",
      "valid loss: 0.0205875214431578\n",
      "New best score: 0.0205875214431578\n",
      "\n",
      "Epoch 368: train loss: 0.02357421470469273\n",
      "valid loss: 0.020584423713228623\n",
      "New best score: 0.020584423713228623\n",
      "\n",
      "Epoch 369: train loss: 0.023503813323929028\n",
      "valid loss: 0.020582286080010195\n",
      "New best score: 0.020582286080010195\n",
      "\n",
      "Epoch 370: train loss: 0.023494631338901784\n",
      "valid loss: 0.020583844850755796\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 371: train loss: 0.023487436644720545\n",
      "valid loss: 0.02057480709279686\n",
      "New best score: 0.02057480709279686\n",
      "\n",
      "Epoch 372: train loss: 0.023483396046347454\n",
      "valid loss: 0.020571725222442615\n",
      "New best score: 0.020571725222442615\n",
      "\n",
      "Epoch 373: train loss: 0.023476739632467308\n",
      "valid loss: 0.020569328454084917\n",
      "New best score: 0.020569328454084917\n",
      "\n",
      "Epoch 374: train loss: 0.023470215906799336\n",
      "valid loss: 0.020568511486881536\n",
      "New best score: 0.020568511486881536\n",
      "\n",
      "Epoch 375: train loss: 0.023462611161301765\n",
      "valid loss: 0.020559928336760395\n",
      "New best score: 0.020559928336760395\n",
      "\n",
      "Epoch 376: train loss: 0.023458191720222954\n",
      "valid loss: 0.020560790695205052\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 377: train loss: 0.023452015721029135\n",
      "valid loss: 0.020559230714302375\n",
      "New best score: 0.020559230714302375\n",
      "\n",
      "Epoch 378: train loss: 0.02344680040388888\n",
      "valid loss: 0.020553829216906384\n",
      "New best score: 0.020553829216906384\n",
      "\n",
      "Epoch 379: train loss: 0.02344106434030081\n",
      "valid loss: 0.020549884093843573\n",
      "New best score: 0.020549884093843573\n",
      "\n",
      "Epoch 380: train loss: 0.023433105407112572\n",
      "valid loss: 0.020542659904116727\n",
      "New best score: 0.020542659904116727\n",
      "\n",
      "Epoch 381: train loss: 0.02342874534449387\n",
      "valid loss: 0.02054030872405017\n",
      "New best score: 0.02054030872405017\n",
      "\n",
      "Epoch 382: train loss: 0.023423627993893\n",
      "valid loss: 0.0205399699805102\n",
      "New best score: 0.0205399699805102\n",
      "\n",
      "Epoch 383: train loss: 0.0234168572470964\n",
      "valid loss: 0.02053676374715627\n",
      "New best score: 0.02053676374715627\n",
      "\n",
      "Epoch 384: train loss: 0.023403503642072786\n",
      "valid loss: 0.020524363831129298\n",
      "New best score: 0.020524363831129298\n",
      "\n",
      "Epoch 385: train loss: 0.02340857673978715\n",
      "valid loss: 0.020526409067896437\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 386: train loss: 0.02340216163526352\n",
      "valid loss: 0.02052459640133999\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 387: train loss: 0.023402853926500176\n",
      "valid loss: 0.02052173450968456\n",
      "New best score: 0.02052173450968456\n",
      "\n",
      "Epoch 388: train loss: 0.023410150479325063\n",
      "valid loss: 0.020517995880822662\n",
      "New best score: 0.020517995880822662\n",
      "\n",
      "Epoch 389: train loss: 0.02338228138521682\n",
      "valid loss: 0.020518345049092737\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 390: train loss: 0.0234138330931622\n",
      "valid loss: 0.020510228399062604\n",
      "New best score: 0.020510228399062604\n",
      "\n",
      "Epoch 391: train loss: 0.02337429354137446\n",
      "valid loss: 0.020513170198654975\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 392: train loss: 0.02336469026270024\n",
      "valid loss: 0.02050811480541645\n",
      "New best score: 0.02050811480541645\n",
      "\n",
      "Epoch 393: train loss: 0.023359646424775202\n",
      "valid loss: 0.020506026506225624\n",
      "New best score: 0.020506026506225624\n",
      "\n",
      "Epoch 394: train loss: 0.023354552367872265\n",
      "valid loss: 0.02050488863526777\n",
      "New best score: 0.02050488863526777\n",
      "\n",
      "Epoch 395: train loss: 0.023408248497316743\n",
      "valid loss: 0.020498223978981097\n",
      "New best score: 0.020498223978981097\n",
      "\n",
      "Epoch 396: train loss: 0.023341603082898014\n",
      "valid loss: 0.02049347007893533\n",
      "New best score: 0.02049347007893533\n",
      "\n",
      "Epoch 397: train loss: 0.02333512846278567\n",
      "valid loss: 0.02049118886358267\n",
      "New best score: 0.02049118886358267\n",
      "\n",
      "Epoch 398: train loss: 0.023331692477964665\n",
      "valid loss: 0.020491020000027543\n",
      "New best score: 0.020491020000027543\n",
      "\n",
      "Epoch 399: train loss: 0.023325737463629367\n",
      "valid loss: 0.020489410075034974\n",
      "New best score: 0.020489410075034974\n",
      "\n",
      "Epoch 400: train loss: 0.02331771281770263\n",
      "valid loss: 0.020492022197936895\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 401: train loss: 0.023314022312213717\n",
      "valid loss: 0.02049351763184684\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 402: train loss: 0.0233104374134766\n",
      "valid loss: 0.020482859176836097\n",
      "New best score: 0.020482859176836097\n",
      "\n",
      "Epoch 403: train loss: 0.02330275716970752\n",
      "valid loss: 0.02047966461082763\n",
      "New best score: 0.02047966461082763\n",
      "\n",
      "Epoch 404: train loss: 0.023319853827947455\n",
      "valid loss: 0.020472123831486007\n",
      "New best score: 0.020472123831486007\n",
      "\n",
      "Epoch 405: train loss: 0.023293187045363744\n",
      "valid loss: 0.020474340950939335\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 406: train loss: 0.023288535731547752\n",
      "valid loss: 0.020469843511419568\n",
      "New best score: 0.020469843511419568\n",
      "\n",
      "Epoch 407: train loss: 0.02328009030437868\n",
      "valid loss: 0.02046187208202779\n",
      "New best score: 0.02046187208202779\n",
      "\n",
      "Epoch 408: train loss: 0.023275732812866014\n",
      "valid loss: 0.020456443495514804\n",
      "New best score: 0.020456443495514804\n",
      "\n",
      "Epoch 409: train loss: 0.023303815001067124\n",
      "valid loss: 0.020456491694524966\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 410: train loss: 0.023266875051488334\n",
      "valid loss: 0.020456448689350527\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 411: train loss: 0.02326145416704298\n",
      "valid loss: 0.02045008257613134\n",
      "New best score: 0.02045008257613134\n",
      "\n",
      "Epoch 412: train loss: 0.023254084615497633\n",
      "valid loss: 0.02044658771635296\n",
      "New best score: 0.02044658771635296\n",
      "\n",
      "Epoch 413: train loss: 0.02324942542863859\n",
      "valid loss: 0.020443366060716343\n",
      "New best score: 0.020443366060716343\n",
      "\n",
      "Epoch 414: train loss: 0.023241689446188162\n",
      "valid loss: 0.02043865597730198\n",
      "New best score: 0.02043865597730198\n",
      "\n",
      "Epoch 415: train loss: 0.023237773714158205\n",
      "valid loss: 0.02043757860295262\n",
      "New best score: 0.02043757860295262\n",
      "\n",
      "Epoch 416: train loss: 0.02323276267345325\n",
      "valid loss: 0.02043466564829121\n",
      "New best score: 0.02043466564829121\n",
      "\n",
      "Epoch 417: train loss: 0.023227665613898982\n",
      "valid loss: 0.02043590833853141\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 418: train loss: 0.023219709640163236\n",
      "valid loss: 0.020431909059725382\n",
      "New best score: 0.020431909059725382\n",
      "\n",
      "Epoch 419: train loss: 0.02321731672599224\n",
      "valid loss: 0.02042984586024021\n",
      "New best score: 0.02042984586024021\n",
      "\n",
      "Epoch 420: train loss: 0.023211113934659466\n",
      "valid loss: 0.020425047275112403\n",
      "New best score: 0.020425047275112403\n",
      "\n",
      "Epoch 421: train loss: 0.02320629296904974\n",
      "valid loss: 0.0204188514692028\n",
      "New best score: 0.0204188514692028\n",
      "\n",
      "Epoch 422: train loss: 0.023200210926435767\n",
      "valid loss: 0.020416386390992254\n",
      "New best score: 0.020416386390992254\n",
      "\n",
      "Epoch 423: train loss: 0.02319429256066283\n",
      "valid loss: 0.020420780740208047\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 424: train loss: 0.023185902812582867\n",
      "valid loss: 0.020408449222463564\n",
      "New best score: 0.020408449222463564\n",
      "\n",
      "Epoch 425: train loss: 0.023183869548650596\n",
      "valid loss: 0.020410362202991066\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 426: train loss: 0.023247321576923913\n",
      "valid loss: 0.02040738563342623\n",
      "New best score: 0.02040738563342623\n",
      "\n",
      "Epoch 427: train loss: 0.023173215931067005\n",
      "valid loss: 0.02040147324451516\n",
      "New best score: 0.02040147324451516\n",
      "\n",
      "Epoch 428: train loss: 0.023169336705818618\n",
      "valid loss: 0.020400247157067796\n",
      "New best score: 0.020400247157067796\n",
      "\n",
      "Epoch 429: train loss: 0.023162949374379797\n",
      "valid loss: 0.020395552580020952\n",
      "New best score: 0.020395552580020952\n",
      "\n",
      "Epoch 430: train loss: 0.023158799238604\n",
      "valid loss: 0.02040040281777087\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 431: train loss: 0.02315352972154705\n",
      "valid loss: 0.02039613291819536\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 432: train loss: 0.023147956995855305\n",
      "valid loss: 0.020389018612775776\n",
      "New best score: 0.020389018612775776\n",
      "\n",
      "Epoch 433: train loss: 0.023142461438346956\n",
      "valid loss: 0.020388081340624264\n",
      "New best score: 0.020388081340624264\n",
      "\n",
      "Epoch 434: train loss: 0.02313851061136197\n",
      "valid loss: 0.020382831424155756\n",
      "New best score: 0.020382831424155756\n",
      "\n",
      "Epoch 435: train loss: 0.02316354169863871\n",
      "valid loss: 0.020384017922726393\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 436: train loss: 0.023125131733469192\n",
      "valid loss: 0.02038868140218663\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 437: train loss: 0.023121561492872127\n",
      "valid loss: 0.020385620939983903\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,\n",
    "                                                            max_epochs=500,verbose=True, save_path='models/DL/mlp_earlystop/simple_mlp_model_earlystop.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7615.11332654953\n"
     ]
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "start_time=time.time()\n",
    "# no need to set model in eval mode since there are no BN, Dropout layers\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.866              0.613     0.624               0.284"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_earlystop)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.866</td></tr><tr><td>Average precision</td><td>0.613</td></tr><tr><td>Card Precision@100</td><td>0.284</td></tr><tr><td>F1 score</td><td>0.624</td></tr><tr><td>Prediction execution time</td><td>0.017</td></tr><tr><td>Training execution time</td><td>7615.11333</td></tr><tr><td>train loss</td><td>0.02312</td></tr><tr><td>val loss</td><td>0.02039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-serenity-73</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/2ek5yj8y\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/2ek5yj8y</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_234955-2ek5yj8y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_earlystop', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and early stopping on')\n",
    "mlp_artifact.add_dir('models/DL/mlp_earlystop')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:14evlvba) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>0.02368</td></tr><tr><td>val loss</td><td>0.02089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sky-69</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/14evlvba\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/14evlvba</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_201538-14evlvba\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:14evlvba). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b51016581d432dbca0d322f17189c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_220042-8bxfytzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz\" target=\"_blank\">vivid-voice-70</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.04304343708135103\n",
      "valid loss: 0.022782339930685666\n",
      "New best score: 0.022782339930685666\n",
      "\n",
      "Epoch 1: train loss: 0.024800907236614312\n",
      "valid loss: 0.020320492451187555\n",
      "New best score: 0.020320492451187555\n",
      "\n",
      "Epoch 2: train loss: 0.022864724756315968\n",
      "valid loss: 0.019964506324946996\n",
      "New best score: 0.019964506324946996\n",
      "\n",
      "Epoch 3: train loss: 0.02190224805079865\n",
      "valid loss: 0.020164376673846823\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 4: train loss: 0.021138214083414146\n",
      "valid loss: 0.019264124811276317\n",
      "New best score: 0.019264124811276317\n",
      "\n",
      "Epoch 5: train loss: 0.020224218546979124\n",
      "valid loss: 0.01902117400453656\n",
      "New best score: 0.01902117400453656\n",
      "\n",
      "Epoch 6: train loss: 0.01973835375529052\n",
      "valid loss: 0.019091747153019298\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 7: train loss: 0.019350004048692814\n",
      "valid loss: 0.018914142219476508\n",
      "New best score: 0.018914142219476508\n",
      "\n",
      "Epoch 8: train loss: 0.018840550201722144\n",
      "valid loss: 0.01943170726620035\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 9: train loss: 0.018541907965013763\n",
      "valid loss: 0.018915812917356303\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 10: train loss: 0.01831518779661627\n",
      "valid loss: 0.018900157220157643\n",
      "New best score: 0.018900157220157643\n",
      "\n",
      "Epoch 11: train loss: 0.01792869847016427\n",
      "valid loss: 0.02006839476507893\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 12: train loss: 0.01787691179751113\n",
      "valid loss: 0.018873483220712844\n",
      "New best score: 0.018873483220712844\n",
      "\n",
      "Epoch 13: train loss: 0.017319376305938404\n",
      "valid loss: 0.018961347108503647\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 14: train loss: 0.01706070191824816\n",
      "valid loss: 0.018536283771272824\n",
      "New best score: 0.018536283771272824\n",
      "\n",
      "Epoch 15: train loss: 0.01708758957149039\n",
      "valid loss: 0.01888867689207918\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 16: train loss: 0.01702439983524462\n",
      "valid loss: 0.019173046517641817\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 17: train loss: 0.016672237222499393\n",
      "valid loss: 0.019898832550744653\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "model,training_execution_time,train_losses_adam,valid_losses_adam = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=True,\n",
    "                                                                        save_path='models/DL/mlp_adam/simple_mlp_model_adam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320.8815701007843\n",
      "0.002000093460083008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0     0.86               0.62     0.672               0.271"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "start_time=time.time()\n",
    "# no need to set model in eval mode since there are no BN, Dropout layers\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_adam)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▃▄▂▂▂▂▂▂▂▄▂▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.86</td></tr><tr><td>Average precision</td><td>0.62</td></tr><tr><td>Card Precision@100</td><td>0.271</td></tr><tr><td>F1 score</td><td>0.672</td></tr><tr><td>Prediction execution time</td><td>0.002</td></tr><tr><td>Training execution time</td><td>320.88157</td></tr><tr><td>train loss</td><td>0.01667</td></tr><tr><td>val loss</td><td>0.0199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vivid-voice-70</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/8bxfytzz</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_220042-8bxfytzz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_adam', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and adam optimizer')\n",
    "mlp_artifact.add_dir('models/DL/mlp_adam')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f30bf511ee44a98cf74d8b1d6bf2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_220628-23rnyhzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs\" target=\"_blank\">neat-fog-71</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0005,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    dropout=0.2,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "model = SimpleFraudMLPWithDropout(len(input_features), 1000,0.2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=False,\n",
    "                                                                                                        save_path='models/DL/mlp_dropout/simple_mlp_model_dropout.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196.85899806022644\n",
      "0.0030002593994140625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.876              0.635     0.644                0.28"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_valid.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_dropout)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▄▃▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.876</td></tr><tr><td>Average precision</td><td>0.635</td></tr><tr><td>Card Precision@100</td><td>0.28</td></tr><tr><td>F1 score</td><td>0.644</td></tr><tr><td>Prediction execution time</td><td>0.003</td></tr><tr><td>Training execution time</td><td>196.859</td></tr><tr><td>train loss</td><td>0.01917</td></tr><tr><td>val loss</td><td>0.019</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">neat-fog-71</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/23rnyhzs</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_220628-23rnyhzs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_dropout', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and dropout')\n",
    "mlp_artifact.add_dir('models/DL/mlp_dropout')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TX_AMOUNT',\n",
       " 'TX_DURING_WEEKEND',\n",
       " 'TX_DURING_NIGHT',\n",
       " 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW',\n",
       " 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW',\n",
       " 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
       " 'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_1DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_7DAY_WINDOW',\n",
       " 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
       " 'TERMINAL_ID_RISK_30DAY_WINDOW']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday(tx_datetime):\n",
    "    \n",
    "    # Transform date into weekday (0 is Monday, 6 is Sunday)\n",
    "    weekday = tx_datetime.weekday()\n",
    "    \n",
    "    return int(weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['TX_WEEKDAY'] = train_df.TX_DATETIME.apply(weekday)\n",
    "valid_df['TX_WEEKDAY'] = valid_df.TX_DATETIME.apply(weekday)\n",
    "input_categorical_features = ['TX_WEEKDAY','TERMINAL_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938f68555b0c4d969c626f890fa2ef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_221001-30lvhzr6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6\" target=\"_blank\">autumn-field-72</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed-extended',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 1000,\n",
    "    optimizer='adam',\n",
    "    lr=0.0001,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=2,\n",
    "    max_epochs=100,\n",
    "    scale=True,\n",
    "    dropout=0.2,\n",
    "    embedding_size=10,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.08502790267285948\n",
      "valid loss: 0.029979193843052643\n",
      "New best score: 0.029979193843052643\n",
      "\n",
      "Epoch 1: train loss: 0.03248775874922047\n",
      "valid loss: 0.024811104128242528\n",
      "New best score: 0.024811104128242528\n",
      "\n",
      "Epoch 2: train loss: 0.02725523496405545\n",
      "valid loss: 0.02281399695258935\n",
      "New best score: 0.02281399695258935\n",
      "\n",
      "Epoch 3: train loss: 0.025421618136376157\n",
      "valid loss: 0.021918508669915564\n",
      "New best score: 0.021918508669915564\n",
      "\n",
      "Epoch 4: train loss: 0.02447954625011795\n",
      "valid loss: 0.02162936261819796\n",
      "New best score: 0.02162936261819796\n",
      "\n",
      "Epoch 5: train loss: 0.02352804730419906\n",
      "valid loss: 0.021723798720363393\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 6: train loss: 0.023122566236075826\n",
      "valid loss: 0.021430913876725768\n",
      "New best score: 0.021430913876725768\n",
      "\n",
      "Epoch 7: train loss: 0.0225583291919388\n",
      "valid loss: 0.021607249014746116\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 8: train loss: 0.022182895909626212\n",
      "valid loss: 0.021199198402019687\n",
      "New best score: 0.021199198402019687\n",
      "\n",
      "Epoch 9: train loss: 0.02162063721362253\n",
      "valid loss: 0.021264854739225878\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 10: train loss: 0.02121260527113309\n",
      "valid loss: 0.02138334601651366\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 11: train loss: 0.021083767425719043\n",
      "valid loss: 0.021322607970949873\n",
      "3  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "training_generator,valid_generator,categorical_inputs_modalities = prepare_generators_with_categorical_features(train_df,valid_df, input_features, input_categorical_features, output_feature, DEVICE, batch_size=64)\n",
    "\n",
    "embedding_sizes = [10]*len(categorical_inputs_modalities)\n",
    "\n",
    "model = FraudMLPWithEmbedding(categorical_inputs_modalities,len(input_features),embedding_sizes, 1000,0.2, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "model,training_execution_time,train_losses_embedding,valid_losses_embedding = training_loop_and_saving_best_wandb(model,training_generator,valid_generator,optimizer,criterion,verbose=True,\n",
    "                                                                                            save_path='models/DL/mlp_embeddings/simple_mlp_model_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.27499628067017\n",
      "0.004000186920166016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.838              0.602     0.605                0.28"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "x_valid_with_cat = prepare_x_valid_with_categorical_features(train_df, valid_df, input_features, input_categorical_features)\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_valid_with_cat.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=valid_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_embeddings)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.838</td></tr><tr><td>Average precision</td><td>0.602</td></tr><tr><td>Card Precision@100</td><td>0.28</td></tr><tr><td>F1 score</td><td>0.605</td></tr><tr><td>Prediction execution time</td><td>0.004</td></tr><tr><td>Training execution time</td><td>183.275</td></tr><tr><td>train loss</td><td>0.02108</td></tr><tr><td>val loss</td><td>0.02132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-field-72</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/30lvhzr6</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_221001-30lvhzr6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_embeddings', type='mlp', description='trained simple multilayer perceptron with 1 hidden layer and embedding layers')\n",
    "mlp_artifact.add_dir('models/DL/mlp_embeddings')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prequential grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudMLP(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, hidden_size=100,num_layers=1,p=0, input_size=len(input_features)):\n",
    "            super(FraudMLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.p = p\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            \n",
    "            self.fc_hidden=[]\n",
    "            for _ in range(num_layers-1):\n",
    "                self.fc_hidden.append(torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "                self.fc_hidden.append(torch.nn.ReLU())\n",
    "                \n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 2)\n",
    "            self.softmax = torch.nn.Softmax()\n",
    "            \n",
    "            self.dropout = torch.nn.Dropout(self.p)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            \n",
    "            hidden = self.fc1(x)\n",
    "            hidden = self.relu(hidden)             \n",
    "            hidden = self.dropout(hidden)\n",
    "            \n",
    "            for layer in self.fc_hidden:\n",
    "                hidden=layer(hidden)\n",
    "                hidden = self.dropout(hidden)\n",
    "            \n",
    "            output = self.fc2(hidden)\n",
    "            output = self.softmax(output)\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDatasetForPipe(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        'Initialization'\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = None\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y.values)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        if self.y is not None:\n",
    "            # DON'T ADD .to(DEVICE) BELOW!!!\n",
    "            # it will slow down training process more than 10 times\n",
    "            # return self.x[index].to(DEVICE), self.y[index].to(DEVICE)\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index], -1       \n",
    "            # return self.x[index].to(DEVICE), -1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=<class '__main__.FraudMLP'>,\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    FraudMLP,\n",
    "    max_epochs=2,\n",
    "    lr=0.001,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    batch_size=64,\n",
    "    dataset=FraudDatasetForPipe,\n",
    "    iterator_train__shuffle=True,\n",
    "    # device=DEVICE\n",
    ")\n",
    "net.set_params(train_split=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep columns that are needed as argument to custom scoring function\n",
    "# to reduce serialization time of transaction dataset\n",
    "transactions_df_scorer=transactions_df[['CUSTOMER_ID', 'TX_FRAUD','TX_TIME_DAYS']]\n",
    "\n",
    "card_precision_top_100 = sklearn.metrics.make_scorer(card_precision_top_k_custom, \n",
    "                                                     needs_proba=True, \n",
    "                                                     top_k=100, \n",
    "                                                     transactions_df=transactions_df_scorer)\n",
    "\n",
    "n_folds=4\n",
    "start_date_training_for_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "start_date_training_for_test = start_date_training+datetime.timedelta(days=(n_folds-1)*delta_test)\n",
    "delta_assessment = delta_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performance before the proper hp search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Total execution time: 82.48s\n"
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "start_time=time.time()\n",
    "\n",
    "parameters = {\n",
    "    'clf__lr': [0.001 ],\n",
    "    'clf__batch_size': [64],\n",
    "    'clf__max_epochs': [10, 20],\n",
    "    'clf__module__hidden_size': [100],\n",
    "    'clf__module__num_layers': [1,2],\n",
    "    'clf__module__p': [0],\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc':'roc_auc',\n",
    "           'average_precision': 'average_precision',\n",
    "           'card_precision@100': card_precision_top_100,\n",
    "           }\n",
    "\n",
    "\n",
    "performance_metrics_list_grid=['roc_auc', 'average_precision', 'card_precision@100']\n",
    "performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100']\n",
    "\n",
    "performances_df_validation=prequential_grid_search(\n",
    "    transactions_df, net, \n",
    "    input_features, output_feature,\n",
    "    parameters, scoring, \n",
    "    start_date_training=start_date_training_with_valid,\n",
    "    n_folds=n_folds,\n",
    "    expe_type='Validation',\n",
    "    delta_train=delta_train, \n",
    "    delta_delay=delta_delay, \n",
    "    delta_assessment=delta_valid,\n",
    "    performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "    performance_metrics_list=performance_metrics_list)\n",
    "\n",
    "print(\"Validation: Total execution time: \"+str(round(time.time()-start_time,2))+\"s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My execution time on GPU: ~70-80s\n",
    "\n",
    "Handbook's execution time: 37.16s\n",
    "\n",
    "Hp search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'clf__lr': [0.001 , 0.0001, 0.0002],\n",
    "    'clf__batch_size': [64,128,256],\n",
    "    'clf__max_epochs': [10,20,40],\n",
    "    'clf__module__hidden_size': [500],\n",
    "    'clf__module__num_layers': [1,2],\n",
    "    'clf__module__p': [0,0.2,0.4],\n",
    "    'clf__module__input_size': [int(len(input_features))],\n",
    "}\n",
    "\n",
    "scoring = {'roc_auc':'roc_auc',\n",
    "           'average_precision': 'average_precision',\n",
    "           'card_precision@100': card_precision_top_100,\n",
    "           }\n",
    "           \n",
    "performance_metrics_list_grid=['roc_auc', 'average_precision', 'card_precision@100']\n",
    "performance_metrics_list=['AUC ROC', 'Average precision', 'Card Precision@100']\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "performances_df=model_selection_wrapper(transactions_df, net, \n",
    "                                        input_features, output_feature,\n",
    "                                        parameters, scoring, \n",
    "                                        start_date_training_for_valid,\n",
    "                                        start_date_training_for_test,\n",
    "                                        n_folds=n_folds,\n",
    "                                        delta_train=delta_train, \n",
    "                                        delta_delay=delta_delay, \n",
    "                                        delta_assessment=delta_assessment,\n",
    "                                        performance_metrics_list_grid=performance_metrics_list_grid,\n",
    "                                        performance_metrics_list=performance_metrics_list,\n",
    "                                        n_jobs=1)\n",
    "\n",
    "\n",
    "execution_time_nn = time.time()-start_time\n",
    "\n",
    "parameters_dict=dict(performances_df['Parameters'])\n",
    "performances_df['Parameters summary']=[str(parameters_dict[i]['clf__lr'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__batch_size'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__max_epochs'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__module__p'])+\n",
    "                                   '/'+\n",
    "                                   str(parameters_dict[i]['clf__module__num_layers'])\n",
    "                                   for i in range(len(parameters_dict))]\n",
    "\n",
    "performances_df_nn=performances_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above search in the handbook took ~120min\n",
    "\n",
    "my execution didn't finalize for over 600min...\n",
    "\n",
    "nvidia-smi shows around 20% GPU usage of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df) = get_train_test_set(transactions_df,start_date_training,\n",
    "                                       delta_train=7,delta_delay=7,delta_test=7)\n",
    "(train_df, test_df)=scaleData(train_df, test_df, input_features)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "x_train = torch.FloatTensor(train_df[input_features].values)\n",
    "x_test = torch.FloatTensor(test_df[input_features].values)\n",
    "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
    "y_test = torch.FloatTensor(test_df[output_feature].values)\n",
    "\n",
    "training_set = FraudDataset(x_train.to(DEVICE), y_train.to(DEVICE))\n",
    "testing_set = FraudDataset(x_test.to(DEVICE), y_test.to(DEVICE))\n",
    "\n",
    "training_generator,testing_generator = prepare_generators(training_set,testing_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudMLPHypertuned(torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, input_size,hidden_size=500,num_layers=2,p=0.2):\n",
    "            super(FraudMLPHypertuned, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.p = p\n",
    "            \n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            \n",
    "            self.fc_hidden=[]\n",
    "            for i in range(num_layers-1):\n",
    "                self.fc_hidden.append(torch.nn.Linear(self.hidden_size, self.hidden_size).to(DEVICE))\n",
    "                self.fc_hidden.append(torch.nn.ReLU())\n",
    "                \n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            \n",
    "            self.dropout = torch.nn.Dropout(self.p)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            \n",
    "            hidden = self.fc1(x)\n",
    "            hidden = self.relu(hidden)             \n",
    "            hidden = self.dropout(hidden)\n",
    "            \n",
    "            for layer in self.fc_hidden:\n",
    "                hidden=layer(hidden)\n",
    "                hidden = self.dropout(hidden)\n",
    "            \n",
    "            output = self.fc2(hidden)\n",
    "            output = self.sigmoid(output)\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1fkae5ab) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b057940ea2438c869fac6b1a55b596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rosy-blaze-67</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/1fkae5ab\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/1fkae5ab</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_200200-1fkae5ab\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1fkae5ab). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660be5d21754be79382c1163fc9cf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\repos\\mgr-anomaly-ts-xai\\wandb\\run-20221219_200531-242hzixy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy\" target=\"_blank\">zesty-gorge-68</a></strong> to <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_mlp = dict(\n",
    "    dataset_id = 'fraud-detection-handbook-transformed',\n",
    "    validation = 'train test split',\n",
    "    seed = 42,\n",
    "    begin_date = '2018-07-25',\n",
    "    delta_train = 7,\n",
    "    delta_delay = 7,\n",
    "    delta_test = 7,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    hidden_size = 500,\n",
    "    num_hidden_layers = 2,\n",
    "    optimizer='adam',\n",
    "    lr=0.001,\n",
    "    early_stopping=False,\n",
    "    max_epochs=20,\n",
    "    scale=True,\n",
    "    dropout=0.2,\n",
    "    criterion='bce'\n",
    ")\n",
    "wandb.init(project=\"mgr-anomaly-tsxai-project\", config=config_mlp, tags=['mlp', 'imbalance-not-considered', 'hypertuned'])\n",
    "config_mlp = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.05643073652563466\n",
      "valid loss: 0.024018097617004054\n",
      "\n",
      "Epoch 1: train loss: 0.02638936078540988\n",
      "valid loss: 0.02153680467099957\n",
      "\n",
      "Epoch 2: train loss: 0.024881045035415328\n",
      "valid loss: 0.020263867595097056\n",
      "\n",
      "Epoch 3: train loss: 0.02339501065342952\n",
      "valid loss: 0.019642853217953907\n",
      "\n",
      "Epoch 4: train loss: 0.022876052330221473\n",
      "valid loss: 0.019326516251773885\n",
      "\n",
      "Epoch 5: train loss: 0.022160829143681973\n",
      "valid loss: 0.01904257496028656\n",
      "\n",
      "Epoch 6: train loss: 0.02143366295834569\n",
      "valid loss: 0.019655168659777535\n",
      "\n",
      "Epoch 7: train loss: 0.021043319834753355\n",
      "valid loss: 0.018881842694638867\n",
      "\n",
      "Epoch 8: train loss: 0.0206349876358818\n",
      "valid loss: 0.018905052699891872\n",
      "\n",
      "Epoch 9: train loss: 0.020476300606205457\n",
      "valid loss: 0.019224997351209826\n",
      "\n",
      "Epoch 10: train loss: 0.019763412286676326\n",
      "valid loss: 0.01863673846995786\n",
      "\n",
      "Epoch 11: train loss: 0.02069019793411389\n",
      "valid loss: 0.018626018677713935\n",
      "\n",
      "Epoch 12: train loss: 0.019821298708868958\n",
      "valid loss: 0.01858314455561718\n",
      "\n",
      "Epoch 13: train loss: 0.019269375057982284\n",
      "valid loss: 0.018931785450290384\n",
      "\n",
      "Epoch 14: train loss: 0.01947174859678481\n",
      "valid loss: 0.01842006734911246\n",
      "\n",
      "Epoch 15: train loss: 0.019054680917040753\n",
      "valid loss: 0.018433201222851234\n",
      "\n",
      "Epoch 16: train loss: 0.01929063935926739\n",
      "valid loss: 0.0186118850693169\n",
      "\n",
      "Epoch 17: train loss: 0.018696328623975307\n",
      "valid loss: 0.01808142625539166\n",
      "\n",
      "Epoch 18: train loss: 0.018717897763488183\n",
      "valid loss: 0.018053305081554928\n",
      "\n",
      "Epoch 19: train loss: 0.018171166658472824\n",
      "valid loss: 0.018024880583444047\n"
     ]
    }
   ],
   "source": [
    "# Best hps taken from the handbook:\n",
    "# learning_rate=0.001\n",
    "# hidden layers 2\n",
    "# hidden size 500\n",
    "# batch_size 64\n",
    "# max epochs 20\n",
    "# dropout p 0.2\n",
    "\n",
    "model = FraudMLPHypertuned(len(input_features)).to(DEVICE)\n",
    "criterion = torch.nn.BCELoss().to(DEVICE)\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "model,training_execution_time,train_losses,valid_losses = training_loop_and_saving_best_wandb(model,training_generator,testing_generator,optimizer,criterion,\n",
    "                                                            apply_early_stopping=False, max_epochs=20,verbose=True, save_path='models/DL/mlp_hypertuned/mlp_hypertuned_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353.0805106163025\n",
      "0.0034372806549072266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Card Precision@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision  F1 score  Card Precision@100\n",
       "0    0.876              0.659     0.686               0.284"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({'Training execution time': training_execution_time})\n",
    "print(training_execution_time)\n",
    "model.eval()\n",
    "start_time=time.time()\n",
    "predictions_test = model(x_test.to(DEVICE))\n",
    "prediction_execution_time=time.time()-start_time\n",
    "wandb.log({'Prediction execution time': prediction_execution_time})\n",
    "print(prediction_execution_time)\n",
    "\n",
    "predictions_df=test_df\n",
    "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
    "    \n",
    "performance_df = performance_assessment_f1_included(predictions_df, top_k_list=[100])\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models\\DL\\mlp_hypertuned)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>▁</td></tr><tr><td>Average precision</td><td>▁</td></tr><tr><td>Card Precision@100</td><td>▁</td></tr><tr><td>F1 score</td><td>▁</td></tr><tr><td>Prediction execution time</td><td>▁</td></tr><tr><td>Training execution time</td><td>▁</td></tr><tr><td>train loss</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC ROC</td><td>0.876</td></tr><tr><td>Average precision</td><td>0.659</td></tr><tr><td>Card Precision@100</td><td>0.284</td></tr><tr><td>F1 score</td><td>0.686</td></tr><tr><td>Prediction execution time</td><td>0.00344</td></tr><tr><td>Training execution time</td><td>353.08051</td></tr><tr><td>train loss</td><td>0.01817</td></tr><tr><td>val loss</td><td>0.01802</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zesty-gorge-68</strong>: <a href=\"https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy\" target=\"_blank\">https://wandb.ai/chamera/mgr-anomaly-ts-xai/runs/242hzixy</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_200531-242hzixy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({'AUC ROC': performance_df.loc[0,'AUC ROC']})\n",
    "wandb.log({'Average precision': performance_df.loc[0,'Average precision']})\n",
    "wandb.log({'F1 score': performance_df.loc[0,'F1 score']})\n",
    "wandb.log({'Card Precision@100': performance_df.loc[0,'Card Precision@100']})\n",
    "\n",
    "mlp_artifact = wandb.Artifact('mlp_hypertuned', type='mlp', description='trained hypertuned multilayer perceptron with 2 hidden layers')\n",
    "mlp_artifact.add_dir('models/DL/mlp_hypertuned')\n",
    "wandb.log_artifact(mlp_artifact)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a11e3a7eb51e2483c16a5d7cdfda12389edc17230fd81a6fc823433cff3faa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
